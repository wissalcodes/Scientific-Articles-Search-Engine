{
  "title": "ModelGame: A Quality Model for Gamified Software Modeling Learning",
  "authors": [
    "Ed Wilson J\u00fanior\u2217",
    "Kleinner Farias"
  ],
  "institutions": [
    "Universidade do Vale do Rio dos Sinos\nS\u00e3o Leopoldo, Rio Grande do Sul, Brazil",
    "Universidade do Vale do Rio dos Sinos\nS\u00e3o Leopoldo, Rio Grande do Sul, Brazil"
  ],
  "abstract": "Gamification has been adopted in software development tasks in recent years. This adoption seeks, for example, to improve the en- gagement of developers while creating UML models or writing code. Empirical studies report that UML models suffer from incom- pleteness and inconsistency problems. This study conjectures that gamification mechanics can improve learner engagement while learning software modeling, mitigating such problems concern- ing UML models. The current literature lacks studies that explore gamification and UML model quality in the context of software modeling learning. This article, therefore, proposes ModelGame, which is a quality model to support software modeling learning in a gamified way. It serves as a reference framework so that instructors can obtain a parameterized way to evaluate UML models created by learners. The quality of UML models can be improved by apply- ing gamified activities and providing guidelines aware of quality issues. A qualitative questionnaire was answered by 19 instructors who teach software modeling at higher education institutions. The results show that (1) 94.7% recognize that the proposed model can improve the quality of UML models, indicating that they would adopt the ModelGame in their learning practices; and (2) 47.4% do not use any gamification mechanics in their classes. The results are encouraging, showing the potential for applying and improving the teaching and learning of software modeling.",
  "keywords": [
    "Model design",
    " learning model",
    " Gamification"
  ],
  "article": "Gamification has been adopted in software development tasks in recent years. This adoption seeks, for example, to improve the en- gagement of developers while creating UML models or writing code. Empirical studies [7, 9, 14] report that UML models suffer from incompleteness and inconsistency problems. Lange [14] rein- forces that these defects bring potential risks that can cause mis- interpretation and communication failure, representing a risk to software quality. Thus, finding formats that favor student learning and consequently in generating increasingly effective UML mod- els can become one of the main challenges faced by instructors that include UML (Unified Modeling Language) as part of software modeling content.  Some studies [3, 12, 25] sought to understand how to apply gam- ification in software modeling teaching using some elements such as points, emblems and levels. However, instructors and researchers still find limitations when applying, evaluating, and measuring the use of this tool in the learning of software modeling students and, consequently, in the models developed by them, since in the current literature there is no \u201cframe of reference\u201d that guides them. This study conjectures that gamification mechanics can improve learner engagement while learning software modeling, mitigating such problems concerning UML models. The current literature lacks studies that explore gamification and model quality in the context of software modeling learning.  This article, therefore, introduces ModelGame, which is a quality model to support software modeling learning in a gamified way. It serves as a reference framework so that instructors can obtain a parameterized way to evaluate UML models created by learners. The quality of UML models can be improved by applying gami- fied activities and providing guidelines aware of quality issues. A reference framework would help to (1) establish parameters for eval- uating UML models created by learners; (2) provide guidelines to improve the quality of these artifacts; (3) to analyze which elements of gamification could be included in each of the phases of modeling using UML; (4) identify intrinsic and extrinsic aspects of students during the modeling stages, to improve the models; (5) to compare validated theories about the inclusion of gamification in software modeling teaching, taking into account the types of learning and methodologies used; and (6) contributing to the identification of gamification use objectives in modeling activities.  A qualitative questionnaire was answered by 19 instructors who teach software modeling at higher education institutions. The re- sults show that (1) 94.7% recognize that the proposed model can improve the quality of UML models, indicating that they would adopt it in their learning practices; and (2) 47.4% do not use any  100\fSBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil  Ed Wilson J\u00fanior and Kleinner Farias  gamification mechanics in their classes. These results are encourag- ing, showing the potential for applying and improving the teaching and learning of software modeling.  The remainder of the paper is organized as follows. Section 2 presents the main concepts discussed throughout the article. Sec- tion 3 discusses the related work, highlighting research opportu- nities. Section 4 introduces the proposed quality model. Section 5 presents how the quality model was evaluated. Section 6 points out some threats to validity. Finally, Section 7 presents some concluding remarks and future work.  2 BACKGROUND This section presents the essential concepts for understanding this work, including gamification and software engineering teaching (Section 2.1), and software modeling and model quality (Section 2.2).  2.1 Gamification and Software Engineering  Teaching  Gamification aims to use game elements in the context of not game [5], bringing all positive aspects they provide as a way to encourage and engage \u201cplayers,\u201d thereby broadening their motivations.  Werbach [23] classifies gamification into three dimensions: Dy- namics, Mechanics, and Components. Dynamicsinclude all game aspects related to the emotional responses of \u201cplayers\u201d (e.g., rela- tionship, progression, and narrative).Mechanics offer elements that promote the action of a game \u2014 usually elaborated via a rule-based development \u2014, so that the player can interact with such elements, e.g., challenges, feedback, and rewards. Components represent the aesthetic elements of gamification, whose goal is to present visual aspects with which players can perform the interaction, for example, points, scores, and emblems (badges).  Knowing that the teaching of Software Engineering should in- volve students to experience the professional practices of the area so that they can understand which practices and techniques are useful in several different situations [2]. The challenges of teaching new software engineers are not limited to learning programming, but also include paying attention to detail, considering the quality of created models, established schedule and defined budgets [1]. In addition to understanding the technical challenges, these future professionals must be up to date with nontechnical issues, including teamwork, communication and management.  To meet these new demands of the current context, the format with exhibition classes is no longer considered enough and may even become demotivating and ineffective in learning students. In this sense, gamification has been increasingly used in the teach- ing of software engineering as a way to promote behavioral and psychological changes [11] providing an environment that favors communication, cooperation, feedback, reward, achievement and other recurring elements that are capable of improving perfor- mance, efficiency and engagement in educational activities , and can enhance, for example, the learning of software modeling.  2.2 Software Modeling and Model Quality Software modeling encompasses the set of principles, concepts, and practices that lead to the development of a high-quality system or  product. The principles of this activity establish a philosophy that guides the entire software development process.  In this scenario, UML models play a crucial role in software development tasks, for example, documenting project decisions, understanding development details, promoting better communica- tion between teams, and generating greater efficiency in software development [19]. However, these models suffer problems of in- consistency and incompleteness [10, 18], as well as end up being overlooked within the modeling process, as pointed out in some empirical studies in the literature [14, 15]. Class and sequence dia- grams, for example, present inconsistencies when sequence diagram objects are not found in the class diagram, consequently developers end up living with inconsistencies throughout the development process.  A research challenge still open is how to evaluate these diagrams, both in industry and in the teaching process, in terms of quality, such as syntactic and semantic, for example.  3 RELATED WORK The selection of related works was carried out following two steps: (1) search in digital repositories, such as Google Scholar and Scopus (Elsevier) of articles related to gamification, quality modeling, and modeling learning; and (2) filter selected articles considering the alignment of such works with the objective of the work (Section 4). After selecting the works, they were analyzed (Section 3.1) and compared (Section 3.2), seeking to identify research opportunities.  3.1 Analysis of Related Works Porto et al. (2021) [4]. This work performed a systematic map- ping with the objective of characterizing how gamification has been adopted in noneducational contexts of software engineering activities. The main results of this study show that gamification provided benefits for activities such as requirements specification, development, testing, project management, and support process. In addition, he pointed out that the number of publications and new research initiatives has increased over the years, many posi- tive results have been achieved in software engineering activities. Nevertheless, the study reinforced that gamification can still be explored for other tasks in this area, as empirical evidence is very limited.  Marin (2021) [17]. It performed the application of gamification on some topics of a software engineering course to engage students and increase their motivation and argued that, with due motiva- tion, students can better exercise the topics and obtain more solid knowledge. There were five games related to risk management, BPMN modeling, Scrum process, design and inspection of class diagrams, and cosmic functional size measurement to assist in the learning process of the software engineering course. This study also presented the lessons learned about the application of gamification and serious games in software engineering, including limitations or disadvantages.  Jurgelaitis et al. (2018) [12]. This work conducted a research to investigate how gamification could be inserted into an Information Systems Modeling course, which covers a range of topics on UML. As a result, an implementation of the gamified system modeling course in the Moodle environment was presented, using additional  101\fModelGame: A Quality Model for Gamified Software Modeling Learning  SBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil  plugins for the use of the necessary gamified elements. The study showed good results and obtained a positive acceptance by the participating students.  Rodrigues et al. (2018) [22]. They investigated the use of games and game elements in software engineering education, through a research that had the participation of 88 instructors of this disci- pline. The results showed that most instructors are aware of these educational approaches, however, the games were adopted by only 21 participants and game elements were adopted only by 19. Games are most often used to cover \u201cSoftware Process\u201d and \u201cProject Man- agement\u201d. The most commonly used game elements are points, quizzes, and challenges. The results also show that the main rea- sons for not adopting the resources are the lack of knowledge, information about games relevant to the engineering of teaching software, and the lack of time to plan and include these approaches in the classroom.  Cosentino et al. (2017) [3]. They present a model-based ap- proach to learning modeling in a gamified way. The approach in- cludes a new language to model the gamification process itself and an environment where it can be incorporated into current mod- eling tools to allow instructors and students to design and use a complete modeling framework, including gamification elements. In addition, the approach also had as a proposal to provide support to collect and analyze gamification data, thus facilitating monitoring activities.  Yohannis (2016) [25]. This research presents an exploration of game design as an approach to strengthening the student\u2019s mas- tery in software modeling by developing their abstraction skills. It brought together concepts of gamification development, such as the lens of atoms of intrinsic skill and principles of pedagogical design of various theories and models of learning. The research follows the Design Science Research Methodology and explores the best practices of Model Oriented Engineering. As a result, a modeling game design framework and generation structure and a series of produced games are presented.  Pedreira et al. (2015) [21]. They developed a systematic map- ping of gamification in Software Engineering based on 29 studies. The mapping revealed that software implementation is the area in which most studies focus, followed by software requirements, few others in different areas, such as project planning and software testing, and even to a lesser extent in activities involving software modeling. However, the highlight of this work was to highlight that gamification in software engineering is still at a very early stage and the evidence on its impact in this field remains inconclusive.  3.2 Comparative Analysis and Opportunities Five Comparison Criteria (CC) were defined selecting the most rele- vant variables to assist in the process of identifying similarities and differences between the proposed work and the selected articles. This comparison is crucial to make the process of identifying re- search opportunities using objective rather than subjective criteria. The criteria are described below:  \u2022 Context (CC01): Works that explore the use of gamification  in software modeling teaching/learning.  \u2022 Applicability of Gamification in UML (CC03): Studies that evaluated how gamification can contribute to UML mod- els.  \u2022 Model creation (CC04): Studies that have developed a model to improve factors that imply the non-adoption of UML. \u2022 Instructor participation (CC05): Studies that collected qualitative data through the participation of software mod- eling instructors.  Table 1 shows the comparison of the selected works, confronting this work. Some gaps and research opportunities are observed: (1) only the proposed work was the only one to fully meet all compar- ison criteria; (2) although most of them targeted the application of gamification in software modeling teaching, they were not di- rected to the use of UML; (3) no study has developed a model to evaluate the learning and improvement of UML models developed by students; and (4) most of them did not have the participation of instructors to identify the difficulties and opportunities in the application of gamification in the teaching of software modeling. Thus, the next Section presents a quality model to explore these identified opportunities.  Related Work  Proposed Work Porto et al (2021) [4] Marin (2021) [17] Jurgelaitis et al (2018) [12] Rodrigues et al (2018) [22] Cosentino et al (2017) [3] Yohannis (2016) [25] Pedreira et al (2015) [21]  Completely Meets  Comparison Criterion CC3  CC4  CC2  CC5  CC1  (cid:32) (cid:35) (cid:32) (cid:32) (cid:32) (cid:32) (cid:32) (cid:35)  (cid:32) (cid:35) (cid:35) (cid:35) (cid:32) (cid:35) (cid:35) (cid:35)  Partially Meets  (cid:32) (cid:32) (cid:35) (cid:35) (cid:35) (cid:71)(cid:35) (cid:35) (cid:32) (cid:32) (cid:35) (cid:35) (cid:32) (cid:35) (cid:71)(cid:35) Does not attend (cid:35) (cid:35)  (cid:32) (cid:32) (cid:71)(cid:35) (cid:32) (cid:71)(cid:35) (cid:32) (cid:71)(cid:35) (cid:71)(cid:35) (cid:35)  Table 1: Comparative analysis of the selected related works  (cid:71)(cid:35)  (cid:32)  4 PROPOSED QUALITY MODEL This section presents the proposed quality model to support soft- ware modeling learning in a gamified way. It serves as a frame of reference so that instructors can evaluate the UML models cre- ated by students through gamified activities. Section 4.1 presents a proposal of a generic analytical framework. Section 4.2 details the abstract syntax of the proposed quality model. Section 4.3 ex- plains the quality notions related to the gamified software modeling learning.  4.1 Generic Analytical Framework Figure 1 presents the generic analytical framework for improving the quality of the models and serves as the basis for the creation of an evaluation scheme. The arrows (\"links\"), labeled as Evaluation and Gamified Modeling, represent the questions that the evidence must answer; dotted lines represent associations; rectangles rep- resent the Models (rounded corners) or the quality states (square corners) by which these bindings are measured. Ellipses represent the adverse effects that can be generated from the evaluation and use of gamification.  \u2022 Participant profile (CC02): Studies that collected data from participants for screening and profile characterization.  The numbers refer to the key questions and are connected with the concepts and relationships of the abstract syntax of the Quality  102\fSBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil  Ed Wilson J\u00fanior and Kleinner Farias  Figure 1: Generic analytical framework for gamified software modeling learning.  Model (presented in Section 4.2), as follows: (1) Are there tools that assist instructors in evaluating the models developed by stu- dents, thus reducing the poor quality and incompleteness of these artifacts? (2) What is the prevalence of characteristics that cause models to be at risk? (3) Are there notions of quality to evaluate the models as a way to define parameters when performing their correction? (4) Applying the use of gamification in models that need intervention would be a way to identify factors that could generate models with high quality levels? (5) Does the application of gamification improve the quality of the model? (5.a) How are the models without gamification evaluated in relation to those with gamification? (5.b) Are there reasons to expect that gamification models can have better quality results than those that are gener- ated without gamification? (6) Is the output model really effective when associated with reducing the poor quality of the model? (7) Does the absence of evaluation result in adverse effects? (7.a) is the evaluation acceptable for the model? (7.b) What are the potential harms, and how often do they occur? (8) Does gamification result in adverse effects on models?  Fact is that it is not enough just to include this \"toolbox\" in the UML learning process, it is necessary to provide the instructor with a model (guide) that can serve as a reference to evaluate the quality of diagrams elaborated through gamified activities. For example, the instructor could create models predefining inconsistencies by making use of these questions raised to evaluate the models created by the students. The set of questions serves as the starting point for this evaluation. Knowing that the adaptation of the gamification approach requires a significant effort [20], in this study we present The ModelGame as a way to identify factors that contribute to the quality of these artifacts and, consequently, to the students\u2019 learning.  4.2 Abstract Syntax Following the specification pattern of the UML metamodel, Figure 2 presents the abstract syntax of the proposed Quality Model for gamified software modeling learning (ModelGame). It identifies the main concepts and relationships. The numbers represent the  notions of quality that are discussed in Section 4.3. The following are detailed each of these concepts and relationships.  Domain. The first concept presented in this study is the domain, which corresponds to a specific context of the application to be developed to solve the problem. In this process, the design template represents the solution given to the domain.  Association  \u2022 contextualizes: Challenges[*]  Each contextualise refers to the domain that will serve as the  basis for the challenges launched.  Challenges. This concept represents the phase in which the problem is contextualized (domain-based), as well as what will be the missions, phases, scenarios, and other elements presented to the players, in this case the students, who must use the principles of software engineering to perform the modeling and reach the final goal.  Association  \u2022 influences: Design Model[*]  Each influence represents that the proposed challenge interfered in aspects of the design model, causing the user to seek to make a continuous improvement.  Modeling Language. Software modeling is an important step for development to happen in a way that adheres to the require- ments established by the requester, for this, there is the modeling language, which offers a standardized way to document and design software. Through the use of modeling languages, it is possible to achieve a high level of understanding about the software in ques- tion, improving the communication between all those involved in the process, thus avoiding implementation errors. It points out that software engineers use these languages to communicate design decisions and verify the feasibility of implementing the intended design. The UML was consolidated as the Modeling Language in the paradigm of object orientation, in which it is possible through visual notation generated from the diagrams- presented later in this study as Design Models- to perform the representation of various perspectives of the system.  Association  103\fModelGame: A Quality Model for Gamified Software Modeling Learning  SBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil  Figure 2: Abstract Quality Model Syntax.  \u2022 expresses: Design Model[*]  Association  Performs the representation of the intended design templates, in which the Modeling Language should be applicable to the domain type.  User. This concept corresponds to the individual who performs the interpretation of the developed design models, whose objective is to be able to understand the domain in question. In the gamified context, the user has the role of player and it is he who performs the whole process, being able to perform the interpretation of ex- isting models or even creating new ones. The user can also identify and resolve inconsistencies that arise from compositions between models.  Association  \u2022 creates: Design Model[1..*]  Represents the process in which the user creates a design template, which can be one or more.  \u2022 interprets: Design Model[1..*]  In this association, the user performs the interpretation of the design template. When interpreting the model, paths for the resolution of inconsistencies can be identified. \u2022 detects: Inconsistency [*]  Represents the user\u2019s discovery of design model inconsistencies, for example, those that are generated from identifying conflicts, whether a class is abstract or not. \u2022 resolves: Inconsistency [*]  Each resolves equates to the resolution representation of the incon- sistencies by the user that happens after he analyzes and determines the best alternative to perform this action.  \u2022 Without a directed relationship.  Modeling Tool. This concept represents the applications that are used to carry out the construction of design models. There are several tools available, online and desktop, and it is up to the user to choose the one that will best meet their needs and adapt to the context in question, that is, they work in any domain that is being considered.  Design Model. The design model refers to a visual notation (diagram) to represent static and dynamic aspects. These models are built according to a specific objective or task and tend to facilitate the logical interpretation of the software in several aspects. The static and representing a set of actions generated from functional with external users (actors). The second is a static diagram and makes the representation of the logical structure of the software involving the classes, their attributes, methods, and relationships between them [19].  Association  \u2022 describes: Domain[1]  Each describes makes the representation of a specific domain  and means that every design model must describe it.  Inconsistency. It corresponds to the defects found in the models developed by users. They may occur because of the nonidentifi- cation and correction of possible conflicts and even an erroneous interpretation. Association  \u2022 uses: Modeling Tools [*]  \u2022 affects: Design Model[*]  Determines that the user can use modeling tools to generate/update design models.  This association indicates that with each occurrence of the affect, a problem is presented harming the quality of the design model.  104\fSBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil  Ed Wilson J\u00fanior and Kleinner Farias  Points. This concept represents one of the most used game me- chanics in software engineering and functions as a quantitative reward for each action developed, in which it is possible to regulate the number of rewarded points of the player, defined here as user, based on the importance of each action. Through this concept, it is possible to stimulate competition, collaboration, and creativity among users, stimulating learning. Points appear as a derivation of the association affects, since when each inconsistency error is identified or not, the user will receive a score and the association describes, because the points will also be applied when making connections between the model and the domain.  Progress. The concept of progress emerges as a factor that makes the user able to perceive its evolution in the process, in this case, software modeling. Progress emerges as a derivation of the associa- tion interprets, making the user know when they have performed a correct interpretation of the proposed design model or what still needs to be improved.  Feedback. Feedback has the role of making the user realize that the proposed goal can be achieved and follow its evolution, includ- ing analyzing how to change or creating new strategies to achieve the goal. This concept emerges as a derivation between the associa- tions it creates, causing the user to receive a return to the model creation process.  4.3 Quality Notions As discussed in Section 2, gamification can bring important ele- ments for learning software modeling and, therefore, the objective of this section is to produce the notions of quality of the model of this study. The ModelGame is composed of ten counts, four of which are proposed in this study - scope, use, motivational and en- gagement - extracted from the main benefits that the gamification elements presented in Figure 2 can bring to the models. The others are adaptations of previous works [6, 14, 15], they are, syntactic, semantic, social, effort, detection and resolution.  Scope Quality (1). It seeks to determine how much the proposed challenge is contextualized with the design model, as well as the def- inition of the domain, problem, competencies, concepts, behaviors and attitudes that will be developed throughout the process.  Syntactic Quality (2). This notion makes the representation of the process of correction of the design models that are produced by the modeling language, because if it is not used correctly, incon- sistencies will arise. It is important to insert this notion of quality into our study, since during the process of developing the models, users may come across the composition of two class diagrams, for example.  Semantic Quality (3). It is necessary to verify that the design model and the problem domain match, so this notion performs this type of analysis. Communication problems may occur between users if the semantic elements of the model are affected.  Social Quality (4). Design models are used to communicate between members of a team to inform all established decisions about software development [8]. If divergent interpretations occur, this communication will be greatly impaired.  Quality of Effort (5). This notion refers to the production chal- lenges of the model that will be generated, including factors such as time and cost.  Quality of Use (6). To produce design templates, users can use unusual tools such as paper, whiteboard, and more. However, most of the time they choose to use formal tools (CASES) and can be online or desktop. This notion corresponds to the level of ease and applicability of the models elaborated when making use of these tools, it is also important to contribute to communication between users through collaboration-related functionalities.  Detection Quality (7). This notion is referenced to the process of locating inconsistencies, since when users arise, they should perform traceability of them quickly. If the detection is complicated, it could hinder the process of correcting the models.  Resolution Quality (8). It corresponds to the level of quality related to the effort that users take to look for alternatives to solve the identified problem.  Motivational Quality (9). This notion refers to the motiva- tional factors involved during the learning and development of design models, which can be intrinsic and extrinsic. Elements of gamification such as points, feedback and progress bring the user a degree of satisfaction in continuing their discovery and transfor- mations throughout the process.  Quality of Engagement (10). The user in tracking their progress can feel committed to the objective in question, and this notion represents the measurement of the level of commitment of them during the development of design models.  5 EVALUATION This section describes the methodology followed to evaluate the proposed quality model. This methodology follows well-established empirical guidelines [24]. Section 5.1 details the objective and re- search questions (RQ). Section 5.2 presents the questionnaire formu- lated to evaluate the proposed quality model. Section 5.3 explains the context and selection of participants. Section 5.4 describes the presentation of the Model. Section 5.5 presents the analysis of the collected data.  5.1 Objective and Research Questions The objective (O) of this study is twofold: (O1) Introduce Model- Game as a tool for teaching Software Modeling; and (O2) Analyze the applicability of the quality model regarding the improvement of UML models.  To analyze the different facets of the objectives, two Research  Questions (RQ) have been formulated:  \u2022 RQ1: How do instructors evaluate the use of gamification  in software modeling?  \u2022 RQ2: What is the acceptance of ModelGame by software  modeling instructors?  5.2 Questionnaire Data was collected through an online questionnaire created through Google Forms1 following well-established guidelines described in [24]. This strategy was chosen because the questionnaire could be applied quickly and easily collect data from individuals in geo- graphically diverse locations. The questions of the questionnaire  1Questionnaire: https://forms.gle/qjaFDpErEtGdLuWw6  105\fModelGame: A Quality Model for Gamified Software Modeling Learning  SBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil  were concerned with examining the research gaps of previous stud- ies and apprehending the structures of the previously developed questionnaire.  Part 1: Participant profile. The first part of the questionnaire consisted of collecting data that are related to the characteristics and opinions of the participants. The creation of the participant profile through this data is important to make the selection of possible users of ModelGame. Without this profile, participants with an inadequate profile may generate inconsistent assessments. Participants were asked to provide more general information, such as age, education level, academic background. Information about the time of experience in teaching was also considered, including teaching software modeling and level of knowledge about UML models.  Part 2: TAM questionnaire. The second part addressed ques- tions about the usability and acceptance of the technique, aiming to explore q3. To this end, this part of our questionnaire is based on the technology acceptance model (TAM) [16]. This part contained nine questions, which were answered through the Likert Scale, in- cluding Totally Agree, Partially Agree, Neutral, Partially Disagree, and Totally Disagree. The questions formulated (Q) dealt with sev- eral topics, including perceived ease of use (Q1-3), perceived utility (Q4-7), attitude towards use (Q8), and behavioral intention to use (Q9).  5.3 Selection of participants The participants were selected based on the following criteria: in- structors and/or professionals working in the teaching of software modeling in higher education institutions in Brazil. Using this cri- terion, we sought to select participants with academic training and practical experience in teaching. This finite set of all possible participants represents the target population [13]. This popula- tion represents those people who are in a position to answer the questions formulated and to whom the results of the survey apply [13]. In all, 19 people (n) answered the questionnaire. The partici- pants were invited via e-mail to participate in the study and each of them previously received the explanation/training about the model proposed through the researcher and there was no doubt, they could leave for the next step that consisted of completing the TAM questionnaire. We discussed the experimental process in the next section.  The second activity Apply TAM questionnaire (input). Participants received a list of questions about the perception of ease of use, per- ceived utility, attitudes, and intention of behavior, in relation to the ModelGame. Qualitative data (output) were generated, regarding the usability and acceptance of the Model under the perspective of professionals who teach software modeling. This questionnaire followed the guidelines of the TAM [16].  Phase 3: Analysis and result report. It has two activities. The first, Analyze data sought to perform a thorough analysis of the data collected through the questionnaire and the researcher\u2019s perception regarding the participants\u2019 doubts during the presentation stage. For this, the collected data were analyzed separately, as well as con- fronted, aiming to perform a triangulation of them. Subsequently, there was an Evaluation data, as a way to understand in a more depth the context, the perceptions of the participants in relation to the proposed model as well as its applicability.  Figure 3: The experimental process.  5.4 Experimental Process Figure 3 presents the experimental process used in this study, which is composed of three phases discussed below:  Phase 1: Presentation. It has an activity, presentation, in which the researcher explained to the participants through a video detail about the quality model. This process took place individually and in a standard way, where space was also made available for par- ticipants to answer possible doubts about the proposed study and model, lasting an average of 20 minutes.  Phase 2: Application of the TAM questionnaire. It has two activities, the first being Collect demographic data. The participants answered a list of questions (input) so that we could collect their characteristics and opinions about the ModelGame. The demo- graphic data collected (output) became the result of this activity.  5.5 Result Analysis 5.5.1 Profile data of the participants. Table 3 describes the profile data, reporting the characteristics and opinions of the participants. These data were collected from May 18 to June 5, 2021. In total, we had 19 participants. Our participants are between 20 and 49 years old, most of them have a degree in Computer Science (52.6%), Information Systems (26.3%) or Systems Analysis (21.1%) and are specialists (36.8%), masters (36.8%) and doctors (15.8%). About the working time in teaching, the majority (42.1%) they have been teach- ing for more than 8 years and teach disciplines related to software modeling, including software engineering, systems analysis and software projects. A total of 47.4% have a full level of knowledge about UML and almost half of them (47.4%) has not yet used gamifi- cation in the teaching of software modeling. Therefore, we consider  106\fSBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil  Ed Wilson J\u00fanior and Kleinner Farias  Perceived ease of use  I found the quality model easy to use I found the quality model easy to learn I found the quality model easy to master  Perceived usefulness  The model would make it easier to understand which elements of gamification can be used in modeling . Using the quality model would help increase productivity. The model would provide an understanding of how to mitigate the incompleteness of UML diagrams. The model would help compare theories about gamification in software modeling teaching.  Attitude towards use  Using the Quality Model for Gamified Software Modeling Learning is a good idea.  Behavioral intention to use  I would use the quality model in software modeling classes.  Totally agree  Partially agree  Neutral  Partially disagree  Totally disagree  8 10 6  12 9 5 13  13  10  9 9 12  5 8 8 4  5  7  2 0 0  2 2 5 2  1  2  0 0 1  0 0 1  0  0  0 0 0  0 0 0  0  0  Table 2: Collected data related to TAM questionnaire.  elements (scores, challenge, emblem, among others) in their classes, most (52.6%) totally agree and (42.1%) partially agree that the use of these can contribute to the quality of the models developed by the students.  We consider the percentage of instructors who have not yet used gamification in their classes to be high and this may be tied to factors such as lack of knowledge, information about the tool, and even time to plan and include these approaches [22]. Although they were based on software modeling teaching context, previous studies [3, 4, 12, 17, 25] they did not count on the participation of instructors and we understand that this participation is fundamental to understand the perceptions of these professionals since they will be at the forefront of the use of gamification.  The ModelGame proposed in this study could help them insert gamification into their classes, according to the software modeling learning design [25], based on the assumption that for this, it is necessary to develop a better understanding of the tasks, activities, skills and operations that the different elements of gamification can offer and how they can correspond to the desired learning outcomes by developing a more concrete and motivating presentation that can involve students and facilitate deep learning with UML.  that although small, our sample is adequate to carry out an initial evaluation of the proposed approach.  Characteristic and Opinion (n=19)  Age  Education  Undergraduate course  Time of experience in teaching  Experience in teaching software modeling  Level of knowledge about UML models  Used gamification in teaching  Answer  < 20 years 20-29 years 30-39 years 40-49 years > 49 years  Undergraduate* Specialization* Master* PhD* Others  Information Systems Computer Science Computer Engineering System Analysis Others  < 2 years 2-4 years 5-6 years 7-8 years > 8 years  < 2 years 2-4 years 5-6 years 7-8 years > 8 years  Beginner Junior Full Senior  Yes No Maybe  Gamification can contribute to the quality of the models of UML diagrams generated by students  Totally agree Partially agree Neutral Partially disagree Totally disagree  #  0 4 8 5 2  0 7 7 3 2  5 10 0 4 0  4 2 3 2 8  3 5 3 2 6  2 5 9 3  9 9 1  10 8 1 0 0  %  0.0% 21.1% 42.1% 26.3% 10.5%  0.0% 36.8% 36.8% 15.8% 10.6%  26.3% 52.6% 0.0% 21,1% 0.0%  21.1% 10.5% 15.8% 10.5% 42.1%  15.8% 26.3% 15.8% 10.5% 31,6%  10.5% 26.3% 47.4% 15.8%  47.4% 47,4% 5.3%  52.6% 42.1% 5.3% 0.0% 0.0%  Table 3: The profile data of the participants.  5.5.2 RQ1: How do instructors evaluate the use of gamification in software modeling? Table 3 presents the collected data related to the RQ formulated. First, we begin the analysis by verifying how instructors visualize gamification in software modeling teaching. Although most of them (47.4%) have not yet used gamification  5.5.3 RQ2: What is the acceptance of the ModelGame by software modeling instructors? Using the TAM questionnaire, we tried to evaluate the ease of use, perceived usefulness, attitude, and behav- ioral intention to use the Quality Model. Table 2 shows the data obtained. Our data obtained show that no one disagreed that the ModelGame is easy to use, learn, and master. On the contrary, al- most 90% of participants find the model easy to use (42.1% totally agree and 47.4% partially agrees and 10.5% neutral), learn (52.6%  107\fModelGame: A Quality Model for Gamified Software Modeling Learning  SBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil  fully agree and 47.4% partially agree) and master (31.6% fully agree, 63.2% partially agree and 5.3% partially disagree).  The results are also favorable considering the perception of util- ity. Most participants realized that the ModelGame would make it easier to understand which elements of gamification can be used in each of the phases of modeling using UML(63.3% totally agree, 26.3% partially agree and 10.5% neutral), increase productivity (47.4% fully agree, 42.1% partially agree and 10.5% neutral), and the use of the quality model would provide an understanding of how to mitigate the incompleteness of UML diagrams (26.3% agree totalmen 42.1% partially agree, 26.3% neutral and 5.3% partially disagree). Still in the useful aspect, we tried to know if the quality model would help to compare validated theories about the inclusion of gamification in software modeling teaching (68.4% totally agree, 21.1% partially agree and 10.5% neutral).  Considering the attitude towards use, participants believe that using the ModelGame is a good idea (68.4% totally agree, 26.3% partially agree and 5.3% neutral), just as they are confident and would use the Model in software modeling classes (52.6% totally agree, 36.8% partially agree and 10.5% neutral). These findings show the potential for acceptance by people with profiles similar to those of participants. The results are encouraging and show the potential to use the proposed approach in the educational scenario.  6 THREATS TO VALIDITY This section discusses the possible threats to the validity of the study.  Internal validity. The main point affecting the internal validity of our study concerns the total time used for the exploratory phase. To mitigate this threat, we performed the video recording of a pilot explaining the operating details and objectives of the ModelGame. In relation to the methods used, the threats related to internal validity relate to how we extract the perceptions of the discussions and whether they represent the perceptions of teachers about the use of the Model. We try to reduce this threat by applying the TAM questionnaire.  External validity. We identified threats related to external va- lidity, such as the number of participants who never applied the use of gamification. This study was limited to 19 participants (teachers) from various educational institutions, of which 9 (47.4%) never used any element of gamification in their classes, this factor can interfere in the data, since the model intends to evaluate the quality of UML diagrams from gamified activities.  Conclusion validity. Threats related to the validity of the con- clusion are related to treatment and outcome. We try to make the reduction by combining quantitative and qualitative data through different resources. These data were obtained through audio and questionnaires. We analyze this data to answer the research ques- tions.  7 CONCLUSIONS AND FUTURE WORK This study proposed an initial quality model (ModelGame) that serves as a reference framework for instructors for qualitative eval- uations of UML models developed from gamified activities, the application of an empirical study with 19 participants was carried out to understand their vision in relation to gamification and the acceptance of the proposed Model. It was identified that most have not yet used gamification in their classes, but agree that their use can contribute to the quality of the models developed by the stu- dents and were open to using the model. Our findings can enhance the adoption of new teaching practices through gamification, result- ing in the improvement of software modeling learning using UML, and consequently the creation of models developed by students. These approaches can stimulate students\u2019 immersion in the design of systems as future professionals during learning.  Finally, we hope to carry out in the future a series of experimental studies to analyze each stage of application of the ModelGame and that this work represents a first step to better support the application of empirical studies on models of evaluation of the use of gamification in software modeling. We also hope that the questions described throughout the article will encourage other researchers to extend our study to different modeling languages and teaching methodologies.  REFERENCES",
  "references": [
    "1] Rick Adcock, Edward Alef, Bruce Amato, Mark Ardis, Larry Bernstein, Barry Boehm, Pierre Bourque, John Brackett, Murray Cantor, Lillian Cassel, et al. 2009. Curriculum guidelines for graduate degree programs in software engineering. ACM. ",
    "2] Mark Ardis, David Budgen, Gregory W Hislop, Jeff Offutt, Mark Sebern, and Willem Visser. 2015. SE 2014: Curriculum guidelines for undergraduate degree programs in software engineering. Computer 48, 11 (2015), 106\u2013109.  ",
    "3] Valerio Cosentino, S\u00e9bastien G\u00e9rard, and Jordi Cabot Sagrera. 2017. A model- based approach to gamify the learning of modeling. CEUR Workshop Proceed- ings.  ",
    "4] Daniel de Paula Porto, Gabriela Martins de Jesus, Fabiano Cutigi Ferrari, and Sandra Camargo Pinto Ferraz Fabbri. 2021. Initiatives and challenges of using gamification in software engineering: A Systematic Mapping. Journal of Systems and Software 173 (2021), 110870.  ",
    "5] Sebastian Deterding, Miguel Sicart, Lennart Nacke, Kenton O\u2019Hara, and Dan Dixon. 2011. Gamification. using game-design elements in non-gaming contexts. In CHI\u201911 extended abstracts on human factors in computing systems. 2425\u20132428. ",
    "6] Ana Fern\u00e1ndez-Saez et al. 2012. A systematic literature review on the quality of  UML models. J. Data. Manage 22, 3 (2012), 46\u201370.  ",
    "7] Kleinner Farias et al. 2012. Evaluating the impact of aspects on inconsistency detection effort: a controlled experiment. In International Conference on Model Driven Engineering Languages and Systems. Springer, 219\u2013234.  ",
    "8] Kleinner Frias et al. 2014. Towards a quality model for model composition effort.  In 29th Annual ACM Symposium on Applied Computing. 1181\u20131183.  ",
    "9] Kleinner Farias et al. 2015. Evaluating the effort of composing design models: a controlled experiment. Software & Systems Modeling 14, 4 (2015), 1349\u20131365. ",
    "10] Kleinner Farias et al. 2019. UML2Merge: a UML extension for model merging.  IET Software 13, 6 (2019), 575\u2013586.  ",
    "11] Juho Hamari, Jonna Koivisto, and Harri Sarsa. 2014. Does gamification work?\u2013 a literature review of empirical studies on gamification. In 2014 47th Hawaii international conference on system sciences. Ieee, 3025\u20133034.  ",
    "12] Mantas Jurgelaitis, Vaidotas Drungilas, and Lina \u010ceponien\u02d9e. 2018. Gamified Moodle course for teaching UML. Baltic journal of modern computing 6, 2 (2018), 119\u2013127.  ",
    "13] Barbara A Kitchenham and Shari L Pfleeger. 2008. Personal opinion surveys. In  Guide to advanced empirical software engineering. Springer, 63\u201392.  ",
    "14] Christian Franz Josef Lange. 2007. Assessing and Improving the Quality of  Modeling: A series of Empirical Studies about the UML. (2007).  ",
    "15] Odd Ivar Lindland, Guttorm Sindre, and Arne Solvberg. 1994. Understanding  quality in conceptual modeling. IEEE software 11, 2 (1994), 42\u201349.  ",
    "16] Nikola Maranguni\u0107 and Andrina Grani\u0107. 2015. Technology acceptance model: a literature review from 1986 to 2013. Universal access in the information society 14, 1 (2015), 81\u201395.  108\fSBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil  Ed Wilson J\u00fanior and Kleinner Farias  ",
    "17] Beatriz Mar\u00edn. 2021. Lessons Learned About Gamification in Software Engineer- ing Education. In Latin American Women and Research Contributions to the IT Field. IGI Global, 174\u2013197.  ",
    "18] Kleinner Oliveira, Alessandro Garcia, and Jon Whittle. 2008. On the quantitative assessment of class model compositions: An exploratory study. 1th ESMDE at MODELS (2008). 2017.  Infrastructure  specification.  ",
    "19] OMG.  UML:  https://www.omg.org/spec/UML/2.5.1/PDF.  ",
    "20] Sofia Ouhbi and Nuno Pombo. 2020. Software Engineering Education: Challenges and Perspectives. In IEEE Global Engineering Education Conference. 202\u2013209. ",
    "21] Oscar Pedreira, F\u00e9lix Garc\u00eda, Nieves Brisaboa, and Mario Piattini. 2015. Gamifica- tion in software engineering\u2013A systematic mapping. Information and software  technology 57 (2015), 157\u2013168.  ",
    "22] Pedro Rodrigues, Mauricio Souza, and Eduardo Figueiredo. 2018. Games and gamification in software engineering education: A survey with educators. In 2018 IEEE Frontiers in Education Conference (FIE). IEEE, 1\u20139.  ",
    "23] Kevin Werbach and Dan Hunter. 2012. For the win: How game thinking can  revolutionize your business. Wharton digital press.  ",
    "24] Claes Wohlin, Per Runeson, Martin H\u00f6st, Magnus C Ohlsson, Bj\u00f6rn Regnell, and Anders Wessl\u00e9n. 2012. Experimentation in software engineering. Springer Science & Business Media.  ",
    "25] Alfa Yohannis. 2016. Gamification of Software Modelling Learning.. In DS@  MoDELS.  109\f"
  ],
  "url": "https://drive.google.com/uc?id=1xcJmqLk11_uZKrCJejX28cG2M8-cxgUJ",
  "date": "2023-12-26 22:49:32",
  "is_published": false
}
{
  "title": "Interactive and Adaptable Media",
  "authors": [
    "295",
    "AI Model for Computer games based on Case Based",
    "Vlado Menkovski",
    "0.8km Markopoulou Ave.",
    "Peania, 19002, Greece",
    "Dimitrios Metafas",
    "0.8km Markopoulou Ave.",
    "Peania, 19002, Greece"
  ],
  "institutions": [
    "Reasoning and AI Planning",
    "Athens Information Technology",
    "Athens Information Technology"
  ],
  "abstract": "",
  "keywords": [
    "Game AI",
    " Case Based Reasoning",
    " AI Planning",
    " Game Trees"
  ],
  "article": "The  goal  of  this  effort  is  to  explore  a  model  for  design  and  implementation of an AI agent for turn based games. This model  provides for building more capable computer opponents that rely  on  strategies  that  closely  resemble  human  approach  in  solving  problems opposed to classical computational centric heuristics in  game  AI.  In  this  manner  the  computational  resources  can  be  focused on more sensible strategies for the game play.    Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not  made  or  distributed  for  profit  or  commercial  advantage  and  that  copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy  otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires prior specific permission and/or a fee.  DIMEA\u201908, September 10\u201312, 2008, Athens, Greece.  Copyright 2008 ACM 978-1-60558-248-1/08/09... $5.00   With  the  advancement  in  computer  hardware  increasingly  more  computing  power  is  left  for  executing  AI  algorithms  in  games.  In  the  past  AI  in  games  was  mainly  a  cheating  set  of  instructions  that  simulated  the  increasing  difficulty  in  the  game  environment so that the player had the illusion of real counterpart.  Improvement  in  available  memory  and  processing  power  allows  implementation  of  more  intelligent  algorithms  for  building  the  game  environment  as  well  as  direct  interaction  with  the  human  players.     in  games  with   In  this  particular  research  the  emphasis  is  put  on  the  interaction  between  the  AI  agent  and  a  computer  player  in  the  realm  of  the  game  rules.  It  is  particularly  focused  on  turn  based  games that have the elements of uncertainty like dice or concealed  information.  At  the  beginning  a  description  of  Game  AI  algorithms  are  given;  such  as  Game  Trees  and  Minimax.  The  following  section  describes  an  approach  of  using  AI  Planning  to  improve  building  Game  Trees  imperfect  information  where  Game  Trees  tend  to  be  very  large  with  high  growth ratio. Section 4 discusses another approach that provides a  significant reduction to the number of considered moves in order  to find the favorable strategy of the AI player. This approach uses  AI Planning techniques and Case Base Reasoning (CBR) to plan  for different scenarios in predetermined strategies which would be  analogous to human player experience in the particular game. The  CBR  database  illustrates  a  set  of  past  experiences  for  the  AI  problem and the AI Planning illustrates the procedure to deal with  the  given  situation  in  the  game.  In  the  next  two  sections  implementations  and  evaluations  of  both  approaches  are  given.  The  AI  Planning  approach  is  implemented  with  the  Tic-tac-toe  game  and  the  combined  AI  Planning  and  CBR  approach  is  implemented with a model for the Monopoly game. The last part  contains conclusions and future work ideas.    2. Game Trees and Minimax   Game Trees are common model for evaluating how different  combinations  of  moves  from  the  player  and  his  opponents  will  affect  the  future  position  of  the  player  and  eventually  the  end  result of the game. An algorithm that decides on the next move by  evaluating  the  results  from  the  built  Game  Tree  is  minimax  [1].  Minimax assumes that the player at hand will always choose the  best possible move  for him, in other words the player  will try to  select  the  move  that  maximizes  the  result  of  the  evaluation  function over the game state. So basically the player at hand needs  to choose the best move overall while taking into account that the  next  player(s)  will  try  to  do  the  same  thing.  Minimax  tries  to  maximize the minimum gain. Minimax can be applied to multiple   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f296  DIMEA 2008  levels of nodes on the game tree, where the leaves bring the final  known (or considered) game state.    The minimax theorem states:   For  every  two-person,  zero-sum  game  there  is  a  mixed  strategy  for each player, such that the expected payoff for both is the same  value V when the players use these strategies. Furthermore, V is  the  best  payoff  each  can  expect  to  receive  from  a  play  of  the  game; that is, these mixed strategies are the optimal strategies for  the two players.   This  theorem  was  established  by  John  von  Neumann,  who  is  quoted as saying \"As far as I can see, there could be no theory of  games  \u2026  without  that  theorem  \u2026  I  thought  there  was  nothing  worth publishing until the Minimax Theorem was proved\" [2].   A simple example of minimax can be observed by building a  game tree of the tic-tac-toe game. The tic-tac-toe game is a simple  game which can end by the first player wining, the second player  wining or a tie. There are nine positions for each of the players in  which at each turn the player puts X or O sign. If the player has  three adjacent signs in a row, column or the two diagonals he or  she wins. This game has limited number of position and it is well  suited  for  building  the  whole  game  tree.  The  leaves  of  this  tree  will  be  final  positions  in  the  game.  A  heuristics  evaluation  function will also need to be written to evaluate the value of each  node along the way.  3. AI Planning for building Game Trees  3.1.1 AI Planning   AI  Planning  also  referred  as  Automated  Planning  and  Scheduling  is  a  branch  of  Artificial  Intelligence  that  focuses  on  finding strategies or sequences of actions that reach a predefined  goal  [3].  Typical  execution  of  AI  Planning  algorithms  is  by  intelligent  agents,  autonomous  robots  and  unmanned  vehicles.  Opposed to classical control or classification AI Planning results  with  complex  solutions  that  are  derived  from  multidimensional  space.    AI Planning algorithms are also common in the video game  development.  They  solve  broad  range  of  problems  from  path  finding to action planning. A typical planner takes three inputs: a  description  of  the  initial  state  of  the  world,  a  description  of  the  desired  goal,  and  a  set  of  possible  actions.  Some  efforts  for  incorporating  planning  techniques  for  building  game  trees  have  also shown up, similar to the approach explored in this effort. In  addition Cased Based Reasoning [4] techniques are also gathering  popularity  in  developing  strategies  based  in  prior  knowledge  about  the  problems  in  the  games.  One  of  the  benefits  from  Hierarchical  Task  Network  (HTN)  [5]  planning  is  the  possibility  to  build  Game  Trees  based  on  HTN  plans;  this  method  is  described in the following section.   3.2 Game Trees with AI Planning   An  adaptation  of  the  HTN  planning  can  be  used  to  build  much smaller and more efficient game trees. This idea has already  been  implemented  in  the  Bridge  Baron  a  computer  program  for  the game of Contact Bridge [6].   Computer  programs  based  on  Game  Tree  search  techniques  are  now  as  good  as  or  better  than  humans  in  many  games  like  Chess  [7]  and  checkers  [8],  but  there  are  some  difficulties  in  building  a  game  tree  for  games  that  have  imperfect  information  and  added  uncertainty  like  card  or  games  with  dice.  The  main   problem  is  the  enormous  number  of  possibilities  that  the  player  can  choose  from  in  making  his  move.  In  addition  some  of  the  moves  are  accompanied  with  probabilities  based  on  the  random  elements  the  games.  The  number  of  possible  moves  exponentially  grows  with  each  move  so  the  depth  of  the  search  has  to  be  very  limited  to  accommodate  for  the  memory  limitations.    in   The basic idea behind using HTN for building game trees is  that  the  HTN  provides  the  means  of  expressing  high  level  goals  and  describing  strategies  how  to  reach  those  goals.  These  goals  may be decomposed in goals at lower level called sub-goals. This  approach  closely  resembles  the  way  a  human  player  usually  addresses a complex problem. It is also good for domains  where  classical search for solution is not feasible due to the vastness of  the problem domain or uncertainties.   3.2.1 Hierarchical Task Networks   The  Hierarchical  Task  Network,  or  HTN,  is  an  approach  to  automated  planning  in  which  the  dependency  among  actions  can  be given in the form of networks [9] [Figure 1].   A simple task network (or just a task network for short) is an  acyclic  digraph  (cid:2) (cid:3) (cid:4)(cid:5)(cid:6) (cid:7)(cid:8)  in  which  U  is  the  node  set,  E  is  the  edge set, and each node (cid:9) (cid:10) (cid:5) contains a task (cid:11)(cid:12). The edges of (cid:2) define a partial ordering of U. If the partial ordering is total, then  we say that (cid:2) is totally ordered, in which case (cid:2) can be written as  a sequence of tasks (cid:2) (cid:3) (cid:13)(cid:11)(cid:14)(cid:6) (cid:11)(cid:15)(cid:6) (cid:16) (cid:6) (cid:11)(cid:17)(cid:18).  Buy milk  Go to (shop)  Purchase   Go to (home)  Figure 1: Simple Hierarchical Task Network   A  Simple  Task  Network  (STN)  method  is  a  4-tuple  of  its  name,  task,  precondition  and  a  task  network.  The  name  of  the  method  lets  us  refer  unambiguously  to  substitution  instances  of  the  method,  without  having  to  write  the  preconditions  and  effects  explicitly.  The  task  tells  what  kind  of  task  can  be  applied  if  the  preconditions  are  met.  The  preconditions  specify  the  conditions  that the current state needs to satisfy in order for the method to be  applied.  And  the  network  defines  the  specific  subtasks  to  accomplish in order to accomplish the task.   A  method  is  relevant  for  a  task  if  the  current  state  satisfies  the  preconditions of a method that implements that task. This task can  be  then  substituted  with  the  instance  of  the  method.  The  substitution  is  basically  giving  the  method  network  as  a  solution  for the task.   If  there  is  a  task  \u201cGo  home\u201d  and  the  distance  to  home  is  3km  [Figure 2] and there exists a method walk-to and this method has a  precondition that the distance is less than 5km, then a substation  to the task \u201cGo home\u201d can be made with this method instance.    Go-to (from, to)  If (to \u2013 from) < 5km   Walk (to)  Figure 2: HTN Method   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \fInteractive and Adaptable Media  297  If the distance is larger than 5km  another meth to be substituted [Figure 3].   hod instance needs   Go-to (from, to)   If(to \u2013 from) < 5km   If(t  to \u2013 from) < 200km   Walk (to)   Drive(to  )  Figure 3: HTN Method 2   An STN planning domain is a set of operatio methods  M.  A  STN  planning  problem  is  a  4-tu state  S0,  the  task  network  w  called  initial  task STN domain. A plan (cid:19) (cid:3) (cid:13)(cid:20)(cid:14)(cid:6) (cid:16) (cid:6) (cid:20)(cid:21)(cid:18) is a soluti problem if there is a way to decompose w into \u03c0 and  each  decomposition  is  applicable  in  the  ap the  world.  The  algorithm  that  is  capable  to  networks into plans is called Total-forward-deco [9]  or  Partial-forward-decomposition  (PFD).  H cases  where  one  does  not  want  to  use  a  forwa procedure. HTN planning is generalization of S gives  the  planning  procedure  more  freedom construct the task networks.    ons O and a set of  uple  of  the  initial  k  network  and  the  ion for a planning  \u03c0 if \u03c0 is executable  ppropriate  state  of  decompose  these  omposition (TFD)  However  there  are  ard-decomposition  STN planning that  m  about  how  to   In order to provide this freedom, a bookke is needed to represent constraints that the plann not  yet  enforced.  The  bookkeeping  is  done  by unenforced constraints explicitly in the task netw  eeping mechanism  ning algorithm has  y  representing  the  work.   The  HTN  generalizes  the  definition  of  a STN. A task network is the pair (cid:2)  (cid:3)   (cid:4)(cid:5)(cid:6) (cid:23)(cid:8) w task  nodes  and  C is  a  set  of  constraints.  Eac specifies a requirement that must be satisfied by a solution to a planning problem.    a  task  network  in  where (cid:5) is a set of  h  constraint  in  C  y every plan that is   The  definition  of  a  method  in  HTN  also definition  used  in  STN  planning.  A  HTN  pla name,  task,  subtasks,  and  constraints.  The  s constraints form the task network. The HTN plan identical to STN planning domains except they u instead of STN methods.   o  generalizes  the  an  is  a  4-tuple  of  subtasks  and  the  nning domains are  use HTN methods   plan. The branches of the game tree rep the  methods.  Tignum2  applies  all  met state  of  the  world  to  produce  new continues  recursively  until  there  are  n have  not  already  been  applied  to  th world.    present moves generated by  thods  applicable  to  a  given  w  states  of  the  world  and  no  applicable  methods  that  he  appropriate  state  of  the   In the task network generated by Tignu actions will occur is determined by th By  listing  the  actions  in  the  order  network can be \u201cserialized\u201d into a gam  um2, the order in which the  e total-ordering constraints.  they  will  occur,  the  task  me tree [Figure 4] [Figure 5].   Figure 4: HTN to Game Tr  ree Algorithm  Figure 5: Game Tree built fr  rom HTN  Compared  to  classical  planners  the  prim HTN planners is their sophisticated knowledge r reasoning  capabilities.  They  can  represent  and  non-classical  planning  problems;  with  a  good guide them, they can solve classical planning p magnitude  more  quickly  than  classical  or  neoc The  primary  disadvantage  of  HTN  is  the  nee author to write not only a set of planning opera of methods.  3.2.2 HTN Planning in building Game   mary  advantage  of  representation and  solve  a  variety  of  d  set  of  HTNs  to  problems orders of  classical  planners.  ed  of  the  domain  ators but also a set   Trees  For  a  HTN  planning  algorithm  to  be  adap trees  we  need  to  define  the  domain  (set  of  H operators) which is the domain of the game. Thi a  knowledge  representation  of  the  rules  of  the environments and possible strategies of game pla  ted  to  build  game  HTN  methods  and  is is in some sense  e  game,  the  game  ay.  In this domain the game rules as well as kn tackle  specific  task  are  defined.      The  implem is  called  Tign Tree  building  with  HTN  implementation  uses  simila decomposition, but adapted to build up a game   nown strategies to  mentation  of  Game  [9].  This  num2  ar  forward- to  tree rather than a   a  procedure   4. Case Based Reasoning in 4.1 Case Based Reasoning Case-based reasoning (CBR) is a  Artificial  Intelligence  (AI),  both  as  problems and as a basis for standalone   n Game Strategies  well established subfield of  a  mean  for  addressing  AI  AI technology.  Case-based  reasoning  is  a  paradigm solving  and  learning  that has  became  applied  subfield  of  AI  of  recent  yea intuition that problems tend to recur. I are  often  similar  to  previously  en therefore, that past solutions may be of [10].    m  for  combining  problem- one  of  the  most  successful  ars.  CBR  is  based  on  the  It means that new problems  ncountered  problems  and,  f use in the current situation   CBR is particularly applicable to probl available,  even  when  the  domain  is  n for  a  deep  domain  model.  Helpdesks, systems  have  been  the  most  successfu to  determine  a  fault  or  diagnostic  attributes,  or  to  determine  whether  or repair is necessary given a set of past s  lems where earlier cases are  not  understood  well  enough  ,  diagnosis  or  classification  ul  areas  of  application,  e.g.,  an  illness  from  observed  r  not  a  certain  treatment  or  olved cases [11].   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f298  DIMEA 2008  Central tasks that all CBR methods have to deal with are [12]: \"to  identify  the  current  problem  situation,  find  a past  case  similar  to  the  new  one,  use  that  case  to  suggest  a  solution  to  the  current  problem, evaluate the proposed solution, and update the system by  learning from this experience. How this is done, what part of the  process  that  is  focused,  what  type  of  problems  that  drives  the  methods, etc. varies considerably, however\".    And  in  the  third  position  if  the  two  of  center,  middle  top  and  middle left are available the position is a certain victory.   There are many different arrangements of the player\u2019s tokens  that  give  equivalent  positions  as  these  three  positions.  By  using  planning we do not need to consider all possible layouts but just  consider these three similar to what a human would consider.   across   application   the  underlying   ideas  of  CBR  can  be  applied  While  consistently  specific  implementation  of  the  CBR  methods  \u2013in  particular  retrieval  and  similarity  functions\u2013  is  highly  customized  to  the  application  at  hand.  4.2 CBR and Games   domains,   the   Many  different  implementations  of  CBR  exist  in  games.  CBR  technology  is  nicely  suited  for  recognizing  complex  situations much easier and more elegant than traditional parameter  comparison  or  function  evaluation.  There  are  especially  evident  cases in real time strategies where different attack and defense of  global strategies are nicely defined by CBR datasets and later used  in  the  running  games.  Also  intelligent  bots  behavior  is  also  another typical example. Depending on the number of enemy bots  the  layout  of  the  terrain  and  position of  human  players  the  CBR  system  finds  the  closest  CBR  case  and  employs  that  strategy  against the human players which in prior evaluation was proved to  be highly efficient.   5. Game Trees with AI Planning \u2013 Tic-tac-toe  In  order  to  show  the  expressive  power  of  AI  Planning  in  defining strategies  for games, and the use of these plans to build  Game  Trees I implemented an algorithm that builds Game  Trees  for the Tic-Tac-Toe game.   The  game  tree  of  Tic-Tac-Toe  shows  255,168  possible  games  of  which  131,184  are  won  by  X  (the  first  player),  77904  are won by O and the rest 46,080 are draw [13]. All these games  can be derived from building a complete Game Tree.    Even  though  it  is possible  to  build  a  complete  game  tree  of  Tic-tac-toe  it  is  definitely  not  an  optimal  solution.  Many  of  the  moves in this tree would be symmetrical and also there are a many  moves  that  would  be  illogical  or  at  least  a  bad  strategy  to  even  consider.    So what strategy  should X (the first player) choose in order   to win the game?   There  are  few  positions  that  lead  to  certain  victory.  These  positions  involve  simultaneous  attack  on  two  positions  so  the  other player could not defend, basically the only trick in Tic-Tac- Toe.   Figure 6: Tic-tac-toe winning strategy positions   Position 1 leads to victory if the two of the three fields: top  middle,  bottom  left  corner  and  bottom  right  corner  are  free  [Figure 6].   Position 2 lead to victory if two of the three fields: top right  corner, bottom right corner and bottom middle are free [Figure ].    The game starts from an empty table.   The two relevant strategies that would lead to these positions   are to take one corner or to take the center [Figure 7].   Figure 7: Tic-tac-toe Two starting moves   The  center  position  as  we  can  see  in  the  simulation  results  lead  to  a  bigger  number  of  victorious  endings  but  it  is  also  a  straight forward strategy with obvious defense strategy.   At this point we need to consider the moves of the opponent.  If  we take the left branch the opponent moves can be a center, a  corner  or  a  middle  field.  We  also  need  to  differentiate  with  a  move to a corner adjacent with our like top left or bottom right or  across the center to bottom right [Figure 8].   Figure 8: Tic-tac-toe opponent response to corner move   In  cases  one  and  two,  we  have  a  clear  path  to  executing  strategy  3  so  we  need  to  capture  the  diagonally  opposite  field.  And as for the third case the best way to go is to capture the center  and go for strategy 1 or 2 depending of the opponent\u2019s next move.    Figure 9: Tic-tac-toe move 2 after corner opening   The first move leads to certain victory, O will have to go to  the  center  and  X  will  achieve  strategy  3  [Figure  9].  The  second  move is a possible way to strategy 3 if O makes a mistake in the  next  loop,  so  X  goes  to  the  opposite  corner.  For  the  third  case  since  O  is  playing  a  valid  strategy  the  only  move  that  leaves  a  possible mistake from O would be to take the center and wait for  O to go to the middle and then achieve strategy 1 or 3 which will  be  a  symmetric  situation  to  the  one  that  we  will  find  if  we  branched with the center.   Figure 10: Tic-tac-toe opponent response to center move   If  we  go  back  to  the  second  branch  [Figure  10],  a  possible  way for the second player to engage is corner or middle. The first   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \fInteractive and Adaptable Media  299  move  is  a  valid  strategy  for  O  and  can  be  mee corner move from X to try a mistake from O in  the same as in the third case above from the pre another  move  would  be  go  to  the  middle  wh achieves strategy 1 or 2.    et  with  a  opposite  the future exactly  evious branch, and  here  X  eventually   This HTN when executed will re  game scenarios. By creating nodes from them with branches with the move of t tree for the Tic-tac-toe game over whi algorithm.   esult with plans for possible  m each position and linking  the player we create a game  ich we can run the minimax   Figure 11: Tic-tac-toe Move 2 after cent The fist move will lead to win if O moves  draw if it  goes  for the corners [Figure 11]. In t has  to  block  the  lower  left  corner  which  leave middle left or corner left which are strategy 1 and To sum the strategies for the planning, first  corner strategy for the beginning. Then for the ce the  corners  with  the  particularly  the  one  oppo holds. If the center is empty for the second strate we go for the opposite corner. After this point w opponent  or  try  to  implement  strategies  1,  2  or victory.   Plan 1: Take center   Preconditions: Center empty  Plan 2: Take corner   Preconditions: All corners empty  Plan 3: Take corner after center  Preconditions: We have center take corner oppos opponent has  Plan 4: Take diagonal corner  Preconditions: We have a corner, the opponent ha  the corner opposite to the one we have is free.  Plan 5: Block  Precondition: The opponent has tree tokens in a r agonal  Plan 6: Win  Preconditions: We have two tokens in a row, colu nd the third place is free  Plan 7: Tie  Preconditions: If all places are taken, it\u2019s a tie.  5.1 Hierarchical Task Network   ter opening to the middle or a  the second case O  es  X  to  go  for  the  d 2. we have center or  enter we try to get  osite  to  the  one  O  egy we go for it or  we either block the  r  3  which  lead  to   site to the  one the   as the ce\u2212nter and  row, colu\u2212mn or di  mn or dia\u2212gonal a  Top level task is Play [Figure 12]. This is a  can  be  derived  into:  Win,  Block,  Tie  or  Sear Search for plan is derived to both Plan 1 and Pla Plan 4, which later leads to a call for the oppon recursive call to Play.   a complex task and  rch  for  Plan.  The  an 2 or Plan 3 and  nent\u2019s move and a   Figure 12: Tic-tac-toe HT  TN  This set up with 7 plans with 3 ta for Tic-tac-toe which considers all pos player  with only 457 games, 281 of  w and  0  where  the  second  opponent  w reduction over the 255, 168 possible g tree. These reductions can be very use computing  capabilities  but  also  we  pr that planning can be very efficient if d trees  by  applying  reasoning  very  reasoning.   arget strategies creates a tree  ssible moves for the second  which X wins 176 are draw  wins.  This  is  a  significant  ames with a complete game  eful for devices with limited  rove  a  very  important  point  designing meaningful game  similar  to  human  player   me  tree  are  also  possible  if  d, in other words if we drop  moves of the opponent.   Further  improvements  to  the  gam the opponents moves are also planned all the meaningless and symmetrical m 6. Game AI in Monopoly  6.1 Overview of the AI Imp The  AI  agent  is  responsible  for  players in the game. The core principle a Game Tree with all the sensible move make  from  the  current  point  of  time minimax  algorithm  the  agent  selects  t would  bring  the  computer  player  mo with  the  highest  probability.  Building  that would be big enough to consider  is  obstructed  by  the  vastness  of  poss with all the possible random landings  nodes  of  the  game  tree  exponentially tackle  this  problem  the  AI  agents  discussed technologies: Case Based Re The  technologies  are  employed  First the agent searches the CBR datab largest similarity  with the current state associated  with  a  playing  strategy.  Th that the planner needs to build plans f consecutive player  moves that bring th way only moves that are part of that str being a small fraction of the overall po edges of the game tree at each level dec At each level of the game tree the of  a  single  player.  After  the  strateg considered the response to those strate by the opponent(s). The move of the  probability  distribution  of  the  dice  as  player.  A  more general strategy  needs opponent\u2019s  (human  player)  moves  sin the  expertise  of  the  opponent.  This  ge more plausible moves than the focused After  covering  all  opponents  t deducting  a  feature  move  of  the  com CBR  selected  plan  strategy.  After  strategies  and  reaching  a  reasonable  s into  account  the  memory  limits  an probabilities  that  the  move  is  possible the dice the building of the Game Tre algorithm  searches  the  Game  Tree  favorable  move  for  the  AI  player  usi The process is repeated each time the A  plementation the  moves  of  the  artificial  e of the AI agent is building  es that all the players would  e  forward.  Then  using  the  the  move  that  in  the  future  ost  favorable  game  position  a  Game  Tree  in  this  game  sufficient number of moves  sible  moves  in  combination  of the dice. The number of  y  grows  at  each  level.  To  incorporates  two  already   easoning and AI Planning.   in  the  following  manner.  base to find the case with the  e of the board. This case is  he  strategy  consists  of  goal  for, and the plans consist of  he player to that goal. This  rategy are considered, those  ossible moves the number of  creases immensely.  e model considers the moves  gies  of  the  AI  player  are  egies needs to be considered  opponent(s) depends of the  well  as  the  strategy  of  the  s to be implemented for the  nce  we  cannot  be  aware  of  eneral  strategy  would  bring  d strategy of the AI player.   the  agent  comes  back  to  mputer  player  by  using  the  creating  several  loops  of  size  of  a  Game  Tree  taking  nd  the  rapidly  decreasing  e  due  to  the  distribution  of  ee stops. Then the minimax  and  decides  on  the  most  ing  the  minimax  algorithm.  AI player is up.   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f300  DIMEA 2008  On  the  other  hand  the  MonopolySolution  class  holds  the  three  particular  attributes  that  are  needed  for  the  planning,  the  planning Domain, State and TaskList.   The  game  is  implemented  by  using  the  Model-View- Controller  software  development  pattern.  The  controller  is  responsible  for  implementing  the  game  rules  and  handling  all  of  the  events  in  the  game  like  roll  of  dice,  input  commands  for  trading,  auctioning  and  etc  from  the  players.  The  View  layer  is  responsible  for  displaying  the  board  and  all  of  the  input  widgets  on  to  the  game  screen,  and  the  models  are  data  structures  representing the game state [Figure 14].   Buying,  auctioning  and  trading  game  moves  are  always  accompanied  by  return  of  investment  calculations  in  making  the  plans. These calculations represent adaptation of the more general  planning  associated  with  the  cases  in  the  CBR  database.  These  adaptations  are  necessary  due  to  the  fact  that  the  cases  do  not  identically  correspond  to  the  situation  on  the  table.  In  addition  calculating the game position value of each node of the game tree  is  done  by  heuristic  functions  incorporate  economic  calculations of net present value, cash, and strategic layout and so  on.  For  example  railroads  in  monopoly  are  known  to  be  strategically  effective  because  they  bring  constant  income  even  though  the  income  can  be  smaller  than  building  on  other  properties.   6.2 Details on the CBR Implementation   that   The  implementation  of  the  CBR  is  by  using  the  JColibri2  platform.    JColibri2  is  an  object-oriented  framework  in  Java  for  building  CBR  systems  that  is  an  evolution  of  previous  work  on  knowledge intensive CBR [14].    For this implementation we need to look into three particular  classes  of  the  JColibri2  platform.  The  StandardCBRApplication,  Connector,  CBRQuery.  For  a  JColibri2  implementation  the  StandardCBRApplication interface needs to be implemented.    The CBR cycle executed accepts an instance of CBRQuery.  This  class  represents  a  CBR  query  to  the  CBR  database.  The  description  component  (instance  of  CaseComponent)  represents  the description of the case that will be looked up in the database.  All  the  solutions  case  CaseComponent interface.   implementing   cases   and   are   The  JColibri2  platform  connects  to  the  CBR  database  via  a  Connector  class.  Each  connector  implements  all  the  necessary  methods for accessing the database, retrieval of cases, storing and  deletion  of  cases.  This  implementation  uses  a  custom  XML  structure  for  holding  the  CBR  cases.  Since  the  game  will  not  update  the  CBR  database  only  read  it,  a  XML  solution  satisfies  the needs. The XML file to a certain extent is similar to the XML  representation  of  the  board.  We  are  interested  in  finding  one  CBRCase that is the most similar case to the situation in the game  at  the  time  of  the  search.  This  procedure  is  done  in  the  cycle  method of the CBRApplication. The JColibri2 CBR comparison is  done by Nearest Neighbor (NN) search method.    JColibri2  offers  implementations  for  NN  search  algorithms  of  simple  attributes.  These  implementations  are  called  local  similarities.  For  complex  attributes  like  in  our  case  global  customized similarity mechanisms need to be implemented.   The  MonopolyDescription  class  [Figure  13]  is  basically  a  serialization of the  GameState. It holds all the information about  the state of the board, the players, their amount of cash etc.    Figure 13: Class diagram of the Monopoly Case component  models   Figure 14: Class diagram of the Monopoly models   6.2.1 Complex Similarity representation in CBR   implementing   The  similarity  measurement  part  of  the  Nearest  Neighbor  implemented  by  the  algorithm  JColibri2  is  LocalSimiralrityFunction  the  GlobalSimiralityFunction  and  interface. A local similarity function is applied to simple attributes  by the NN algorithm, and a global similarity function is applied to  compound  attributes.  In  the  case  of  our  implementation  the  attributes  of  the  MonopolyDescription  are  compound  attributes  describing  the  state  of  the  board,  number  of  players,  amount  of  cash  for  every  player  and  etc.  Since  MonopolyDescription  is  a  custom  CaseComponent  a  global  similarity  function  needs  to  be  implemented  to  accurately  find  the  distance  between  different  CBR cases.   The similarity mechanism is inseparable core element of the  CBR  system.  This  mechanism  represents  how  the  CBR  decides  which  strategy  is  best  suited  for  the  particular  situation  by   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \fInteractive and Adaptable Media  301  calculating  the  distance  or  similarity  to  other  cases  in  the  database.    For  the  monopoly  implementation  we  need  to  consider  several  basic  strategies.  Monopoly  is  based  on  investing  in  properties and receiving revenues from those investments. One of  the basic strategies of the game is to build a set of properties that  will  bring  constant  income  larger  than  the  one  of  the  opponents.  So in time the opponents will have to declare bankruptcy. But on  the other hand over investment can lead to too stretched resources  with  low  income  that  will  eventually  drove  the  player  to  bankruptcy.  To  decide  on  these  two  we  need  a  clear  separation  into two groups of cases in the CBR database. The first group of  cases will represent a situation on the board where the player has  significant  income  per  loop  formed  of  one  or  more  color  group  properties, maybe railroads, some buildings on them and so on. It  is  important  to  note  that  in  this  case  the  player  is  better  situated  than his opponents so he only needs to survive long enough to win  the  game.  In  the  other  group  of  cases  either  the  opponent  is  not  well  positioned on  the board  or  its  opponents  are better  situated.  In  this  case  further  investments  are  necessary  to  improve  the  situation  so  the  player  can  have  a  chance  of  winning  in  the  long  run.    These metrics can be owning color groups, valuing groups of  railroads, evaluating the other opponents as well, and considering  the amount of cash. As it is obvious in monopoly the number of  streets is not as nearly as important as the combination of streets  the  player  owns.  It  is  also  important  to  note  that  one  CBR  case  does not hold only a single strategy in place, but its solution can  have multiple different strategic goals. For example one CBR case  might simultaneously say buy this land to form a color group but  also  trade  some  other  unimportant  property  to  increase  cash  amount.    The cases do not represent all possible combinations of board  positions. They are only representation of typical game scenarios.  The CBR Case solutions do not give exact instructions in general  but  rather  strategic  goals.  For  example  one  CBR  Solution  might  say trade the streets that  you only have one of  each  for the ones  that you have two of that color already. Then the planner based on  the situation on the board needs to decompose this high level task  to a low level operations. Like offer \"Mediterranean Avenue\" for  \"Reading Railroad\" and offer $50. The exact amounts and actual  streets are left to the planer to evaluate.    The monopoly CBR database is currently in development on  a  monopoly  clone  game  called  Spaceopoly.  The  cases  are  architected  based  on  human  player  experience  and  knowledge.  There is a plan of making a number of slightly different strategies  that  differ  on  the  style  of  playing  and  then  running  simulation  tests that would determine the particular validity of each database  as  well  as  validity  of  certain  segments  of  the  strategy  or  even  particular cases in the database.    JSHOP2  uses  ordered  task  decomposition  in  reducing  the  HTN to list of primitive tasks  which form the plans.  An ordered  task decomposition planner is an HTN planner that plans for tasks  in  the  same  order  that  they  will  be  executed.  This  reduces  the  complexity  of  reasoning  by  removing  a  great  deal  of  uncertainty  about  the  world,  which  makes  it  easy  to  incorporate  substantial  expressive  power  into  the  planning  algorithm.  In  addition  to  the  usual HTN methods and operators, the planners can make use of  axioms,  can  do  mixed  symbolic/numeric  conditions,  and  can  do  external function calls.    In  order  for  the  JSHOP2  planer  to  generate  plans  it  needs  tree  crucial  components:  Domain,  State  and  Tasks.  The  Domain  defines  all  the  functionalities  that  the  particular  domain  offers.  These  are  simple  and  complex  tasks.  The  complex  tasks  also  called methods create the hierarchy with the fact that they can be  evaluated  by  simple  tasks  of  other  complex  tasks.  This  is  how  a  hierarchical structure of tasks is formed. The problem reduction is  done by reducing the high level complex tasks to simpler until all  the tasks are primitive. The list of primitive tasks forms the plan.   The  State  represents  the  state  of  the  system.  It  is  a  simple  database of facts that represent the state of the system. The State  is  necessary  to  determine  the  way  the  problems  or  tasks  are  reduced  to  their  primitive  level.  The  reduction  is  done  by  satisfying  different  prerequisites  set  in  the  methods;  these  prerequisites  are  defined  in  the  state.  The  Tasks  are  high  level  tasks or methods defined in the Domain. The planner based on the  State and the goals selects one or more high level tasks that need  to be reduced to plans [Figure  15].   Tasks  Core Planner   State  Plan  Figure 15: Diagram of a Planner   The  plans  then  generate  the  game  moves.  The  number  of  moves  generated  by  the  plans  is  just  a  fraction  of  the  possible  moves  at  that  point.  This  reduces  the  game  tree  providing  the  opportunity to generate smaller and deeper game trees and making  more efficient decisions in general.    7. Conclusion   Even  though  the  results  from  the  CBR  database  are  not  complete  at  this  time  partial  strategies  are  implemented  as  cases  and  recognized  during  game  play  by  the  CBR  system.  These  smaller  local  strategies  coupled  with  more  global  higher  level  strategies  that  are  particularly  important  at  the  beginning  of  the  game  would  form  a  complete  CBR  database  and  represent  a  knowledge engineered style of playing of the AI player.    The  actual  execution  of  the  strategies  will  not  differ  from  strategy to strategy since the plan execution is more related to the  structure and rules of the game than to the actual playing strategy.   The AI Planning approach is a proven method by the tic-tac- toe  experiment  and  is  suitable  for  implementing  the  strategies  associated with the CBR cases.   6.3 Details on the Planning Implementation   For  the  purpose  of  planning  this  implementation  uses  a  the  JSHOP2  planner.  The  Java  Simple  modification  of  Hierarchical  Ordered  Planner  2  is  a  domain  independent  HTN  planning system [15].    This  approach  in  general  benefits  from  both  technologies,  CBR  as  well  as  AI  Planning  and  comprises  an  elegant  solution.  Even  though  AI  Planning  can  be  enough  as  a  single  technology  for  some  simpler  problems  like  tic-tac-toe  the  complexity  of  Monopoly would mean that the Planner would have to incorporate   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f302  DIMEA 2008  large and complex domain and a very big state model. The CBR  application helps reduce this complexity by focusing the planning  on  smaller  domain  of  the  game.  Basically  the  CBR  reduces  the  overall  goal  of  the  play  (wining  the  game)  to  smaller  more  concrete  goals  suitable  to  the  particular  state  of  the  game,  thus  reducing  the  need  for  global  planning  strategies  and  complex  planning domain.    Furthermore  this  symbiosis  of  technologies  gives  way  for  more precise and finely tuned strategies which can be difficult to  include into global plan for the whole game. One simple example  for  the  Monopoly  game  would  be  this:  Sometimes  it\u2019s  better  to  stay  in  jail  because  rolling  double  increases  the  probability  of  landing  on  some  field  (two,  four,  six,  eight,  ten  or  twelve  steps  from  the  jail)  that  can  be  of  great  importance  to  the  rest  of  the  game.  These  and  similar  small  local  strategies  can  be  easily  recognized by similar cases in the CBR database.    In  other  words  the  system  is  flexible  enough  so  that  new  strategies can be incorporated easily missing strategies can be also  recognized by the distance metrics as well as wrong assumptions  in the strategies can be easily recognized.   One other important property of  the system is that is highly  configurable.  The  game  its  self  can  be  diversely  different  depending  on  the  configuration  of  the  board.  Even  though  the  platform  is  restricted  to  Monopoly  type  of  games,  changing  the  layout  and  values  of  the  fields  effectively  brings  completely  different  properties  of  the  game.  In  addition  the  CBR  database  represents the entire experience  of the AI Player. It can be filled  with rich set of strategies or even configured with different flavors  of difficulties of play, this of course coupled with the domain of  the planner which can differ from a case to a case as well.    8. Future Work   Further  exploration  of  this  technology  would  go  towards  complete  implementation  of  an  AI  aware  agent  for  monopoly.  Initial  results  from  the  local  cases  with  more  specific  strategies  show CBR as a capable tool for representing expertise in playing  the  game.  Completing  the  more  general  strategies  and  coupling  them  with  the  planning  domain  will  give  precise  results  on  the  benefits from this architecture.   There is also need for exploring the planning of strategies of  opponents.  This  task  is  to  some  extent  different  because  we  cannot  always  expect  the  opponent  to  select  the  best  move  we  think.  In  the  Tic-tac-toe  example  all  possible  moves  of  the  opponent  were  taken  into  consideration,  if  we  used  the  same  planner  for  the  opponent  only  tie  games  would  result  from  the  game tree. In other words mistakes of the players also need to be  considered.    The CBR Platform brings other functionalities well worth of  exploring as well. The revision stage of the JColibri2 platform is  basically capable of fine tuning strategies or even developing new  strategies  for  the  games.  A  well  written  underlying  AI  planning  model  with a  capable  feedback of the  game tree evaluation back  to  the  CBR  revision  capability  can  be  an  interesting  concept  in  automatic experience acquisition for the AI model.   There  are  also  many  other  fields  were  combined  CBR  and  planning  approach  can  be  incorporated  into  a  problem  solution.  This combination is analogous in a big extent to a human way of   reasoning.  People  in  addition  to  logic  of  reasoning  in  situations  with  lack  of  information  rely  to  planning  strategies  and  prior  experience,  exactly  the  intuition  behind  CBR  \u2013  AI  Planning  architecture.    9. ACKNOWLEDGMENTS   We  would  like  to  thank  Prof.  Sofia  Tsekeridou  for  her  involvement  in  the  valuable  discussions  we  had  on  the  topic  of  CBR.   10. REFERENCES  [1] Minimax. Wikipedia. [Online] [Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Minimax.   [2] Von Neumann, J: Zur theorie der gesellschaftsspiele Math.   Annalen. 100 (1928) 295-320   [3] Automated Planning. Wikipedia. [Online] [Cited: April 23,   2008.] http://en.wikipedia.org/wiki/Automated_planning.   [4] Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based  Strategy Game with Plan Adaptation and Ontology-based  retrieval.  [5] K. Erol, J. Hendler, and D. Nau (1994). Semantics for   hierarchical task-network planning. Technical Report TR-94- 31, UMIACS.   [6] Smith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning  approach decrarer play in contract bridge. Computational  Intelligence. 1996, Vol. 12, 1.   [7] One Jump Ahead: Challenging Human Supremacy in   Checkers. J.Schaeffer. s.l. : Springer-Verlag, 1997.   [8]  IBM. How Deep Blue works. [Online] 1997. [Cited: April  23, 2008.]  http://www.research.ibm.com/deepblue/meet/html/d.3.2.html  [9] Ghallab, Malik, Nau, Dana and Traverso, Paolo.  Automated Planning theory and practice. s.l. : Morgan  Kaufmann Publishers, May 2004. ISBN 1-55860-856-7.   [10] Case Based Reasoning. Experiences, Lessons and Future.  Leake, David. s.l. : AAAI Press. MIT Press., 1997.   [11] Applying case-based reasoning: techniques for enterprise   systems. Watson, I. San Francisco, CA, USA : Morgan  Kaufmann Publishers Inc., 1998.   [12] Plaza, A. Aamodt and E. Case-based reasoning:   Foundational issues, methodological. AI Communications.  1994, 7(i).   [13] Tic-tac-toe. Wikipedia. [Online] [Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Tic-tac-toe.   [14] D\u00edaz-Agudo, B. and Gonz\u00e1lez-Calero, P. A. An   architecture for knowledge intensive CBR systems. Advances  in Case-Based Reasoning \u2013 (EWCBR\u201900). New York :  Springer-Verlag, Berlin Heidelberg, 2000.   [15] Ilghami, Okhtay and Nau, Dana S. A General Approach to   Synthesize Problem-Specific Planners. 2003.   3rd International Conference on Digital Interactive Media in Entertainment and Arts",
  "references": [
    "1] Minimax. Wikipedia. ",
    "Online] ",
    "Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Minimax.   ",
    "2] Von Neumann, J: Zur theorie der gesellschaftsspiele Math.   Annalen. 100 (1928) 295-320   ",
    "3] Automated Planning. Wikipedia. ",
    "Online] ",
    "Cited: April 23,   2008.] http://en.wikipedia.org/wiki/Automated_planning.   ",
    "4] Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based  Strategy Game with Plan Adaptation and Ontology-based  retrieval.  ",
    "5] K. Erol, J. Hendler, and D. Nau (1994). Semantics for   hierarchical task-network planning. Technical Report TR-94- 31, UMIACS.   ",
    "6] Smith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning  approach decrarer play in contract bridge. Computational  Intelligence. 1996, Vol. 12, 1.   ",
    "7] One Jump Ahead: Challenging Human Supremacy in   Checkers. J.Schaeffer. s.l. : Springer-Verlag, 1997.   ",
    "8]  IBM. How Deep Blue works. ",
    "Online] 1997. ",
    "Cited: April  23, 2008.]  http://www.research.ibm.com/deepblue/meet/html/d.3.2.html  ",
    "9] Ghallab, Malik, Nau, Dana and Traverso, Paolo.  Automated Planning theory and practice. s.l. : Morgan  Kaufmann Publishers, May 2004. ISBN 1-55860-856-7.   ",
    "10] Case Based Reasoning. Experiences, Lessons and Future.  Leake, David. s.l. : AAAI Press. MIT Press., 1997.   ",
    "11] Applying case-based reasoning: techniques for enterprise   systems. Watson, I. San Francisco, CA, USA : Morgan  Kaufmann Publishers Inc., 1998.   ",
    "12] Plaza, A. Aamodt and E. Case-based reasoning:   Foundational issues, methodological. AI Communications.  1994, 7(i).   ",
    "13] Tic-tac-toe. Wikipedia. ",
    "Online] ",
    "Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Tic-tac-toe.   ",
    "14] D\u00edaz-Agudo, B. and Gonz\u00e1lez-Calero, P. A. An   architecture for knowledge intensive CBR systems. Advances  in Case-Based Reasoning \u2013 (EWCBR\u201900). New York :  Springer-Verlag, Berlin Heidelberg, 2000.   ",
    "15] Ilghami, Okhtay and Nau, Dana S. A General Approach to   Synthesize Problem-Specific Planners. 2003.   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f"
  ],
  "url": "https://drive.google.com/uc?id=1asTkg4Uf6_MHZeqZ642nPzp-IsQ4mTKM",
  "date": "2024-01-09 18:19:01",
  "is_published": false
}
{
  "title": "Interactive and Adaptable Media",
  "authors": [
    "295",
    "AI Model for Computer games based on Case Based",
    "Vlado Menkovski",
    "0.8km Markopoulou Ave.",
    "Peania, 19002, Greece",
    "Dimitrios Metafas",
    "0.8km Markopoulou Ave.",
    "Peania, 19002, Greece"
  ],
  "institutions": [
    "Reasoning and AI Planning",
    "Athens Information Technology",
    "Athens Information Technology"
  ],
  "abstract": "",
  "keywords": [
    "Game AI",
    " Case Based Reasoning",
    " AI Planning",
    " Game Trees"
  ],
  "article": "The  goal  of  this  effort  is  to  explore  a  model  for  design  and  implementation of an AI agent for turn based games. This model  provides for building more capable computer opponents that rely  on  strategies  that  closely  resemble  human  approach  in  solving  problems opposed to classical computational centric heuristics in  game  AI.  In  this  manner  the  computational  resources  can  be  focused on more sensible strategies for the game play.    Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not  made  or  distributed  for  profit  or  commercial  advantage  and  that  copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy  otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires prior specific permission and/or a fee.  DIMEA\u201908, September 10\u201312, 2008, Athens, Greece.  Copyright 2008 ACM 978-1-60558-248-1/08/09... $5.00   With  the  advancement  in  computer  hardware  increasingly  more  computing  power  is  left  for  executing  AI  algorithms  in  games.  In  the  past  AI  in  games  was  mainly  a  cheating  set  of  instructions  that  simulated  the  increasing  difficulty  in  the  game  environment so that the player had the illusion of real counterpart.  Improvement  in  available  memory  and  processing  power  allows  implementation  of  more  intelligent  algorithms  for  building  the  game  environment  as  well  as  direct  interaction  with  the  human  players.     in  games  with   In  this  particular  research  the  emphasis  is  put  on  the  interaction  between  the  AI  agent  and  a  computer  player  in  the  realm  of  the  game  rules.  It  is  particularly  focused  on  turn  based  games that have the elements of uncertainty like dice or concealed  information.  At  the  beginning  a  description  of  Game  AI  algorithms  are  given;  such  as  Game  Trees  and  Minimax.  The  following  section  describes  an  approach  of  using  AI  Planning  to  improve  building  Game  Trees  imperfect  information  where  Game  Trees  tend  to  be  very  large  with  high  growth ratio. Section 4 discusses another approach that provides a  significant reduction to the number of considered moves in order  to find the favorable strategy of the AI player. This approach uses  AI Planning techniques and Case Base Reasoning (CBR) to plan  for different scenarios in predetermined strategies which would be  analogous to human player experience in the particular game. The  CBR  database  illustrates  a  set  of  past  experiences  for  the  AI  problem and the AI Planning illustrates the procedure to deal with  the  given  situation  in  the  game.  In  the  next  two  sections  implementations  and  evaluations  of  both  approaches  are  given.  The  AI  Planning  approach  is  implemented  with  the  Tic-tac-toe  game  and  the  combined  AI  Planning  and  CBR  approach  is  implemented with a model for the Monopoly game. The last part  contains conclusions and future work ideas.    2. Game Trees and Minimax   Game Trees are common model for evaluating how different  combinations  of  moves  from  the  player  and  his  opponents  will  affect  the  future  position  of  the  player  and  eventually  the  end  result of the game. An algorithm that decides on the next move by  evaluating  the  results  from  the  built  Game  Tree  is  minimax  [1].  Minimax assumes that the player at hand will always choose the  best possible move  for him, in other words the player  will try to  select  the  move  that  maximizes  the  result  of  the  evaluation  function over the game state. So basically the player at hand needs  to choose the best move overall while taking into account that the  next  player(s)  will  try  to  do  the  same  thing.  Minimax  tries  to  maximize the minimum gain. Minimax can be applied to multiple   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f296  DIMEA 2008  levels of nodes on the game tree, where the leaves bring the final  known (or considered) game state.    The minimax theorem states:   For  every  two-person,  zero-sum  game  there  is  a  mixed  strategy  for each player, such that the expected payoff for both is the same  value V when the players use these strategies. Furthermore, V is  the  best  payoff  each  can  expect  to  receive  from  a  play  of  the  game; that is, these mixed strategies are the optimal strategies for  the two players.   This  theorem  was  established  by  John  von  Neumann,  who  is  quoted as saying \"As far as I can see, there could be no theory of  games  \u2026  without  that  theorem  \u2026  I  thought  there  was  nothing  worth publishing until the Minimax Theorem was proved\" [2].   A simple example of minimax can be observed by building a  game tree of the tic-tac-toe game. The tic-tac-toe game is a simple  game which can end by the first player wining, the second player  wining or a tie. There are nine positions for each of the players in  which at each turn the player puts X or O sign. If the player has  three adjacent signs in a row, column or the two diagonals he or  she wins. This game has limited number of position and it is well  suited  for  building  the  whole  game  tree.  The  leaves  of  this  tree  will  be  final  positions  in  the  game.  A  heuristics  evaluation  function will also need to be written to evaluate the value of each  node along the way.  3. AI Planning for building Game Trees  3.1.1 AI Planning   AI  Planning  also  referred  as  Automated  Planning  and  Scheduling  is  a  branch  of  Artificial  Intelligence  that  focuses  on  finding strategies or sequences of actions that reach a predefined  goal  [3].  Typical  execution  of  AI  Planning  algorithms  is  by  intelligent  agents,  autonomous  robots  and  unmanned  vehicles.  Opposed to classical control or classification AI Planning results  with  complex  solutions  that  are  derived  from  multidimensional  space.    AI Planning algorithms are also common in the video game  development.  They  solve  broad  range  of  problems  from  path  finding to action planning. A typical planner takes three inputs: a  description  of  the  initial  state  of  the  world,  a  description  of  the  desired  goal,  and  a  set  of  possible  actions.  Some  efforts  for  incorporating  planning  techniques  for  building  game  trees  have  also shown up, similar to the approach explored in this effort. In  addition Cased Based Reasoning [4] techniques are also gathering  popularity  in  developing  strategies  based  in  prior  knowledge  about  the  problems  in  the  games.  One  of  the  benefits  from  Hierarchical  Task  Network  (HTN)  [5]  planning  is  the  possibility  to  build  Game  Trees  based  on  HTN  plans;  this  method  is  described in the following section.   3.2 Game Trees with AI Planning   An  adaptation  of  the  HTN  planning  can  be  used  to  build  much smaller and more efficient game trees. This idea has already  been  implemented  in  the  Bridge  Baron  a  computer  program  for  the game of Contact Bridge [6].   Computer  programs  based  on  Game  Tree  search  techniques  are  now  as  good  as  or  better  than  humans  in  many  games  like  Chess  [7]  and  checkers  [8],  but  there  are  some  difficulties  in  building  a  game  tree  for  games  that  have  imperfect  information  and  added  uncertainty  like  card  or  games  with  dice.  The  main   problem  is  the  enormous  number  of  possibilities  that  the  player  can  choose  from  in  making  his  move.  In  addition  some  of  the  moves  are  accompanied  with  probabilities  based  on  the  random  elements  the  games.  The  number  of  possible  moves  exponentially  grows  with  each  move  so  the  depth  of  the  search  has  to  be  very  limited  to  accommodate  for  the  memory  limitations.    in   The basic idea behind using HTN for building game trees is  that  the  HTN  provides  the  means  of  expressing  high  level  goals  and  describing  strategies  how  to  reach  those  goals.  These  goals  may be decomposed in goals at lower level called sub-goals. This  approach  closely  resembles  the  way  a  human  player  usually  addresses a complex problem. It is also good for domains  where  classical search for solution is not feasible due to the vastness of  the problem domain or uncertainties.   3.2.1 Hierarchical Task Networks   The  Hierarchical  Task  Network,  or  HTN,  is  an  approach  to  automated  planning  in  which  the  dependency  among  actions  can  be given in the form of networks [9] [Figure 1].   A simple task network (or just a task network for short) is an  acyclic  digraph  (cid:2) (cid:3) (cid:4)(cid:5)(cid:6) (cid:7)(cid:8)  in  which  U  is  the  node  set,  E  is  the  edge set, and each node (cid:9) (cid:10) (cid:5) contains a task (cid:11)(cid:12). The edges of (cid:2) define a partial ordering of U. If the partial ordering is total, then  we say that (cid:2) is totally ordered, in which case (cid:2) can be written as  a sequence of tasks (cid:2) (cid:3) (cid:13)(cid:11)(cid:14)(cid:6) (cid:11)(cid:15)(cid:6) (cid:16) (cid:6) (cid:11)(cid:17)(cid:18).  Buy milk  Go to (shop)  Purchase   Go to (home)  Figure 1: Simple Hierarchical Task Network   A  Simple  Task  Network  (STN)  method  is  a  4-tuple  of  its  name,  task,  precondition  and  a  task  network.  The  name  of  the  method  lets  us  refer  unambiguously  to  substitution  instances  of  the  method,  without  having  to  write  the  preconditions  and  effects  explicitly.  The  task  tells  what  kind  of  task  can  be  applied  if  the  preconditions  are  met.  The  preconditions  specify  the  conditions  that the current state needs to satisfy in order for the method to be  applied.  And  the  network  defines  the  specific  subtasks  to  accomplish in order to accomplish the task.   A  method  is  relevant  for  a  task  if  the  current  state  satisfies  the  preconditions of a method that implements that task. This task can  be  then  substituted  with  the  instance  of  the  method.  The  substitution  is  basically  giving  the  method  network  as  a  solution  for the task.   If  there  is  a  task  \u201cGo  home\u201d  and  the  distance  to  home  is  3km  [Figure 2] and there exists a method walk-to and this method has a  precondition that the distance is less than 5km, then a substation  to the task \u201cGo home\u201d can be made with this method instance.    Go-to (from, to)  If (to \u2013 from) < 5km   Walk (to)  Figure 2: HTN Method   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \fInteractive and Adaptable Media  297  If the distance is larger than 5km  another meth to be substituted [Figure 3].   hod instance needs   Go-to (from, to)   If(to \u2013 from) < 5km   If(t  to \u2013 from) < 200km   Walk (to)   Drive(to  )  Figure 3: HTN Method 2   An STN planning domain is a set of operatio methods  M.  A  STN  planning  problem  is  a  4-tu state  S0,  the  task  network  w  called  initial  task STN domain. A plan (cid:19) (cid:3) (cid:13)(cid:20)(cid:14)(cid:6) (cid:16) (cid:6) (cid:20)(cid:21)(cid:18) is a soluti problem if there is a way to decompose w into \u03c0 and  each  decomposition  is  applicable  in  the  ap the  world.  The  algorithm  that  is  capable  to  networks into plans is called Total-forward-deco [9]  or  Partial-forward-decomposition  (PFD).  H cases  where  one  does  not  want  to  use  a  forwa procedure. HTN planning is generalization of S gives  the  planning  procedure  more  freedom construct the task networks.    ons O and a set of  uple  of  the  initial  k  network  and  the  ion for a planning  \u03c0 if \u03c0 is executable  ppropriate  state  of  decompose  these  omposition (TFD)  However  there  are  ard-decomposition  STN planning that  m  about  how  to   In order to provide this freedom, a bookke is needed to represent constraints that the plann not  yet  enforced.  The  bookkeeping  is  done  by unenforced constraints explicitly in the task netw  eeping mechanism  ning algorithm has  y  representing  the  work.   The  HTN  generalizes  the  definition  of  a STN. A task network is the pair (cid:2)  (cid:3)   (cid:4)(cid:5)(cid:6) (cid:23)(cid:8) w task  nodes  and  C is  a  set  of  constraints.  Eac specifies a requirement that must be satisfied by a solution to a planning problem.    a  task  network  in  where (cid:5) is a set of  h  constraint  in  C  y every plan that is   The  definition  of  a  method  in  HTN  also definition  used  in  STN  planning.  A  HTN  pla name,  task,  subtasks,  and  constraints.  The  s constraints form the task network. The HTN plan identical to STN planning domains except they u instead of STN methods.   o  generalizes  the  an  is  a  4-tuple  of  subtasks  and  the  nning domains are  use HTN methods   plan. The branches of the game tree rep the  methods.  Tignum2  applies  all  met state  of  the  world  to  produce  new continues  recursively  until  there  are  n have  not  already  been  applied  to  th world.    present moves generated by  thods  applicable  to  a  given  w  states  of  the  world  and  no  applicable  methods  that  he  appropriate  state  of  the   In the task network generated by Tignu actions will occur is determined by th By  listing  the  actions  in  the  order  network can be \u201cserialized\u201d into a gam  um2, the order in which the  e total-ordering constraints.  they  will  occur,  the  task  me tree [Figure 4] [Figure 5].   Figure 4: HTN to Game Tr  ree Algorithm  Figure 5: Game Tree built fr  rom HTN  Compared  to  classical  planners  the  prim HTN planners is their sophisticated knowledge r reasoning  capabilities.  They  can  represent  and  non-classical  planning  problems;  with  a  good guide them, they can solve classical planning p magnitude  more  quickly  than  classical  or  neoc The  primary  disadvantage  of  HTN  is  the  nee author to write not only a set of planning opera of methods.  3.2.2 HTN Planning in building Game   mary  advantage  of  representation and  solve  a  variety  of  d  set  of  HTNs  to  problems orders of  classical  planners.  ed  of  the  domain  ators but also a set   Trees  For  a  HTN  planning  algorithm  to  be  adap trees  we  need  to  define  the  domain  (set  of  H operators) which is the domain of the game. Thi a  knowledge  representation  of  the  rules  of  the environments and possible strategies of game pla  ted  to  build  game  HTN  methods  and  is is in some sense  e  game,  the  game  ay.  In this domain the game rules as well as kn tackle  specific  task  are  defined.      The  implem is  called  Tign Tree  building  with  HTN  implementation  uses  simila decomposition, but adapted to build up a game   nown strategies to  mentation  of  Game  [9].  This  num2  ar  forward- to  tree rather than a   a  procedure   4. Case Based Reasoning in 4.1 Case Based Reasoning Case-based reasoning (CBR) is a  Artificial  Intelligence  (AI),  both  as  problems and as a basis for standalone   n Game Strategies  well established subfield of  a  mean  for  addressing  AI  AI technology.  Case-based  reasoning  is  a  paradigm solving  and  learning  that has  became  applied  subfield  of  AI  of  recent  yea intuition that problems tend to recur. I are  often  similar  to  previously  en therefore, that past solutions may be of [10].    m  for  combining  problem- one  of  the  most  successful  ars.  CBR  is  based  on  the  It means that new problems  ncountered  problems  and,  f use in the current situation   CBR is particularly applicable to probl available,  even  when  the  domain  is  n for  a  deep  domain  model.  Helpdesks, systems  have  been  the  most  successfu to  determine  a  fault  or  diagnostic  attributes,  or  to  determine  whether  or repair is necessary given a set of past s  lems where earlier cases are  not  understood  well  enough  ,  diagnosis  or  classification  ul  areas  of  application,  e.g.,  an  illness  from  observed  r  not  a  certain  treatment  or  olved cases [11].   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f298  DIMEA 2008  Central tasks that all CBR methods have to deal with are [12]: \"to  identify  the  current  problem  situation,  find  a past  case  similar  to  the  new  one,  use  that  case  to  suggest  a  solution  to  the  current  problem, evaluate the proposed solution, and update the system by  learning from this experience. How this is done, what part of the  process  that  is  focused,  what  type  of  problems  that  drives  the  methods, etc. varies considerably, however\".    And  in  the  third  position  if  the  two  of  center,  middle  top  and  middle left are available the position is a certain victory.   There are many different arrangements of the player\u2019s tokens  that  give  equivalent  positions  as  these  three  positions.  By  using  planning we do not need to consider all possible layouts but just  consider these three similar to what a human would consider.   across   application   the  underlying   ideas  of  CBR  can  be  applied  While  consistently  specific  implementation  of  the  CBR  methods  \u2013in  particular  retrieval  and  similarity  functions\u2013  is  highly  customized  to  the  application  at  hand.  4.2 CBR and Games   domains,   the   Many  different  implementations  of  CBR  exist  in  games.  CBR  technology  is  nicely  suited  for  recognizing  complex  situations much easier and more elegant than traditional parameter  comparison  or  function  evaluation.  There  are  especially  evident  cases in real time strategies where different attack and defense of  global strategies are nicely defined by CBR datasets and later used  in  the  running  games.  Also  intelligent  bots  behavior  is  also  another typical example. Depending on the number of enemy bots  the  layout  of  the  terrain  and  position of  human  players  the  CBR  system  finds  the  closest  CBR  case  and  employs  that  strategy  against the human players which in prior evaluation was proved to  be highly efficient.   5. Game Trees with AI Planning \u2013 Tic-tac-toe  In  order  to  show  the  expressive  power  of  AI  Planning  in  defining strategies  for games, and the use of these plans to build  Game  Trees I implemented an algorithm that builds Game  Trees  for the Tic-Tac-Toe game.   The  game  tree  of  Tic-Tac-Toe  shows  255,168  possible  games  of  which  131,184  are  won  by  X  (the  first  player),  77904  are won by O and the rest 46,080 are draw [13]. All these games  can be derived from building a complete Game Tree.    Even  though  it  is possible  to  build  a  complete  game  tree  of  Tic-tac-toe  it  is  definitely  not  an  optimal  solution.  Many  of  the  moves in this tree would be symmetrical and also there are a many  moves  that  would  be  illogical  or  at  least  a  bad  strategy  to  even  consider.    So what strategy  should X (the first player) choose in order   to win the game?   There  are  few  positions  that  lead  to  certain  victory.  These  positions  involve  simultaneous  attack  on  two  positions  so  the  other player could not defend, basically the only trick in Tic-Tac- Toe.   Figure 6: Tic-tac-toe winning strategy positions   Position 1 leads to victory if the two of the three fields: top  middle,  bottom  left  corner  and  bottom  right  corner  are  free  [Figure 6].   Position 2 lead to victory if two of the three fields: top right  corner, bottom right corner and bottom middle are free [Figure ].    The game starts from an empty table.   The two relevant strategies that would lead to these positions   are to take one corner or to take the center [Figure 7].   Figure 7: Tic-tac-toe Two starting moves   The  center  position  as  we  can  see  in  the  simulation  results  lead  to  a  bigger  number  of  victorious  endings  but  it  is  also  a  straight forward strategy with obvious defense strategy.   At this point we need to consider the moves of the opponent.  If  we take the left branch the opponent moves can be a center, a  corner  or  a  middle  field.  We  also  need  to  differentiate  with  a  move to a corner adjacent with our like top left or bottom right or  across the center to bottom right [Figure 8].   Figure 8: Tic-tac-toe opponent response to corner move   In  cases  one  and  two,  we  have  a  clear  path  to  executing  strategy  3  so  we  need  to  capture  the  diagonally  opposite  field.  And as for the third case the best way to go is to capture the center  and go for strategy 1 or 2 depending of the opponent\u2019s next move.    Figure 9: Tic-tac-toe move 2 after corner opening   The first move leads to certain victory, O will have to go to  the  center  and  X  will  achieve  strategy  3  [Figure  9].  The  second  move is a possible way to strategy 3 if O makes a mistake in the  next  loop,  so  X  goes  to  the  opposite  corner.  For  the  third  case  since  O  is  playing  a  valid  strategy  the  only  move  that  leaves  a  possible mistake from O would be to take the center and wait for  O to go to the middle and then achieve strategy 1 or 3 which will  be  a  symmetric  situation  to  the  one  that  we  will  find  if  we  branched with the center.   Figure 10: Tic-tac-toe opponent response to center move   If  we  go  back  to  the  second  branch  [Figure  10],  a  possible  way for the second player to engage is corner or middle. The first   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \fInteractive and Adaptable Media  299  move  is  a  valid  strategy  for  O  and  can  be  mee corner move from X to try a mistake from O in  the same as in the third case above from the pre another  move  would  be  go  to  the  middle  wh achieves strategy 1 or 2.    et  with  a  opposite  the future exactly  evious branch, and  here  X  eventually   This HTN when executed will re  game scenarios. By creating nodes from them with branches with the move of t tree for the Tic-tac-toe game over whi algorithm.   esult with plans for possible  m each position and linking  the player we create a game  ich we can run the minimax   Figure 11: Tic-tac-toe Move 2 after cent The fist move will lead to win if O moves  draw if it  goes  for the corners [Figure 11]. In t has  to  block  the  lower  left  corner  which  leave middle left or corner left which are strategy 1 and To sum the strategies for the planning, first  corner strategy for the beginning. Then for the ce the  corners  with  the  particularly  the  one  oppo holds. If the center is empty for the second strate we go for the opposite corner. After this point w opponent  or  try  to  implement  strategies  1,  2  or victory.   Plan 1: Take center   Preconditions: Center empty  Plan 2: Take corner   Preconditions: All corners empty  Plan 3: Take corner after center  Preconditions: We have center take corner oppos opponent has  Plan 4: Take diagonal corner  Preconditions: We have a corner, the opponent ha  the corner opposite to the one we have is free.  Plan 5: Block  Precondition: The opponent has tree tokens in a r agonal  Plan 6: Win  Preconditions: We have two tokens in a row, colu nd the third place is free  Plan 7: Tie  Preconditions: If all places are taken, it\u2019s a tie.  5.1 Hierarchical Task Network   ter opening to the middle or a  the second case O  es  X  to  go  for  the  d 2. we have center or  enter we try to get  osite  to  the  one  O  egy we go for it or  we either block the  r  3  which  lead  to   site to the  one the   as the ce\u2212nter and  row, colu\u2212mn or di  mn or dia\u2212gonal a  Top level task is Play [Figure 12]. This is a  can  be  derived  into:  Win,  Block,  Tie  or  Sear Search for plan is derived to both Plan 1 and Pla Plan 4, which later leads to a call for the oppon recursive call to Play.   a complex task and  rch  for  Plan.  The  an 2 or Plan 3 and  nent\u2019s move and a   Figure 12: Tic-tac-toe HT  TN  This set up with 7 plans with 3 ta for Tic-tac-toe which considers all pos player  with only 457 games, 281 of  w and  0  where  the  second  opponent  w reduction over the 255, 168 possible g tree. These reductions can be very use computing  capabilities  but  also  we  pr that planning can be very efficient if d trees  by  applying  reasoning  very  reasoning.   arget strategies creates a tree  ssible moves for the second  which X wins 176 are draw  wins.  This  is  a  significant  ames with a complete game  eful for devices with limited  rove  a  very  important  point  designing meaningful game  similar  to  human  player   me  tree  are  also  possible  if  d, in other words if we drop  moves of the opponent.   Further  improvements  to  the  gam the opponents moves are also planned all the meaningless and symmetrical m 6. Game AI in Monopoly  6.1 Overview of the AI Imp The  AI  agent  is  responsible  for  players in the game. The core principle a Game Tree with all the sensible move make  from  the  current  point  of  time minimax  algorithm  the  agent  selects  t would  bring  the  computer  player  mo with  the  highest  probability.  Building  that would be big enough to consider  is  obstructed  by  the  vastness  of  poss with all the possible random landings  nodes  of  the  game  tree  exponentially tackle  this  problem  the  AI  agents  discussed technologies: Case Based Re The  technologies  are  employed  First the agent searches the CBR datab largest similarity  with the current state associated  with  a  playing  strategy.  Th that the planner needs to build plans f consecutive player  moves that bring th way only moves that are part of that str being a small fraction of the overall po edges of the game tree at each level dec At each level of the game tree the of  a  single  player.  After  the  strateg considered the response to those strate by the opponent(s). The move of the  probability  distribution  of  the  dice  as  player.  A  more general strategy  needs opponent\u2019s  (human  player)  moves  sin the  expertise  of  the  opponent.  This  ge more plausible moves than the focused After  covering  all  opponents  t deducting  a  feature  move  of  the  com CBR  selected  plan  strategy.  After  strategies  and  reaching  a  reasonable  s into  account  the  memory  limits  an probabilities  that  the  move  is  possible the dice the building of the Game Tre algorithm  searches  the  Game  Tree  favorable  move  for  the  AI  player  usi The process is repeated each time the A  plementation the  moves  of  the  artificial  e of the AI agent is building  es that all the players would  e  forward.  Then  using  the  the  move  that  in  the  future  ost  favorable  game  position  a  Game  Tree  in  this  game  sufficient number of moves  sible  moves  in  combination  of the dice. The number of  y  grows  at  each  level.  To  incorporates  two  already   easoning and AI Planning.   in  the  following  manner.  base to find the case with the  e of the board. This case is  he  strategy  consists  of  goal  for, and the plans consist of  he player to that goal. This  rategy are considered, those  ossible moves the number of  creases immensely.  e model considers the moves  gies  of  the  AI  player  are  egies needs to be considered  opponent(s) depends of the  well  as  the  strategy  of  the  s to be implemented for the  nce  we  cannot  be  aware  of  eneral  strategy  would  bring  d strategy of the AI player.   the  agent  comes  back  to  mputer  player  by  using  the  creating  several  loops  of  size  of  a  Game  Tree  taking  nd  the  rapidly  decreasing  e  due  to  the  distribution  of  ee stops. Then the minimax  and  decides  on  the  most  ing  the  minimax  algorithm.  AI player is up.   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f300  DIMEA 2008  On  the  other  hand  the  MonopolySolution  class  holds  the  three  particular  attributes  that  are  needed  for  the  planning,  the  planning Domain, State and TaskList.   The  game  is  implemented  by  using  the  Model-View- Controller  software  development  pattern.  The  controller  is  responsible  for  implementing  the  game  rules  and  handling  all  of  the  events  in  the  game  like  roll  of  dice,  input  commands  for  trading,  auctioning  and  etc  from  the  players.  The  View  layer  is  responsible  for  displaying  the  board  and  all  of  the  input  widgets  on  to  the  game  screen,  and  the  models  are  data  structures  representing the game state [Figure 14].   Buying,  auctioning  and  trading  game  moves  are  always  accompanied  by  return  of  investment  calculations  in  making  the  plans. These calculations represent adaptation of the more general  planning  associated  with  the  cases  in  the  CBR  database.  These  adaptations  are  necessary  due  to  the  fact  that  the  cases  do  not  identically  correspond  to  the  situation  on  the  table.  In  addition  calculating the game position value of each node of the game tree  is  done  by  heuristic  functions  incorporate  economic  calculations of net present value, cash, and strategic layout and so  on.  For  example  railroads  in  monopoly  are  known  to  be  strategically  effective  because  they  bring  constant  income  even  though  the  income  can  be  smaller  than  building  on  other  properties.   6.2 Details on the CBR Implementation   that   The  implementation  of  the  CBR  is  by  using  the  JColibri2  platform.    JColibri2  is  an  object-oriented  framework  in  Java  for  building  CBR  systems  that  is  an  evolution  of  previous  work  on  knowledge intensive CBR [14].    For this implementation we need to look into three particular  classes  of  the  JColibri2  platform.  The  StandardCBRApplication,  Connector,  CBRQuery.  For  a  JColibri2  implementation  the  StandardCBRApplication interface needs to be implemented.    The CBR cycle executed accepts an instance of CBRQuery.  This  class  represents  a  CBR  query  to  the  CBR  database.  The  description  component  (instance  of  CaseComponent)  represents  the description of the case that will be looked up in the database.  All  the  solutions  case  CaseComponent interface.   implementing   cases   and   are   The  JColibri2  platform  connects  to  the  CBR  database  via  a  Connector  class.  Each  connector  implements  all  the  necessary  methods for accessing the database, retrieval of cases, storing and  deletion  of  cases.  This  implementation  uses  a  custom  XML  structure  for  holding  the  CBR  cases.  Since  the  game  will  not  update  the  CBR  database  only  read  it,  a  XML  solution  satisfies  the needs. The XML file to a certain extent is similar to the XML  representation  of  the  board.  We  are  interested  in  finding  one  CBRCase that is the most similar case to the situation in the game  at  the  time  of  the  search.  This  procedure  is  done  in  the  cycle  method of the CBRApplication. The JColibri2 CBR comparison is  done by Nearest Neighbor (NN) search method.    JColibri2  offers  implementations  for  NN  search  algorithms  of  simple  attributes.  These  implementations  are  called  local  similarities.  For  complex  attributes  like  in  our  case  global  customized similarity mechanisms need to be implemented.   The  MonopolyDescription  class  [Figure  13]  is  basically  a  serialization of the  GameState. It holds all the information about  the state of the board, the players, their amount of cash etc.    Figure 13: Class diagram of the Monopoly Case component  models   Figure 14: Class diagram of the Monopoly models   6.2.1 Complex Similarity representation in CBR   implementing   The  similarity  measurement  part  of  the  Nearest  Neighbor  implemented  by  the  algorithm  JColibri2  is  LocalSimiralrityFunction  the  GlobalSimiralityFunction  and  interface. A local similarity function is applied to simple attributes  by the NN algorithm, and a global similarity function is applied to  compound  attributes.  In  the  case  of  our  implementation  the  attributes  of  the  MonopolyDescription  are  compound  attributes  describing  the  state  of  the  board,  number  of  players,  amount  of  cash  for  every  player  and  etc.  Since  MonopolyDescription  is  a  custom  CaseComponent  a  global  similarity  function  needs  to  be  implemented  to  accurately  find  the  distance  between  different  CBR cases.   The similarity mechanism is inseparable core element of the  CBR  system.  This  mechanism  represents  how  the  CBR  decides  which  strategy  is  best  suited  for  the  particular  situation  by   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \fInteractive and Adaptable Media  301  calculating  the  distance  or  similarity  to  other  cases  in  the  database.    For  the  monopoly  implementation  we  need  to  consider  several  basic  strategies.  Monopoly  is  based  on  investing  in  properties and receiving revenues from those investments. One of  the basic strategies of the game is to build a set of properties that  will  bring  constant  income  larger  than  the  one  of  the  opponents.  So in time the opponents will have to declare bankruptcy. But on  the other hand over investment can lead to too stretched resources  with  low  income  that  will  eventually  drove  the  player  to  bankruptcy.  To  decide  on  these  two  we  need  a  clear  separation  into two groups of cases in the CBR database. The first group of  cases will represent a situation on the board where the player has  significant  income  per  loop  formed  of  one  or  more  color  group  properties, maybe railroads, some buildings on them and so on. It  is  important  to  note  that  in  this  case  the  player  is  better  situated  than his opponents so he only needs to survive long enough to win  the  game.  In  the  other  group  of  cases  either  the  opponent  is  not  well  positioned on  the board  or  its  opponents  are better  situated.  In  this  case  further  investments  are  necessary  to  improve  the  situation  so  the  player  can  have  a  chance  of  winning  in  the  long  run.    These metrics can be owning color groups, valuing groups of  railroads, evaluating the other opponents as well, and considering  the amount of cash. As it is obvious in monopoly the number of  streets is not as nearly as important as the combination of streets  the  player  owns.  It  is  also  important  to  note  that  one  CBR  case  does not hold only a single strategy in place, but its solution can  have multiple different strategic goals. For example one CBR case  might simultaneously say buy this land to form a color group but  also  trade  some  other  unimportant  property  to  increase  cash  amount.    The cases do not represent all possible combinations of board  positions. They are only representation of typical game scenarios.  The CBR Case solutions do not give exact instructions in general  but  rather  strategic  goals.  For  example  one  CBR  Solution  might  say trade the streets that  you only have one of  each  for the ones  that you have two of that color already. Then the planner based on  the situation on the board needs to decompose this high level task  to a low level operations. Like offer \"Mediterranean Avenue\" for  \"Reading Railroad\" and offer $50. The exact amounts and actual  streets are left to the planer to evaluate.    The monopoly CBR database is currently in development on  a  monopoly  clone  game  called  Spaceopoly.  The  cases  are  architected  based  on  human  player  experience  and  knowledge.  There is a plan of making a number of slightly different strategies  that  differ  on  the  style  of  playing  and  then  running  simulation  tests that would determine the particular validity of each database  as  well  as  validity  of  certain  segments  of  the  strategy  or  even  particular cases in the database.    JSHOP2  uses  ordered  task  decomposition  in  reducing  the  HTN to list of primitive tasks  which form the plans.  An ordered  task decomposition planner is an HTN planner that plans for tasks  in  the  same  order  that  they  will  be  executed.  This  reduces  the  complexity  of  reasoning  by  removing  a  great  deal  of  uncertainty  about  the  world,  which  makes  it  easy  to  incorporate  substantial  expressive  power  into  the  planning  algorithm.  In  addition  to  the  usual HTN methods and operators, the planners can make use of  axioms,  can  do  mixed  symbolic/numeric  conditions,  and  can  do  external function calls.    In  order  for  the  JSHOP2  planer  to  generate  plans  it  needs  tree  crucial  components:  Domain,  State  and  Tasks.  The  Domain  defines  all  the  functionalities  that  the  particular  domain  offers.  These  are  simple  and  complex  tasks.  The  complex  tasks  also  called methods create the hierarchy with the fact that they can be  evaluated  by  simple  tasks  of  other  complex  tasks.  This  is  how  a  hierarchical structure of tasks is formed. The problem reduction is  done by reducing the high level complex tasks to simpler until all  the tasks are primitive. The list of primitive tasks forms the plan.   The  State  represents  the  state  of  the  system.  It  is  a  simple  database of facts that represent the state of the system. The State  is  necessary  to  determine  the  way  the  problems  or  tasks  are  reduced  to  their  primitive  level.  The  reduction  is  done  by  satisfying  different  prerequisites  set  in  the  methods;  these  prerequisites  are  defined  in  the  state.  The  Tasks  are  high  level  tasks or methods defined in the Domain. The planner based on the  State and the goals selects one or more high level tasks that need  to be reduced to plans [Figure  15].   Tasks  Core Planner   State  Plan  Figure 15: Diagram of a Planner   The  plans  then  generate  the  game  moves.  The  number  of  moves  generated  by  the  plans  is  just  a  fraction  of  the  possible  moves  at  that  point.  This  reduces  the  game  tree  providing  the  opportunity to generate smaller and deeper game trees and making  more efficient decisions in general.    7. Conclusion   Even  though  the  results  from  the  CBR  database  are  not  complete  at  this  time  partial  strategies  are  implemented  as  cases  and  recognized  during  game  play  by  the  CBR  system.  These  smaller  local  strategies  coupled  with  more  global  higher  level  strategies  that  are  particularly  important  at  the  beginning  of  the  game  would  form  a  complete  CBR  database  and  represent  a  knowledge engineered style of playing of the AI player.    The  actual  execution  of  the  strategies  will  not  differ  from  strategy to strategy since the plan execution is more related to the  structure and rules of the game than to the actual playing strategy.   The AI Planning approach is a proven method by the tic-tac- toe  experiment  and  is  suitable  for  implementing  the  strategies  associated with the CBR cases.   6.3 Details on the Planning Implementation   For  the  purpose  of  planning  this  implementation  uses  a  the  JSHOP2  planner.  The  Java  Simple  modification  of  Hierarchical  Ordered  Planner  2  is  a  domain  independent  HTN  planning system [15].    This  approach  in  general  benefits  from  both  technologies,  CBR  as  well  as  AI  Planning  and  comprises  an  elegant  solution.  Even  though  AI  Planning  can  be  enough  as  a  single  technology  for  some  simpler  problems  like  tic-tac-toe  the  complexity  of  Monopoly would mean that the Planner would have to incorporate   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f302  DIMEA 2008  large and complex domain and a very big state model. The CBR  application helps reduce this complexity by focusing the planning  on  smaller  domain  of  the  game.  Basically  the  CBR  reduces  the  overall  goal  of  the  play  (wining  the  game)  to  smaller  more  concrete  goals  suitable  to  the  particular  state  of  the  game,  thus  reducing  the  need  for  global  planning  strategies  and  complex  planning domain.    Furthermore  this  symbiosis  of  technologies  gives  way  for  more precise and finely tuned strategies which can be difficult to  include into global plan for the whole game. One simple example  for  the  Monopoly  game  would  be  this:  Sometimes  it\u2019s  better  to  stay  in  jail  because  rolling  double  increases  the  probability  of  landing  on  some  field  (two,  four,  six,  eight,  ten  or  twelve  steps  from  the  jail)  that  can  be  of  great  importance  to  the  rest  of  the  game.  These  and  similar  small  local  strategies  can  be  easily  recognized by similar cases in the CBR database.    In  other  words  the  system  is  flexible  enough  so  that  new  strategies can be incorporated easily missing strategies can be also  recognized by the distance metrics as well as wrong assumptions  in the strategies can be easily recognized.   One other important property of  the system is that is highly  configurable.  The  game  its  self  can  be  diversely  different  depending  on  the  configuration  of  the  board.  Even  though  the  platform  is  restricted  to  Monopoly  type  of  games,  changing  the  layout  and  values  of  the  fields  effectively  brings  completely  different  properties  of  the  game.  In  addition  the  CBR  database  represents the entire experience  of the AI Player. It can be filled  with rich set of strategies or even configured with different flavors  of difficulties of play, this of course coupled with the domain of  the planner which can differ from a case to a case as well.    8. Future Work   Further  exploration  of  this  technology  would  go  towards  complete  implementation  of  an  AI  aware  agent  for  monopoly.  Initial  results  from  the  local  cases  with  more  specific  strategies  show CBR as a capable tool for representing expertise in playing  the  game.  Completing  the  more  general  strategies  and  coupling  them  with  the  planning  domain  will  give  precise  results  on  the  benefits from this architecture.   There is also need for exploring the planning of strategies of  opponents.  This  task  is  to  some  extent  different  because  we  cannot  always  expect  the  opponent  to  select  the  best  move  we  think.  In  the  Tic-tac-toe  example  all  possible  moves  of  the  opponent  were  taken  into  consideration,  if  we  used  the  same  planner  for  the  opponent  only  tie  games  would  result  from  the  game tree. In other words mistakes of the players also need to be  considered.    The CBR Platform brings other functionalities well worth of  exploring as well. The revision stage of the JColibri2 platform is  basically capable of fine tuning strategies or even developing new  strategies  for  the  games.  A  well  written  underlying  AI  planning  model  with a  capable  feedback of the  game tree evaluation back  to  the  CBR  revision  capability  can  be  an  interesting  concept  in  automatic experience acquisition for the AI model.   There  are  also  many  other  fields  were  combined  CBR  and  planning  approach  can  be  incorporated  into  a  problem  solution.  This combination is analogous in a big extent to a human way of   reasoning.  People  in  addition  to  logic  of  reasoning  in  situations  with  lack  of  information  rely  to  planning  strategies  and  prior  experience,  exactly  the  intuition  behind  CBR  \u2013  AI  Planning  architecture.    9. ACKNOWLEDGMENTS   We  would  like  to  thank  Prof.  Sofia  Tsekeridou  for  her  involvement  in  the  valuable  discussions  we  had  on  the  topic  of  CBR.   10. REFERENCES  [1] Minimax. Wikipedia. [Online] [Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Minimax.   [2] Von Neumann, J: Zur theorie der gesellschaftsspiele Math.   Annalen. 100 (1928) 295-320   [3] Automated Planning. Wikipedia. [Online] [Cited: April 23,   2008.] http://en.wikipedia.org/wiki/Automated_planning.   [4] Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based  Strategy Game with Plan Adaptation and Ontology-based  retrieval.  [5] K. Erol, J. Hendler, and D. Nau (1994). Semantics for   hierarchical task-network planning. Technical Report TR-94- 31, UMIACS.   [6] Smith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning  approach decrarer play in contract bridge. Computational  Intelligence. 1996, Vol. 12, 1.   [7] One Jump Ahead: Challenging Human Supremacy in   Checkers. J.Schaeffer. s.l. : Springer-Verlag, 1997.   [8]  IBM. How Deep Blue works. [Online] 1997. [Cited: April  23, 2008.]  http://www.research.ibm.com/deepblue/meet/html/d.3.2.html  [9] Ghallab, Malik, Nau, Dana and Traverso, Paolo.  Automated Planning theory and practice. s.l. : Morgan  Kaufmann Publishers, May 2004. ISBN 1-55860-856-7.   [10] Case Based Reasoning. Experiences, Lessons and Future.  Leake, David. s.l. : AAAI Press. MIT Press., 1997.   [11] Applying case-based reasoning: techniques for enterprise   systems. Watson, I. San Francisco, CA, USA : Morgan  Kaufmann Publishers Inc., 1998.   [12] Plaza, A. Aamodt and E. Case-based reasoning:   Foundational issues, methodological. AI Communications.  1994, 7(i).   [13] Tic-tac-toe. Wikipedia. [Online] [Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Tic-tac-toe.   [14] D\u00edaz-Agudo, B. and Gonz\u00e1lez-Calero, P. A. An   architecture for knowledge intensive CBR systems. Advances  in Case-Based Reasoning \u2013 (EWCBR\u201900). New York :  Springer-Verlag, Berlin Heidelberg, 2000.   [15] Ilghami, Okhtay and Nau, Dana S. A General Approach to   Synthesize Problem-Specific Planners. 2003.   3rd International Conference on Digital Interactive Media in Entertainment and Arts",
  "references": [
    "1] Minimax. Wikipedia. ",
    "Online] ",
    "Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Minimax.   ",
    "2] Von Neumann, J: Zur theorie der gesellschaftsspiele Math.   Annalen. 100 (1928) 295-320   ",
    "3] Automated Planning. Wikipedia. ",
    "Online] ",
    "Cited: April 23,   2008.] http://en.wikipedia.org/wiki/Automated_planning.   ",
    "4] Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based  Strategy Game with Plan Adaptation and Ontology-based  retrieval.  ",
    "5] K. Erol, J. Hendler, and D. Nau (1994). Semantics for   hierarchical task-network planning. Technical Report TR-94- 31, UMIACS.   ",
    "6] Smith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning  approach decrarer play in contract bridge. Computational  Intelligence. 1996, Vol. 12, 1.   ",
    "7] One Jump Ahead: Challenging Human Supremacy in   Checkers. J.Schaeffer. s.l. : Springer-Verlag, 1997.   ",
    "8]  IBM. How Deep Blue works. ",
    "Online] 1997. ",
    "Cited: April  23, 2008.]  http://www.research.ibm.com/deepblue/meet/html/d.3.2.html  ",
    "9] Ghallab, Malik, Nau, Dana and Traverso, Paolo.  Automated Planning theory and practice. s.l. : Morgan  Kaufmann Publishers, May 2004. ISBN 1-55860-856-7.   ",
    "10] Case Based Reasoning. Experiences, Lessons and Future.  Leake, David. s.l. : AAAI Press. MIT Press., 1997.   ",
    "11] Applying case-based reasoning: techniques for enterprise   systems. Watson, I. San Francisco, CA, USA : Morgan  Kaufmann Publishers Inc., 1998.   ",
    "12] Plaza, A. Aamodt and E. Case-based reasoning:   Foundational issues, methodological. AI Communications.  1994, 7(i).   ",
    "13] Tic-tac-toe. Wikipedia. ",
    "Online] ",
    "Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Tic-tac-toe.   ",
    "14] D\u00edaz-Agudo, B. and Gonz\u00e1lez-Calero, P. A. An   architecture for knowledge intensive CBR systems. Advances  in Case-Based Reasoning \u2013 (EWCBR\u201900). New York :  Springer-Verlag, Berlin Heidelberg, 2000.   ",
    "15] Ilghami, Okhtay and Nau, Dana S. A General Approach to   Synthesize Problem-Specific Planners. 2003.   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f"
  ],
  "url": "https://drive.google.com/uc?id=1asTkg4Uf6_MHZeqZ642nPzp-IsQ4mTKM",
  "date": "2024-01-09 18:22:20",
  "is_published": false
}
{
  "title": "Interactive and Adaptable Media",
  "authors": [
    "295",
    "AI Model for Computer games based on Case Based",
    "Vlado Menkovski",
    "0.8km Markopoulou Ave.",
    "Peania, 19002, Greece",
    "Dimitrios Metafas",
    "0.8km Markopoulou Ave.",
    "Peania, 19002, Greece"
  ],
  "institutions": [
    "Reasoning and AI Planning",
    "Athens Information Technology",
    "Athens Information Technology"
  ],
  "abstract": "",
  "keywords": [
    "Game AI",
    " Case Based Reasoning",
    " AI Planning",
    " Game Trees"
  ],
  "article": "The  goal  of  this  effort  is  to  explore  a  model  for  design  and  implementation of an AI agent for turn based games. This model  provides for building more capable computer opponents that rely  on  strategies  that  closely  resemble  human  approach  in  solving  problems opposed to classical computational centric heuristics in  game  AI.  In  this  manner  the  computational  resources  can  be  focused on more sensible strategies for the game play.    Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not  made  or  distributed  for  profit  or  commercial  advantage  and  that  copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy  otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires prior specific permission and/or a fee.  DIMEA\u201908, September 10\u201312, 2008, Athens, Greece.  Copyright 2008 ACM 978-1-60558-248-1/08/09... $5.00   With  the  advancement  in  computer  hardware  increasingly  more  computing  power  is  left  for  executing  AI  algorithms  in  games.  In  the  past  AI  in  games  was  mainly  a  cheating  set  of  instructions  that  simulated  the  increasing  difficulty  in  the  game  environment so that the player had the illusion of real counterpart.  Improvement  in  available  memory  and  processing  power  allows  implementation  of  more  intelligent  algorithms  for  building  the  game  environment  as  well  as  direct  interaction  with  the  human  players.     in  games  with   In  this  particular  research  the  emphasis  is  put  on  the  interaction  between  the  AI  agent  and  a  computer  player  in  the  realm  of  the  game  rules.  It  is  particularly  focused  on  turn  based  games that have the elements of uncertainty like dice or concealed  information.  At  the  beginning  a  description  of  Game  AI  algorithms  are  given;  such  as  Game  Trees  and  Minimax.  The  following  section  describes  an  approach  of  using  AI  Planning  to  improve  building  Game  Trees  imperfect  information  where  Game  Trees  tend  to  be  very  large  with  high  growth ratio. Section 4 discusses another approach that provides a  significant reduction to the number of considered moves in order  to find the favorable strategy of the AI player. This approach uses  AI Planning techniques and Case Base Reasoning (CBR) to plan  for different scenarios in predetermined strategies which would be  analogous to human player experience in the particular game. The  CBR  database  illustrates  a  set  of  past  experiences  for  the  AI  problem and the AI Planning illustrates the procedure to deal with  the  given  situation  in  the  game.  In  the  next  two  sections  implementations  and  evaluations  of  both  approaches  are  given.  The  AI  Planning  approach  is  implemented  with  the  Tic-tac-toe  game  and  the  combined  AI  Planning  and  CBR  approach  is  implemented with a model for the Monopoly game. The last part  contains conclusions and future work ideas.    2. Game Trees and Minimax   Game Trees are common model for evaluating how different  combinations  of  moves  from  the  player  and  his  opponents  will  affect  the  future  position  of  the  player  and  eventually  the  end  result of the game. An algorithm that decides on the next move by  evaluating  the  results  from  the  built  Game  Tree  is  minimax  [1].  Minimax assumes that the player at hand will always choose the  best possible move  for him, in other words the player  will try to  select  the  move  that  maximizes  the  result  of  the  evaluation  function over the game state. So basically the player at hand needs  to choose the best move overall while taking into account that the  next  player(s)  will  try  to  do  the  same  thing.  Minimax  tries  to  maximize the minimum gain. Minimax can be applied to multiple   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f296  DIMEA 2008  levels of nodes on the game tree, where the leaves bring the final  known (or considered) game state.    The minimax theorem states:   For  every  two-person,  zero-sum  game  there  is  a  mixed  strategy  for each player, such that the expected payoff for both is the same  value V when the players use these strategies. Furthermore, V is  the  best  payoff  each  can  expect  to  receive  from  a  play  of  the  game; that is, these mixed strategies are the optimal strategies for  the two players.   This  theorem  was  established  by  John  von  Neumann,  who  is  quoted as saying \"As far as I can see, there could be no theory of  games  \u2026  without  that  theorem  \u2026  I  thought  there  was  nothing  worth publishing until the Minimax Theorem was proved\" [2].   A simple example of minimax can be observed by building a  game tree of the tic-tac-toe game. The tic-tac-toe game is a simple  game which can end by the first player wining, the second player  wining or a tie. There are nine positions for each of the players in  which at each turn the player puts X or O sign. If the player has  three adjacent signs in a row, column or the two diagonals he or  she wins. This game has limited number of position and it is well  suited  for  building  the  whole  game  tree.  The  leaves  of  this  tree  will  be  final  positions  in  the  game.  A  heuristics  evaluation  function will also need to be written to evaluate the value of each  node along the way.  3. AI Planning for building Game Trees  3.1.1 AI Planning   AI  Planning  also  referred  as  Automated  Planning  and  Scheduling  is  a  branch  of  Artificial  Intelligence  that  focuses  on  finding strategies or sequences of actions that reach a predefined  goal  [3].  Typical  execution  of  AI  Planning  algorithms  is  by  intelligent  agents,  autonomous  robots  and  unmanned  vehicles.  Opposed to classical control or classification AI Planning results  with  complex  solutions  that  are  derived  from  multidimensional  space.    AI Planning algorithms are also common in the video game  development.  They  solve  broad  range  of  problems  from  path  finding to action planning. A typical planner takes three inputs: a  description  of  the  initial  state  of  the  world,  a  description  of  the  desired  goal,  and  a  set  of  possible  actions.  Some  efforts  for  incorporating  planning  techniques  for  building  game  trees  have  also shown up, similar to the approach explored in this effort. In  addition Cased Based Reasoning [4] techniques are also gathering  popularity  in  developing  strategies  based  in  prior  knowledge  about  the  problems  in  the  games.  One  of  the  benefits  from  Hierarchical  Task  Network  (HTN)  [5]  planning  is  the  possibility  to  build  Game  Trees  based  on  HTN  plans;  this  method  is  described in the following section.   3.2 Game Trees with AI Planning   An  adaptation  of  the  HTN  planning  can  be  used  to  build  much smaller and more efficient game trees. This idea has already  been  implemented  in  the  Bridge  Baron  a  computer  program  for  the game of Contact Bridge [6].   Computer  programs  based  on  Game  Tree  search  techniques  are  now  as  good  as  or  better  than  humans  in  many  games  like  Chess  [7]  and  checkers  [8],  but  there  are  some  difficulties  in  building  a  game  tree  for  games  that  have  imperfect  information  and  added  uncertainty  like  card  or  games  with  dice.  The  main   problem  is  the  enormous  number  of  possibilities  that  the  player  can  choose  from  in  making  his  move.  In  addition  some  of  the  moves  are  accompanied  with  probabilities  based  on  the  random  elements  the  games.  The  number  of  possible  moves  exponentially  grows  with  each  move  so  the  depth  of  the  search  has  to  be  very  limited  to  accommodate  for  the  memory  limitations.    in   The basic idea behind using HTN for building game trees is  that  the  HTN  provides  the  means  of  expressing  high  level  goals  and  describing  strategies  how  to  reach  those  goals.  These  goals  may be decomposed in goals at lower level called sub-goals. This  approach  closely  resembles  the  way  a  human  player  usually  addresses a complex problem. It is also good for domains  where  classical search for solution is not feasible due to the vastness of  the problem domain or uncertainties.   3.2.1 Hierarchical Task Networks   The  Hierarchical  Task  Network,  or  HTN,  is  an  approach  to  automated  planning  in  which  the  dependency  among  actions  can  be given in the form of networks [9] [Figure 1].   A simple task network (or just a task network for short) is an  acyclic  digraph  (cid:2) (cid:3) (cid:4)(cid:5)(cid:6) (cid:7)(cid:8)  in  which  U  is  the  node  set,  E  is  the  edge set, and each node (cid:9) (cid:10) (cid:5) contains a task (cid:11)(cid:12). The edges of (cid:2) define a partial ordering of U. If the partial ordering is total, then  we say that (cid:2) is totally ordered, in which case (cid:2) can be written as  a sequence of tasks (cid:2) (cid:3) (cid:13)(cid:11)(cid:14)(cid:6) (cid:11)(cid:15)(cid:6) (cid:16) (cid:6) (cid:11)(cid:17)(cid:18).  Buy milk  Go to (shop)  Purchase   Go to (home)  Figure 1: Simple Hierarchical Task Network   A  Simple  Task  Network  (STN)  method  is  a  4-tuple  of  its  name,  task,  precondition  and  a  task  network.  The  name  of  the  method  lets  us  refer  unambiguously  to  substitution  instances  of  the  method,  without  having  to  write  the  preconditions  and  effects  explicitly.  The  task  tells  what  kind  of  task  can  be  applied  if  the  preconditions  are  met.  The  preconditions  specify  the  conditions  that the current state needs to satisfy in order for the method to be  applied.  And  the  network  defines  the  specific  subtasks  to  accomplish in order to accomplish the task.   A  method  is  relevant  for  a  task  if  the  current  state  satisfies  the  preconditions of a method that implements that task. This task can  be  then  substituted  with  the  instance  of  the  method.  The  substitution  is  basically  giving  the  method  network  as  a  solution  for the task.   If  there  is  a  task  \u201cGo  home\u201d  and  the  distance  to  home  is  3km  [Figure 2] and there exists a method walk-to and this method has a  precondition that the distance is less than 5km, then a substation  to the task \u201cGo home\u201d can be made with this method instance.    Go-to (from, to)  If (to \u2013 from) < 5km   Walk (to)  Figure 2: HTN Method   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \fInteractive and Adaptable Media  297  If the distance is larger than 5km  another meth to be substituted [Figure 3].   hod instance needs   Go-to (from, to)   If(to \u2013 from) < 5km   If(t  to \u2013 from) < 200km   Walk (to)   Drive(to  )  Figure 3: HTN Method 2   An STN planning domain is a set of operatio methods  M.  A  STN  planning  problem  is  a  4-tu state  S0,  the  task  network  w  called  initial  task STN domain. A plan (cid:19) (cid:3) (cid:13)(cid:20)(cid:14)(cid:6) (cid:16) (cid:6) (cid:20)(cid:21)(cid:18) is a soluti problem if there is a way to decompose w into \u03c0 and  each  decomposition  is  applicable  in  the  ap the  world.  The  algorithm  that  is  capable  to  networks into plans is called Total-forward-deco [9]  or  Partial-forward-decomposition  (PFD).  H cases  where  one  does  not  want  to  use  a  forwa procedure. HTN planning is generalization of S gives  the  planning  procedure  more  freedom construct the task networks.    ons O and a set of  uple  of  the  initial  k  network  and  the  ion for a planning  \u03c0 if \u03c0 is executable  ppropriate  state  of  decompose  these  omposition (TFD)  However  there  are  ard-decomposition  STN planning that  m  about  how  to   In order to provide this freedom, a bookke is needed to represent constraints that the plann not  yet  enforced.  The  bookkeeping  is  done  by unenforced constraints explicitly in the task netw  eeping mechanism  ning algorithm has  y  representing  the  work.   The  HTN  generalizes  the  definition  of  a STN. A task network is the pair (cid:2)  (cid:3)   (cid:4)(cid:5)(cid:6) (cid:23)(cid:8) w task  nodes  and  C is  a  set  of  constraints.  Eac specifies a requirement that must be satisfied by a solution to a planning problem.    a  task  network  in  where (cid:5) is a set of  h  constraint  in  C  y every plan that is   The  definition  of  a  method  in  HTN  also definition  used  in  STN  planning.  A  HTN  pla name,  task,  subtasks,  and  constraints.  The  s constraints form the task network. The HTN plan identical to STN planning domains except they u instead of STN methods.   o  generalizes  the  an  is  a  4-tuple  of  subtasks  and  the  nning domains are  use HTN methods   plan. The branches of the game tree rep the  methods.  Tignum2  applies  all  met state  of  the  world  to  produce  new continues  recursively  until  there  are  n have  not  already  been  applied  to  th world.    present moves generated by  thods  applicable  to  a  given  w  states  of  the  world  and  no  applicable  methods  that  he  appropriate  state  of  the   In the task network generated by Tignu actions will occur is determined by th By  listing  the  actions  in  the  order  network can be \u201cserialized\u201d into a gam  um2, the order in which the  e total-ordering constraints.  they  will  occur,  the  task  me tree [Figure 4] [Figure 5].   Figure 4: HTN to Game Tr  ree Algorithm  Figure 5: Game Tree built fr  rom HTN  Compared  to  classical  planners  the  prim HTN planners is their sophisticated knowledge r reasoning  capabilities.  They  can  represent  and  non-classical  planning  problems;  with  a  good guide them, they can solve classical planning p magnitude  more  quickly  than  classical  or  neoc The  primary  disadvantage  of  HTN  is  the  nee author to write not only a set of planning opera of methods.  3.2.2 HTN Planning in building Game   mary  advantage  of  representation and  solve  a  variety  of  d  set  of  HTNs  to  problems orders of  classical  planners.  ed  of  the  domain  ators but also a set   Trees  For  a  HTN  planning  algorithm  to  be  adap trees  we  need  to  define  the  domain  (set  of  H operators) which is the domain of the game. Thi a  knowledge  representation  of  the  rules  of  the environments and possible strategies of game pla  ted  to  build  game  HTN  methods  and  is is in some sense  e  game,  the  game  ay.  In this domain the game rules as well as kn tackle  specific  task  are  defined.      The  implem is  called  Tign Tree  building  with  HTN  implementation  uses  simila decomposition, but adapted to build up a game   nown strategies to  mentation  of  Game  [9].  This  num2  ar  forward- to  tree rather than a   a  procedure   4. Case Based Reasoning in 4.1 Case Based Reasoning Case-based reasoning (CBR) is a  Artificial  Intelligence  (AI),  both  as  problems and as a basis for standalone   n Game Strategies  well established subfield of  a  mean  for  addressing  AI  AI technology.  Case-based  reasoning  is  a  paradigm solving  and  learning  that has  became  applied  subfield  of  AI  of  recent  yea intuition that problems tend to recur. I are  often  similar  to  previously  en therefore, that past solutions may be of [10].    m  for  combining  problem- one  of  the  most  successful  ars.  CBR  is  based  on  the  It means that new problems  ncountered  problems  and,  f use in the current situation   CBR is particularly applicable to probl available,  even  when  the  domain  is  n for  a  deep  domain  model.  Helpdesks, systems  have  been  the  most  successfu to  determine  a  fault  or  diagnostic  attributes,  or  to  determine  whether  or repair is necessary given a set of past s  lems where earlier cases are  not  understood  well  enough  ,  diagnosis  or  classification  ul  areas  of  application,  e.g.,  an  illness  from  observed  r  not  a  certain  treatment  or  olved cases [11].   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f298  DIMEA 2008  Central tasks that all CBR methods have to deal with are [12]: \"to  identify  the  current  problem  situation,  find  a past  case  similar  to  the  new  one,  use  that  case  to  suggest  a  solution  to  the  current  problem, evaluate the proposed solution, and update the system by  learning from this experience. How this is done, what part of the  process  that  is  focused,  what  type  of  problems  that  drives  the  methods, etc. varies considerably, however\".    And  in  the  third  position  if  the  two  of  center,  middle  top  and  middle left are available the position is a certain victory.   There are many different arrangements of the player\u2019s tokens  that  give  equivalent  positions  as  these  three  positions.  By  using  planning we do not need to consider all possible layouts but just  consider these three similar to what a human would consider.   across   application   the  underlying   ideas  of  CBR  can  be  applied  While  consistently  specific  implementation  of  the  CBR  methods  \u2013in  particular  retrieval  and  similarity  functions\u2013  is  highly  customized  to  the  application  at  hand.  4.2 CBR and Games   domains,   the   Many  different  implementations  of  CBR  exist  in  games.  CBR  technology  is  nicely  suited  for  recognizing  complex  situations much easier and more elegant than traditional parameter  comparison  or  function  evaluation.  There  are  especially  evident  cases in real time strategies where different attack and defense of  global strategies are nicely defined by CBR datasets and later used  in  the  running  games.  Also  intelligent  bots  behavior  is  also  another typical example. Depending on the number of enemy bots  the  layout  of  the  terrain  and  position of  human  players  the  CBR  system  finds  the  closest  CBR  case  and  employs  that  strategy  against the human players which in prior evaluation was proved to  be highly efficient.   5. Game Trees with AI Planning \u2013 Tic-tac-toe  In  order  to  show  the  expressive  power  of  AI  Planning  in  defining strategies  for games, and the use of these plans to build  Game  Trees I implemented an algorithm that builds Game  Trees  for the Tic-Tac-Toe game.   The  game  tree  of  Tic-Tac-Toe  shows  255,168  possible  games  of  which  131,184  are  won  by  X  (the  first  player),  77904  are won by O and the rest 46,080 are draw [13]. All these games  can be derived from building a complete Game Tree.    Even  though  it  is possible  to  build  a  complete  game  tree  of  Tic-tac-toe  it  is  definitely  not  an  optimal  solution.  Many  of  the  moves in this tree would be symmetrical and also there are a many  moves  that  would  be  illogical  or  at  least  a  bad  strategy  to  even  consider.    So what strategy  should X (the first player) choose in order   to win the game?   There  are  few  positions  that  lead  to  certain  victory.  These  positions  involve  simultaneous  attack  on  two  positions  so  the  other player could not defend, basically the only trick in Tic-Tac- Toe.   Figure 6: Tic-tac-toe winning strategy positions   Position 1 leads to victory if the two of the three fields: top  middle,  bottom  left  corner  and  bottom  right  corner  are  free  [Figure 6].   Position 2 lead to victory if two of the three fields: top right  corner, bottom right corner and bottom middle are free [Figure ].    The game starts from an empty table.   The two relevant strategies that would lead to these positions   are to take one corner or to take the center [Figure 7].   Figure 7: Tic-tac-toe Two starting moves   The  center  position  as  we  can  see  in  the  simulation  results  lead  to  a  bigger  number  of  victorious  endings  but  it  is  also  a  straight forward strategy with obvious defense strategy.   At this point we need to consider the moves of the opponent.  If  we take the left branch the opponent moves can be a center, a  corner  or  a  middle  field.  We  also  need  to  differentiate  with  a  move to a corner adjacent with our like top left or bottom right or  across the center to bottom right [Figure 8].   Figure 8: Tic-tac-toe opponent response to corner move   In  cases  one  and  two,  we  have  a  clear  path  to  executing  strategy  3  so  we  need  to  capture  the  diagonally  opposite  field.  And as for the third case the best way to go is to capture the center  and go for strategy 1 or 2 depending of the opponent\u2019s next move.    Figure 9: Tic-tac-toe move 2 after corner opening   The first move leads to certain victory, O will have to go to  the  center  and  X  will  achieve  strategy  3  [Figure  9].  The  second  move is a possible way to strategy 3 if O makes a mistake in the  next  loop,  so  X  goes  to  the  opposite  corner.  For  the  third  case  since  O  is  playing  a  valid  strategy  the  only  move  that  leaves  a  possible mistake from O would be to take the center and wait for  O to go to the middle and then achieve strategy 1 or 3 which will  be  a  symmetric  situation  to  the  one  that  we  will  find  if  we  branched with the center.   Figure 10: Tic-tac-toe opponent response to center move   If  we  go  back  to  the  second  branch  [Figure  10],  a  possible  way for the second player to engage is corner or middle. The first   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \fInteractive and Adaptable Media  299  move  is  a  valid  strategy  for  O  and  can  be  mee corner move from X to try a mistake from O in  the same as in the third case above from the pre another  move  would  be  go  to  the  middle  wh achieves strategy 1 or 2.    et  with  a  opposite  the future exactly  evious branch, and  here  X  eventually   This HTN when executed will re  game scenarios. By creating nodes from them with branches with the move of t tree for the Tic-tac-toe game over whi algorithm.   esult with plans for possible  m each position and linking  the player we create a game  ich we can run the minimax   Figure 11: Tic-tac-toe Move 2 after cent The fist move will lead to win if O moves  draw if it  goes  for the corners [Figure 11]. In t has  to  block  the  lower  left  corner  which  leave middle left or corner left which are strategy 1 and To sum the strategies for the planning, first  corner strategy for the beginning. Then for the ce the  corners  with  the  particularly  the  one  oppo holds. If the center is empty for the second strate we go for the opposite corner. After this point w opponent  or  try  to  implement  strategies  1,  2  or victory.   Plan 1: Take center   Preconditions: Center empty  Plan 2: Take corner   Preconditions: All corners empty  Plan 3: Take corner after center  Preconditions: We have center take corner oppos opponent has  Plan 4: Take diagonal corner  Preconditions: We have a corner, the opponent ha  the corner opposite to the one we have is free.  Plan 5: Block  Precondition: The opponent has tree tokens in a r agonal  Plan 6: Win  Preconditions: We have two tokens in a row, colu nd the third place is free  Plan 7: Tie  Preconditions: If all places are taken, it\u2019s a tie.  5.1 Hierarchical Task Network   ter opening to the middle or a  the second case O  es  X  to  go  for  the  d 2. we have center or  enter we try to get  osite  to  the  one  O  egy we go for it or  we either block the  r  3  which  lead  to   site to the  one the   as the ce\u2212nter and  row, colu\u2212mn or di  mn or dia\u2212gonal a  Top level task is Play [Figure 12]. This is a  can  be  derived  into:  Win,  Block,  Tie  or  Sear Search for plan is derived to both Plan 1 and Pla Plan 4, which later leads to a call for the oppon recursive call to Play.   a complex task and  rch  for  Plan.  The  an 2 or Plan 3 and  nent\u2019s move and a   Figure 12: Tic-tac-toe HT  TN  This set up with 7 plans with 3 ta for Tic-tac-toe which considers all pos player  with only 457 games, 281 of  w and  0  where  the  second  opponent  w reduction over the 255, 168 possible g tree. These reductions can be very use computing  capabilities  but  also  we  pr that planning can be very efficient if d trees  by  applying  reasoning  very  reasoning.   arget strategies creates a tree  ssible moves for the second  which X wins 176 are draw  wins.  This  is  a  significant  ames with a complete game  eful for devices with limited  rove  a  very  important  point  designing meaningful game  similar  to  human  player   me  tree  are  also  possible  if  d, in other words if we drop  moves of the opponent.   Further  improvements  to  the  gam the opponents moves are also planned all the meaningless and symmetrical m 6. Game AI in Monopoly  6.1 Overview of the AI Imp The  AI  agent  is  responsible  for  players in the game. The core principle a Game Tree with all the sensible move make  from  the  current  point  of  time minimax  algorithm  the  agent  selects  t would  bring  the  computer  player  mo with  the  highest  probability.  Building  that would be big enough to consider  is  obstructed  by  the  vastness  of  poss with all the possible random landings  nodes  of  the  game  tree  exponentially tackle  this  problem  the  AI  agents  discussed technologies: Case Based Re The  technologies  are  employed  First the agent searches the CBR datab largest similarity  with the current state associated  with  a  playing  strategy.  Th that the planner needs to build plans f consecutive player  moves that bring th way only moves that are part of that str being a small fraction of the overall po edges of the game tree at each level dec At each level of the game tree the of  a  single  player.  After  the  strateg considered the response to those strate by the opponent(s). The move of the  probability  distribution  of  the  dice  as  player.  A  more general strategy  needs opponent\u2019s  (human  player)  moves  sin the  expertise  of  the  opponent.  This  ge more plausible moves than the focused After  covering  all  opponents  t deducting  a  feature  move  of  the  com CBR  selected  plan  strategy.  After  strategies  and  reaching  a  reasonable  s into  account  the  memory  limits  an probabilities  that  the  move  is  possible the dice the building of the Game Tre algorithm  searches  the  Game  Tree  favorable  move  for  the  AI  player  usi The process is repeated each time the A  plementation the  moves  of  the  artificial  e of the AI agent is building  es that all the players would  e  forward.  Then  using  the  the  move  that  in  the  future  ost  favorable  game  position  a  Game  Tree  in  this  game  sufficient number of moves  sible  moves  in  combination  of the dice. The number of  y  grows  at  each  level.  To  incorporates  two  already   easoning and AI Planning.   in  the  following  manner.  base to find the case with the  e of the board. This case is  he  strategy  consists  of  goal  for, and the plans consist of  he player to that goal. This  rategy are considered, those  ossible moves the number of  creases immensely.  e model considers the moves  gies  of  the  AI  player  are  egies needs to be considered  opponent(s) depends of the  well  as  the  strategy  of  the  s to be implemented for the  nce  we  cannot  be  aware  of  eneral  strategy  would  bring  d strategy of the AI player.   the  agent  comes  back  to  mputer  player  by  using  the  creating  several  loops  of  size  of  a  Game  Tree  taking  nd  the  rapidly  decreasing  e  due  to  the  distribution  of  ee stops. Then the minimax  and  decides  on  the  most  ing  the  minimax  algorithm.  AI player is up.   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f300  DIMEA 2008  On  the  other  hand  the  MonopolySolution  class  holds  the  three  particular  attributes  that  are  needed  for  the  planning,  the  planning Domain, State and TaskList.   The  game  is  implemented  by  using  the  Model-View- Controller  software  development  pattern.  The  controller  is  responsible  for  implementing  the  game  rules  and  handling  all  of  the  events  in  the  game  like  roll  of  dice,  input  commands  for  trading,  auctioning  and  etc  from  the  players.  The  View  layer  is  responsible  for  displaying  the  board  and  all  of  the  input  widgets  on  to  the  game  screen,  and  the  models  are  data  structures  representing the game state [Figure 14].   Buying,  auctioning  and  trading  game  moves  are  always  accompanied  by  return  of  investment  calculations  in  making  the  plans. These calculations represent adaptation of the more general  planning  associated  with  the  cases  in  the  CBR  database.  These  adaptations  are  necessary  due  to  the  fact  that  the  cases  do  not  identically  correspond  to  the  situation  on  the  table.  In  addition  calculating the game position value of each node of the game tree  is  done  by  heuristic  functions  incorporate  economic  calculations of net present value, cash, and strategic layout and so  on.  For  example  railroads  in  monopoly  are  known  to  be  strategically  effective  because  they  bring  constant  income  even  though  the  income  can  be  smaller  than  building  on  other  properties.   6.2 Details on the CBR Implementation   that   The  implementation  of  the  CBR  is  by  using  the  JColibri2  platform.    JColibri2  is  an  object-oriented  framework  in  Java  for  building  CBR  systems  that  is  an  evolution  of  previous  work  on  knowledge intensive CBR [14].    For this implementation we need to look into three particular  classes  of  the  JColibri2  platform.  The  StandardCBRApplication,  Connector,  CBRQuery.  For  a  JColibri2  implementation  the  StandardCBRApplication interface needs to be implemented.    The CBR cycle executed accepts an instance of CBRQuery.  This  class  represents  a  CBR  query  to  the  CBR  database.  The  description  component  (instance  of  CaseComponent)  represents  the description of the case that will be looked up in the database.  All  the  solutions  case  CaseComponent interface.   implementing   cases   and   are   The  JColibri2  platform  connects  to  the  CBR  database  via  a  Connector  class.  Each  connector  implements  all  the  necessary  methods for accessing the database, retrieval of cases, storing and  deletion  of  cases.  This  implementation  uses  a  custom  XML  structure  for  holding  the  CBR  cases.  Since  the  game  will  not  update  the  CBR  database  only  read  it,  a  XML  solution  satisfies  the needs. The XML file to a certain extent is similar to the XML  representation  of  the  board.  We  are  interested  in  finding  one  CBRCase that is the most similar case to the situation in the game  at  the  time  of  the  search.  This  procedure  is  done  in  the  cycle  method of the CBRApplication. The JColibri2 CBR comparison is  done by Nearest Neighbor (NN) search method.    JColibri2  offers  implementations  for  NN  search  algorithms  of  simple  attributes.  These  implementations  are  called  local  similarities.  For  complex  attributes  like  in  our  case  global  customized similarity mechanisms need to be implemented.   The  MonopolyDescription  class  [Figure  13]  is  basically  a  serialization of the  GameState. It holds all the information about  the state of the board, the players, their amount of cash etc.    Figure 13: Class diagram of the Monopoly Case component  models   Figure 14: Class diagram of the Monopoly models   6.2.1 Complex Similarity representation in CBR   implementing   The  similarity  measurement  part  of  the  Nearest  Neighbor  implemented  by  the  algorithm  JColibri2  is  LocalSimiralrityFunction  the  GlobalSimiralityFunction  and  interface. A local similarity function is applied to simple attributes  by the NN algorithm, and a global similarity function is applied to  compound  attributes.  In  the  case  of  our  implementation  the  attributes  of  the  MonopolyDescription  are  compound  attributes  describing  the  state  of  the  board,  number  of  players,  amount  of  cash  for  every  player  and  etc.  Since  MonopolyDescription  is  a  custom  CaseComponent  a  global  similarity  function  needs  to  be  implemented  to  accurately  find  the  distance  between  different  CBR cases.   The similarity mechanism is inseparable core element of the  CBR  system.  This  mechanism  represents  how  the  CBR  decides  which  strategy  is  best  suited  for  the  particular  situation  by   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \fInteractive and Adaptable Media  301  calculating  the  distance  or  similarity  to  other  cases  in  the  database.    For  the  monopoly  implementation  we  need  to  consider  several  basic  strategies.  Monopoly  is  based  on  investing  in  properties and receiving revenues from those investments. One of  the basic strategies of the game is to build a set of properties that  will  bring  constant  income  larger  than  the  one  of  the  opponents.  So in time the opponents will have to declare bankruptcy. But on  the other hand over investment can lead to too stretched resources  with  low  income  that  will  eventually  drove  the  player  to  bankruptcy.  To  decide  on  these  two  we  need  a  clear  separation  into two groups of cases in the CBR database. The first group of  cases will represent a situation on the board where the player has  significant  income  per  loop  formed  of  one  or  more  color  group  properties, maybe railroads, some buildings on them and so on. It  is  important  to  note  that  in  this  case  the  player  is  better  situated  than his opponents so he only needs to survive long enough to win  the  game.  In  the  other  group  of  cases  either  the  opponent  is  not  well  positioned on  the board  or  its  opponents  are better  situated.  In  this  case  further  investments  are  necessary  to  improve  the  situation  so  the  player  can  have  a  chance  of  winning  in  the  long  run.    These metrics can be owning color groups, valuing groups of  railroads, evaluating the other opponents as well, and considering  the amount of cash. As it is obvious in monopoly the number of  streets is not as nearly as important as the combination of streets  the  player  owns.  It  is  also  important  to  note  that  one  CBR  case  does not hold only a single strategy in place, but its solution can  have multiple different strategic goals. For example one CBR case  might simultaneously say buy this land to form a color group but  also  trade  some  other  unimportant  property  to  increase  cash  amount.    The cases do not represent all possible combinations of board  positions. They are only representation of typical game scenarios.  The CBR Case solutions do not give exact instructions in general  but  rather  strategic  goals.  For  example  one  CBR  Solution  might  say trade the streets that  you only have one of  each  for the ones  that you have two of that color already. Then the planner based on  the situation on the board needs to decompose this high level task  to a low level operations. Like offer \"Mediterranean Avenue\" for  \"Reading Railroad\" and offer $50. The exact amounts and actual  streets are left to the planer to evaluate.    The monopoly CBR database is currently in development on  a  monopoly  clone  game  called  Spaceopoly.  The  cases  are  architected  based  on  human  player  experience  and  knowledge.  There is a plan of making a number of slightly different strategies  that  differ  on  the  style  of  playing  and  then  running  simulation  tests that would determine the particular validity of each database  as  well  as  validity  of  certain  segments  of  the  strategy  or  even  particular cases in the database.    JSHOP2  uses  ordered  task  decomposition  in  reducing  the  HTN to list of primitive tasks  which form the plans.  An ordered  task decomposition planner is an HTN planner that plans for tasks  in  the  same  order  that  they  will  be  executed.  This  reduces  the  complexity  of  reasoning  by  removing  a  great  deal  of  uncertainty  about  the  world,  which  makes  it  easy  to  incorporate  substantial  expressive  power  into  the  planning  algorithm.  In  addition  to  the  usual HTN methods and operators, the planners can make use of  axioms,  can  do  mixed  symbolic/numeric  conditions,  and  can  do  external function calls.    In  order  for  the  JSHOP2  planer  to  generate  plans  it  needs  tree  crucial  components:  Domain,  State  and  Tasks.  The  Domain  defines  all  the  functionalities  that  the  particular  domain  offers.  These  are  simple  and  complex  tasks.  The  complex  tasks  also  called methods create the hierarchy with the fact that they can be  evaluated  by  simple  tasks  of  other  complex  tasks.  This  is  how  a  hierarchical structure of tasks is formed. The problem reduction is  done by reducing the high level complex tasks to simpler until all  the tasks are primitive. The list of primitive tasks forms the plan.   The  State  represents  the  state  of  the  system.  It  is  a  simple  database of facts that represent the state of the system. The State  is  necessary  to  determine  the  way  the  problems  or  tasks  are  reduced  to  their  primitive  level.  The  reduction  is  done  by  satisfying  different  prerequisites  set  in  the  methods;  these  prerequisites  are  defined  in  the  state.  The  Tasks  are  high  level  tasks or methods defined in the Domain. The planner based on the  State and the goals selects one or more high level tasks that need  to be reduced to plans [Figure  15].   Tasks  Core Planner   State  Plan  Figure 15: Diagram of a Planner   The  plans  then  generate  the  game  moves.  The  number  of  moves  generated  by  the  plans  is  just  a  fraction  of  the  possible  moves  at  that  point.  This  reduces  the  game  tree  providing  the  opportunity to generate smaller and deeper game trees and making  more efficient decisions in general.    7. Conclusion   Even  though  the  results  from  the  CBR  database  are  not  complete  at  this  time  partial  strategies  are  implemented  as  cases  and  recognized  during  game  play  by  the  CBR  system.  These  smaller  local  strategies  coupled  with  more  global  higher  level  strategies  that  are  particularly  important  at  the  beginning  of  the  game  would  form  a  complete  CBR  database  and  represent  a  knowledge engineered style of playing of the AI player.    The  actual  execution  of  the  strategies  will  not  differ  from  strategy to strategy since the plan execution is more related to the  structure and rules of the game than to the actual playing strategy.   The AI Planning approach is a proven method by the tic-tac- toe  experiment  and  is  suitable  for  implementing  the  strategies  associated with the CBR cases.   6.3 Details on the Planning Implementation   For  the  purpose  of  planning  this  implementation  uses  a  the  JSHOP2  planner.  The  Java  Simple  modification  of  Hierarchical  Ordered  Planner  2  is  a  domain  independent  HTN  planning system [15].    This  approach  in  general  benefits  from  both  technologies,  CBR  as  well  as  AI  Planning  and  comprises  an  elegant  solution.  Even  though  AI  Planning  can  be  enough  as  a  single  technology  for  some  simpler  problems  like  tic-tac-toe  the  complexity  of  Monopoly would mean that the Planner would have to incorporate   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f302  DIMEA 2008  large and complex domain and a very big state model. The CBR  application helps reduce this complexity by focusing the planning  on  smaller  domain  of  the  game.  Basically  the  CBR  reduces  the  overall  goal  of  the  play  (wining  the  game)  to  smaller  more  concrete  goals  suitable  to  the  particular  state  of  the  game,  thus  reducing  the  need  for  global  planning  strategies  and  complex  planning domain.    Furthermore  this  symbiosis  of  technologies  gives  way  for  more precise and finely tuned strategies which can be difficult to  include into global plan for the whole game. One simple example  for  the  Monopoly  game  would  be  this:  Sometimes  it\u2019s  better  to  stay  in  jail  because  rolling  double  increases  the  probability  of  landing  on  some  field  (two,  four,  six,  eight,  ten  or  twelve  steps  from  the  jail)  that  can  be  of  great  importance  to  the  rest  of  the  game.  These  and  similar  small  local  strategies  can  be  easily  recognized by similar cases in the CBR database.    In  other  words  the  system  is  flexible  enough  so  that  new  strategies can be incorporated easily missing strategies can be also  recognized by the distance metrics as well as wrong assumptions  in the strategies can be easily recognized.   One other important property of  the system is that is highly  configurable.  The  game  its  self  can  be  diversely  different  depending  on  the  configuration  of  the  board.  Even  though  the  platform  is  restricted  to  Monopoly  type  of  games,  changing  the  layout  and  values  of  the  fields  effectively  brings  completely  different  properties  of  the  game.  In  addition  the  CBR  database  represents the entire experience  of the AI Player. It can be filled  with rich set of strategies or even configured with different flavors  of difficulties of play, this of course coupled with the domain of  the planner which can differ from a case to a case as well.    8. Future Work   Further  exploration  of  this  technology  would  go  towards  complete  implementation  of  an  AI  aware  agent  for  monopoly.  Initial  results  from  the  local  cases  with  more  specific  strategies  show CBR as a capable tool for representing expertise in playing  the  game.  Completing  the  more  general  strategies  and  coupling  them  with  the  planning  domain  will  give  precise  results  on  the  benefits from this architecture.   There is also need for exploring the planning of strategies of  opponents.  This  task  is  to  some  extent  different  because  we  cannot  always  expect  the  opponent  to  select  the  best  move  we  think.  In  the  Tic-tac-toe  example  all  possible  moves  of  the  opponent  were  taken  into  consideration,  if  we  used  the  same  planner  for  the  opponent  only  tie  games  would  result  from  the  game tree. In other words mistakes of the players also need to be  considered.    The CBR Platform brings other functionalities well worth of  exploring as well. The revision stage of the JColibri2 platform is  basically capable of fine tuning strategies or even developing new  strategies  for  the  games.  A  well  written  underlying  AI  planning  model  with a  capable  feedback of the  game tree evaluation back  to  the  CBR  revision  capability  can  be  an  interesting  concept  in  automatic experience acquisition for the AI model.   There  are  also  many  other  fields  were  combined  CBR  and  planning  approach  can  be  incorporated  into  a  problem  solution.  This combination is analogous in a big extent to a human way of   reasoning.  People  in  addition  to  logic  of  reasoning  in  situations  with  lack  of  information  rely  to  planning  strategies  and  prior  experience,  exactly  the  intuition  behind  CBR  \u2013  AI  Planning  architecture.    9. ACKNOWLEDGMENTS   We  would  like  to  thank  Prof.  Sofia  Tsekeridou  for  her  involvement  in  the  valuable  discussions  we  had  on  the  topic  of  CBR.   10. REFERENCES  [1] Minimax. Wikipedia. [Online] [Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Minimax.   [2] Von Neumann, J: Zur theorie der gesellschaftsspiele Math.   Annalen. 100 (1928) 295-320   [3] Automated Planning. Wikipedia. [Online] [Cited: April 23,   2008.] http://en.wikipedia.org/wiki/Automated_planning.   [4] Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based  Strategy Game with Plan Adaptation and Ontology-based  retrieval.  [5] K. Erol, J. Hendler, and D. Nau (1994). Semantics for   hierarchical task-network planning. Technical Report TR-94- 31, UMIACS.   [6] Smith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning  approach decrarer play in contract bridge. Computational  Intelligence. 1996, Vol. 12, 1.   [7] One Jump Ahead: Challenging Human Supremacy in   Checkers. J.Schaeffer. s.l. : Springer-Verlag, 1997.   [8]  IBM. How Deep Blue works. [Online] 1997. [Cited: April  23, 2008.]  http://www.research.ibm.com/deepblue/meet/html/d.3.2.html  [9] Ghallab, Malik, Nau, Dana and Traverso, Paolo.  Automated Planning theory and practice. s.l. : Morgan  Kaufmann Publishers, May 2004. ISBN 1-55860-856-7.   [10] Case Based Reasoning. Experiences, Lessons and Future.  Leake, David. s.l. : AAAI Press. MIT Press., 1997.   [11] Applying case-based reasoning: techniques for enterprise   systems. Watson, I. San Francisco, CA, USA : Morgan  Kaufmann Publishers Inc., 1998.   [12] Plaza, A. Aamodt and E. Case-based reasoning:   Foundational issues, methodological. AI Communications.  1994, 7(i).   [13] Tic-tac-toe. Wikipedia. [Online] [Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Tic-tac-toe.   [14] D\u00edaz-Agudo, B. and Gonz\u00e1lez-Calero, P. A. An   architecture for knowledge intensive CBR systems. Advances  in Case-Based Reasoning \u2013 (EWCBR\u201900). New York :  Springer-Verlag, Berlin Heidelberg, 2000.   [15] Ilghami, Okhtay and Nau, Dana S. A General Approach to   Synthesize Problem-Specific Planners. 2003.   3rd International Conference on Digital Interactive Media in Entertainment and Arts",
  "references": [
    "1] Minimax. Wikipedia. ",
    "Online] ",
    "Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Minimax.   ",
    "2] Von Neumann, J: Zur theorie der gesellschaftsspiele Math.   Annalen. 100 (1928) 295-320   ",
    "3] Automated Planning. Wikipedia. ",
    "Online] ",
    "Cited: April 23,   2008.] http://en.wikipedia.org/wiki/Automated_planning.   ",
    "4] Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based  Strategy Game with Plan Adaptation and Ontology-based  retrieval.  ",
    "5] K. Erol, J. Hendler, and D. Nau (1994). Semantics for   hierarchical task-network planning. Technical Report TR-94- 31, UMIACS.   ",
    "6] Smith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning  approach decrarer play in contract bridge. Computational  Intelligence. 1996, Vol. 12, 1.   ",
    "7] One Jump Ahead: Challenging Human Supremacy in   Checkers. J.Schaeffer. s.l. : Springer-Verlag, 1997.   ",
    "8]  IBM. How Deep Blue works. ",
    "Online] 1997. ",
    "Cited: April  23, 2008.]  http://www.research.ibm.com/deepblue/meet/html/d.3.2.html  ",
    "9] Ghallab, Malik, Nau, Dana and Traverso, Paolo.  Automated Planning theory and practice. s.l. : Morgan  Kaufmann Publishers, May 2004. ISBN 1-55860-856-7.   ",
    "10] Case Based Reasoning. Experiences, Lessons and Future.  Leake, David. s.l. : AAAI Press. MIT Press., 1997.   ",
    "11] Applying case-based reasoning: techniques for enterprise   systems. Watson, I. San Francisco, CA, USA : Morgan  Kaufmann Publishers Inc., 1998.   ",
    "12] Plaza, A. Aamodt and E. Case-based reasoning:   Foundational issues, methodological. AI Communications.  1994, 7(i).   ",
    "13] Tic-tac-toe. Wikipedia. ",
    "Online] ",
    "Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Tic-tac-toe.   ",
    "14] D\u00edaz-Agudo, B. and Gonz\u00e1lez-Calero, P. A. An   architecture for knowledge intensive CBR systems. Advances  in Case-Based Reasoning \u2013 (EWCBR\u201900). New York :  Springer-Verlag, Berlin Heidelberg, 2000.   ",
    "15] Ilghami, Okhtay and Nau, Dana S. A General Approach to   Synthesize Problem-Specific Planners. 2003.   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f"
  ],
  "url": "https://drive.google.com/uc?id=1asTkg4Uf6_MHZeqZ642nPzp-IsQ4mTKM",
  "date": "2024-01-09 18:22:30",
  "is_published": false
}
{
  "title": "Interactive and Adaptable Media",
  "authors": [
    "295",
    "AI Model for Computer games based on Case Based",
    "Vlado Menkovski",
    "0.8km Markopoulou Ave.",
    "Peania, 19002, Greece",
    "Dimitrios Metafas",
    "0.8km Markopoulou Ave.",
    "Peania, 19002, Greece"
  ],
  "institutions": [
    "Reasoning and AI Planning",
    "Athens Information Technology",
    "Athens Information Technology"
  ],
  "abstract": "",
  "keywords": [
    "Game AI",
    " Case Based Reasoning",
    " AI Planning",
    " Game Trees"
  ],
  "article": "The  goal  of  this  effort  is  to  explore  a  model  for  design  and  implementation of an AI agent for turn based games. This model  provides for building more capable computer opponents that rely  on  strategies  that  closely  resemble  human  approach  in  solving  problems opposed to classical computational centric heuristics in  game  AI.  In  this  manner  the  computational  resources  can  be  focused on more sensible strategies for the game play.    Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not  made  or  distributed  for  profit  or  commercial  advantage  and  that  copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy  otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires prior specific permission and/or a fee.  DIMEA\u201908, September 10\u201312, 2008, Athens, Greece.  Copyright 2008 ACM 978-1-60558-248-1/08/09... $5.00   With  the  advancement  in  computer  hardware  increasingly  more  computing  power  is  left  for  executing  AI  algorithms  in  games.  In  the  past  AI  in  games  was  mainly  a  cheating  set  of  instructions  that  simulated  the  increasing  difficulty  in  the  game  environment so that the player had the illusion of real counterpart.  Improvement  in  available  memory  and  processing  power  allows  implementation  of  more  intelligent  algorithms  for  building  the  game  environment  as  well  as  direct  interaction  with  the  human  players.     in  games  with   In  this  particular  research  the  emphasis  is  put  on  the  interaction  between  the  AI  agent  and  a  computer  player  in  the  realm  of  the  game  rules.  It  is  particularly  focused  on  turn  based  games that have the elements of uncertainty like dice or concealed  information.  At  the  beginning  a  description  of  Game  AI  algorithms  are  given;  such  as  Game  Trees  and  Minimax.  The  following  section  describes  an  approach  of  using  AI  Planning  to  improve  building  Game  Trees  imperfect  information  where  Game  Trees  tend  to  be  very  large  with  high  growth ratio. Section 4 discusses another approach that provides a  significant reduction to the number of considered moves in order  to find the favorable strategy of the AI player. This approach uses  AI Planning techniques and Case Base Reasoning (CBR) to plan  for different scenarios in predetermined strategies which would be  analogous to human player experience in the particular game. The  CBR  database  illustrates  a  set  of  past  experiences  for  the  AI  problem and the AI Planning illustrates the procedure to deal with  the  given  situation  in  the  game.  In  the  next  two  sections  implementations  and  evaluations  of  both  approaches  are  given.  The  AI  Planning  approach  is  implemented  with  the  Tic-tac-toe  game  and  the  combined  AI  Planning  and  CBR  approach  is  implemented with a model for the Monopoly game. The last part  contains conclusions and future work ideas.    2. Game Trees and Minimax   Game Trees are common model for evaluating how different  combinations  of  moves  from  the  player  and  his  opponents  will  affect  the  future  position  of  the  player  and  eventually  the  end  result of the game. An algorithm that decides on the next move by  evaluating  the  results  from  the  built  Game  Tree  is  minimax  [1].  Minimax assumes that the player at hand will always choose the  best possible move  for him, in other words the player  will try to  select  the  move  that  maximizes  the  result  of  the  evaluation  function over the game state. So basically the player at hand needs  to choose the best move overall while taking into account that the  next  player(s)  will  try  to  do  the  same  thing.  Minimax  tries  to  maximize the minimum gain. Minimax can be applied to multiple   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f296  DIMEA 2008  levels of nodes on the game tree, where the leaves bring the final  known (or considered) game state.    The minimax theorem states:   For  every  two-person,  zero-sum  game  there  is  a  mixed  strategy  for each player, such that the expected payoff for both is the same  value V when the players use these strategies. Furthermore, V is  the  best  payoff  each  can  expect  to  receive  from  a  play  of  the  game; that is, these mixed strategies are the optimal strategies for  the two players.   This  theorem  was  established  by  John  von  Neumann,  who  is  quoted as saying \"As far as I can see, there could be no theory of  games  \u2026  without  that  theorem  \u2026  I  thought  there  was  nothing  worth publishing until the Minimax Theorem was proved\" [2].   A simple example of minimax can be observed by building a  game tree of the tic-tac-toe game. The tic-tac-toe game is a simple  game which can end by the first player wining, the second player  wining or a tie. There are nine positions for each of the players in  which at each turn the player puts X or O sign. If the player has  three adjacent signs in a row, column or the two diagonals he or  she wins. This game has limited number of position and it is well  suited  for  building  the  whole  game  tree.  The  leaves  of  this  tree  will  be  final  positions  in  the  game.  A  heuristics  evaluation  function will also need to be written to evaluate the value of each  node along the way.  3. AI Planning for building Game Trees  3.1.1 AI Planning   AI  Planning  also  referred  as  Automated  Planning  and  Scheduling  is  a  branch  of  Artificial  Intelligence  that  focuses  on  finding strategies or sequences of actions that reach a predefined  goal  [3].  Typical  execution  of  AI  Planning  algorithms  is  by  intelligent  agents,  autonomous  robots  and  unmanned  vehicles.  Opposed to classical control or classification AI Planning results  with  complex  solutions  that  are  derived  from  multidimensional  space.    AI Planning algorithms are also common in the video game  development.  They  solve  broad  range  of  problems  from  path  finding to action planning. A typical planner takes three inputs: a  description  of  the  initial  state  of  the  world,  a  description  of  the  desired  goal,  and  a  set  of  possible  actions.  Some  efforts  for  incorporating  planning  techniques  for  building  game  trees  have  also shown up, similar to the approach explored in this effort. In  addition Cased Based Reasoning [4] techniques are also gathering  popularity  in  developing  strategies  based  in  prior  knowledge  about  the  problems  in  the  games.  One  of  the  benefits  from  Hierarchical  Task  Network  (HTN)  [5]  planning  is  the  possibility  to  build  Game  Trees  based  on  HTN  plans;  this  method  is  described in the following section.   3.2 Game Trees with AI Planning   An  adaptation  of  the  HTN  planning  can  be  used  to  build  much smaller and more efficient game trees. This idea has already  been  implemented  in  the  Bridge  Baron  a  computer  program  for  the game of Contact Bridge [6].   Computer  programs  based  on  Game  Tree  search  techniques  are  now  as  good  as  or  better  than  humans  in  many  games  like  Chess  [7]  and  checkers  [8],  but  there  are  some  difficulties  in  building  a  game  tree  for  games  that  have  imperfect  information  and  added  uncertainty  like  card  or  games  with  dice.  The  main   problem  is  the  enormous  number  of  possibilities  that  the  player  can  choose  from  in  making  his  move.  In  addition  some  of  the  moves  are  accompanied  with  probabilities  based  on  the  random  elements  the  games.  The  number  of  possible  moves  exponentially  grows  with  each  move  so  the  depth  of  the  search  has  to  be  very  limited  to  accommodate  for  the  memory  limitations.    in   The basic idea behind using HTN for building game trees is  that  the  HTN  provides  the  means  of  expressing  high  level  goals  and  describing  strategies  how  to  reach  those  goals.  These  goals  may be decomposed in goals at lower level called sub-goals. This  approach  closely  resembles  the  way  a  human  player  usually  addresses a complex problem. It is also good for domains  where  classical search for solution is not feasible due to the vastness of  the problem domain or uncertainties.   3.2.1 Hierarchical Task Networks   The  Hierarchical  Task  Network,  or  HTN,  is  an  approach  to  automated  planning  in  which  the  dependency  among  actions  can  be given in the form of networks [9] [Figure 1].   A simple task network (or just a task network for short) is an  acyclic  digraph  (cid:2) (cid:3) (cid:4)(cid:5)(cid:6) (cid:7)(cid:8)  in  which  U  is  the  node  set,  E  is  the  edge set, and each node (cid:9) (cid:10) (cid:5) contains a task (cid:11)(cid:12). The edges of (cid:2) define a partial ordering of U. If the partial ordering is total, then  we say that (cid:2) is totally ordered, in which case (cid:2) can be written as  a sequence of tasks (cid:2) (cid:3) (cid:13)(cid:11)(cid:14)(cid:6) (cid:11)(cid:15)(cid:6) (cid:16) (cid:6) (cid:11)(cid:17)(cid:18).  Buy milk  Go to (shop)  Purchase   Go to (home)  Figure 1: Simple Hierarchical Task Network   A  Simple  Task  Network  (STN)  method  is  a  4-tuple  of  its  name,  task,  precondition  and  a  task  network.  The  name  of  the  method  lets  us  refer  unambiguously  to  substitution  instances  of  the  method,  without  having  to  write  the  preconditions  and  effects  explicitly.  The  task  tells  what  kind  of  task  can  be  applied  if  the  preconditions  are  met.  The  preconditions  specify  the  conditions  that the current state needs to satisfy in order for the method to be  applied.  And  the  network  defines  the  specific  subtasks  to  accomplish in order to accomplish the task.   A  method  is  relevant  for  a  task  if  the  current  state  satisfies  the  preconditions of a method that implements that task. This task can  be  then  substituted  with  the  instance  of  the  method.  The  substitution  is  basically  giving  the  method  network  as  a  solution  for the task.   If  there  is  a  task  \u201cGo  home\u201d  and  the  distance  to  home  is  3km  [Figure 2] and there exists a method walk-to and this method has a  precondition that the distance is less than 5km, then a substation  to the task \u201cGo home\u201d can be made with this method instance.    Go-to (from, to)  If (to \u2013 from) < 5km   Walk (to)  Figure 2: HTN Method   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \fInteractive and Adaptable Media  297  If the distance is larger than 5km  another meth to be substituted [Figure 3].   hod instance needs   Go-to (from, to)   If(to \u2013 from) < 5km   If(t  to \u2013 from) < 200km   Walk (to)   Drive(to  )  Figure 3: HTN Method 2   An STN planning domain is a set of operatio methods  M.  A  STN  planning  problem  is  a  4-tu state  S0,  the  task  network  w  called  initial  task STN domain. A plan (cid:19) (cid:3) (cid:13)(cid:20)(cid:14)(cid:6) (cid:16) (cid:6) (cid:20)(cid:21)(cid:18) is a soluti problem if there is a way to decompose w into \u03c0 and  each  decomposition  is  applicable  in  the  ap the  world.  The  algorithm  that  is  capable  to  networks into plans is called Total-forward-deco [9]  or  Partial-forward-decomposition  (PFD).  H cases  where  one  does  not  want  to  use  a  forwa procedure. HTN planning is generalization of S gives  the  planning  procedure  more  freedom construct the task networks.    ons O and a set of  uple  of  the  initial  k  network  and  the  ion for a planning  \u03c0 if \u03c0 is executable  ppropriate  state  of  decompose  these  omposition (TFD)  However  there  are  ard-decomposition  STN planning that  m  about  how  to   In order to provide this freedom, a bookke is needed to represent constraints that the plann not  yet  enforced.  The  bookkeeping  is  done  by unenforced constraints explicitly in the task netw  eeping mechanism  ning algorithm has  y  representing  the  work.   The  HTN  generalizes  the  definition  of  a STN. A task network is the pair (cid:2)  (cid:3)   (cid:4)(cid:5)(cid:6) (cid:23)(cid:8) w task  nodes  and  C is  a  set  of  constraints.  Eac specifies a requirement that must be satisfied by a solution to a planning problem.    a  task  network  in  where (cid:5) is a set of  h  constraint  in  C  y every plan that is   The  definition  of  a  method  in  HTN  also definition  used  in  STN  planning.  A  HTN  pla name,  task,  subtasks,  and  constraints.  The  s constraints form the task network. The HTN plan identical to STN planning domains except they u instead of STN methods.   o  generalizes  the  an  is  a  4-tuple  of  subtasks  and  the  nning domains are  use HTN methods   plan. The branches of the game tree rep the  methods.  Tignum2  applies  all  met state  of  the  world  to  produce  new continues  recursively  until  there  are  n have  not  already  been  applied  to  th world.    present moves generated by  thods  applicable  to  a  given  w  states  of  the  world  and  no  applicable  methods  that  he  appropriate  state  of  the   In the task network generated by Tignu actions will occur is determined by th By  listing  the  actions  in  the  order  network can be \u201cserialized\u201d into a gam  um2, the order in which the  e total-ordering constraints.  they  will  occur,  the  task  me tree [Figure 4] [Figure 5].   Figure 4: HTN to Game Tr  ree Algorithm  Figure 5: Game Tree built fr  rom HTN  Compared  to  classical  planners  the  prim HTN planners is their sophisticated knowledge r reasoning  capabilities.  They  can  represent  and  non-classical  planning  problems;  with  a  good guide them, they can solve classical planning p magnitude  more  quickly  than  classical  or  neoc The  primary  disadvantage  of  HTN  is  the  nee author to write not only a set of planning opera of methods.  3.2.2 HTN Planning in building Game   mary  advantage  of  representation and  solve  a  variety  of  d  set  of  HTNs  to  problems orders of  classical  planners.  ed  of  the  domain  ators but also a set   Trees  For  a  HTN  planning  algorithm  to  be  adap trees  we  need  to  define  the  domain  (set  of  H operators) which is the domain of the game. Thi a  knowledge  representation  of  the  rules  of  the environments and possible strategies of game pla  ted  to  build  game  HTN  methods  and  is is in some sense  e  game,  the  game  ay.  In this domain the game rules as well as kn tackle  specific  task  are  defined.      The  implem is  called  Tign Tree  building  with  HTN  implementation  uses  simila decomposition, but adapted to build up a game   nown strategies to  mentation  of  Game  [9].  This  num2  ar  forward- to  tree rather than a   a  procedure   4. Case Based Reasoning in 4.1 Case Based Reasoning Case-based reasoning (CBR) is a  Artificial  Intelligence  (AI),  both  as  problems and as a basis for standalone   n Game Strategies  well established subfield of  a  mean  for  addressing  AI  AI technology.  Case-based  reasoning  is  a  paradigm solving  and  learning  that has  became  applied  subfield  of  AI  of  recent  yea intuition that problems tend to recur. I are  often  similar  to  previously  en therefore, that past solutions may be of [10].    m  for  combining  problem- one  of  the  most  successful  ars.  CBR  is  based  on  the  It means that new problems  ncountered  problems  and,  f use in the current situation   CBR is particularly applicable to probl available,  even  when  the  domain  is  n for  a  deep  domain  model.  Helpdesks, systems  have  been  the  most  successfu to  determine  a  fault  or  diagnostic  attributes,  or  to  determine  whether  or repair is necessary given a set of past s  lems where earlier cases are  not  understood  well  enough  ,  diagnosis  or  classification  ul  areas  of  application,  e.g.,  an  illness  from  observed  r  not  a  certain  treatment  or  olved cases [11].   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f298  DIMEA 2008  Central tasks that all CBR methods have to deal with are [12]: \"to  identify  the  current  problem  situation,  find  a past  case  similar  to  the  new  one,  use  that  case  to  suggest  a  solution  to  the  current  problem, evaluate the proposed solution, and update the system by  learning from this experience. How this is done, what part of the  process  that  is  focused,  what  type  of  problems  that  drives  the  methods, etc. varies considerably, however\".    And  in  the  third  position  if  the  two  of  center,  middle  top  and  middle left are available the position is a certain victory.   There are many different arrangements of the player\u2019s tokens  that  give  equivalent  positions  as  these  three  positions.  By  using  planning we do not need to consider all possible layouts but just  consider these three similar to what a human would consider.   across   application   the  underlying   ideas  of  CBR  can  be  applied  While  consistently  specific  implementation  of  the  CBR  methods  \u2013in  particular  retrieval  and  similarity  functions\u2013  is  highly  customized  to  the  application  at  hand.  4.2 CBR and Games   domains,   the   Many  different  implementations  of  CBR  exist  in  games.  CBR  technology  is  nicely  suited  for  recognizing  complex  situations much easier and more elegant than traditional parameter  comparison  or  function  evaluation.  There  are  especially  evident  cases in real time strategies where different attack and defense of  global strategies are nicely defined by CBR datasets and later used  in  the  running  games.  Also  intelligent  bots  behavior  is  also  another typical example. Depending on the number of enemy bots  the  layout  of  the  terrain  and  position of  human  players  the  CBR  system  finds  the  closest  CBR  case  and  employs  that  strategy  against the human players which in prior evaluation was proved to  be highly efficient.   5. Game Trees with AI Planning \u2013 Tic-tac-toe  In  order  to  show  the  expressive  power  of  AI  Planning  in  defining strategies  for games, and the use of these plans to build  Game  Trees I implemented an algorithm that builds Game  Trees  for the Tic-Tac-Toe game.   The  game  tree  of  Tic-Tac-Toe  shows  255,168  possible  games  of  which  131,184  are  won  by  X  (the  first  player),  77904  are won by O and the rest 46,080 are draw [13]. All these games  can be derived from building a complete Game Tree.    Even  though  it  is possible  to  build  a  complete  game  tree  of  Tic-tac-toe  it  is  definitely  not  an  optimal  solution.  Many  of  the  moves in this tree would be symmetrical and also there are a many  moves  that  would  be  illogical  or  at  least  a  bad  strategy  to  even  consider.    So what strategy  should X (the first player) choose in order   to win the game?   There  are  few  positions  that  lead  to  certain  victory.  These  positions  involve  simultaneous  attack  on  two  positions  so  the  other player could not defend, basically the only trick in Tic-Tac- Toe.   Figure 6: Tic-tac-toe winning strategy positions   Position 1 leads to victory if the two of the three fields: top  middle,  bottom  left  corner  and  bottom  right  corner  are  free  [Figure 6].   Position 2 lead to victory if two of the three fields: top right  corner, bottom right corner and bottom middle are free [Figure ].    The game starts from an empty table.   The two relevant strategies that would lead to these positions   are to take one corner or to take the center [Figure 7].   Figure 7: Tic-tac-toe Two starting moves   The  center  position  as  we  can  see  in  the  simulation  results  lead  to  a  bigger  number  of  victorious  endings  but  it  is  also  a  straight forward strategy with obvious defense strategy.   At this point we need to consider the moves of the opponent.  If  we take the left branch the opponent moves can be a center, a  corner  or  a  middle  field.  We  also  need  to  differentiate  with  a  move to a corner adjacent with our like top left or bottom right or  across the center to bottom right [Figure 8].   Figure 8: Tic-tac-toe opponent response to corner move   In  cases  one  and  two,  we  have  a  clear  path  to  executing  strategy  3  so  we  need  to  capture  the  diagonally  opposite  field.  And as for the third case the best way to go is to capture the center  and go for strategy 1 or 2 depending of the opponent\u2019s next move.    Figure 9: Tic-tac-toe move 2 after corner opening   The first move leads to certain victory, O will have to go to  the  center  and  X  will  achieve  strategy  3  [Figure  9].  The  second  move is a possible way to strategy 3 if O makes a mistake in the  next  loop,  so  X  goes  to  the  opposite  corner.  For  the  third  case  since  O  is  playing  a  valid  strategy  the  only  move  that  leaves  a  possible mistake from O would be to take the center and wait for  O to go to the middle and then achieve strategy 1 or 3 which will  be  a  symmetric  situation  to  the  one  that  we  will  find  if  we  branched with the center.   Figure 10: Tic-tac-toe opponent response to center move   If  we  go  back  to  the  second  branch  [Figure  10],  a  possible  way for the second player to engage is corner or middle. The first   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \fInteractive and Adaptable Media  299  move  is  a  valid  strategy  for  O  and  can  be  mee corner move from X to try a mistake from O in  the same as in the third case above from the pre another  move  would  be  go  to  the  middle  wh achieves strategy 1 or 2.    et  with  a  opposite  the future exactly  evious branch, and  here  X  eventually   This HTN when executed will re  game scenarios. By creating nodes from them with branches with the move of t tree for the Tic-tac-toe game over whi algorithm.   esult with plans for possible  m each position and linking  the player we create a game  ich we can run the minimax   Figure 11: Tic-tac-toe Move 2 after cent The fist move will lead to win if O moves  draw if it  goes  for the corners [Figure 11]. In t has  to  block  the  lower  left  corner  which  leave middle left or corner left which are strategy 1 and To sum the strategies for the planning, first  corner strategy for the beginning. Then for the ce the  corners  with  the  particularly  the  one  oppo holds. If the center is empty for the second strate we go for the opposite corner. After this point w opponent  or  try  to  implement  strategies  1,  2  or victory.   Plan 1: Take center   Preconditions: Center empty  Plan 2: Take corner   Preconditions: All corners empty  Plan 3: Take corner after center  Preconditions: We have center take corner oppos opponent has  Plan 4: Take diagonal corner  Preconditions: We have a corner, the opponent ha  the corner opposite to the one we have is free.  Plan 5: Block  Precondition: The opponent has tree tokens in a r agonal  Plan 6: Win  Preconditions: We have two tokens in a row, colu nd the third place is free  Plan 7: Tie  Preconditions: If all places are taken, it\u2019s a tie.  5.1 Hierarchical Task Network   ter opening to the middle or a  the second case O  es  X  to  go  for  the  d 2. we have center or  enter we try to get  osite  to  the  one  O  egy we go for it or  we either block the  r  3  which  lead  to   site to the  one the   as the ce\u2212nter and  row, colu\u2212mn or di  mn or dia\u2212gonal a  Top level task is Play [Figure 12]. This is a  can  be  derived  into:  Win,  Block,  Tie  or  Sear Search for plan is derived to both Plan 1 and Pla Plan 4, which later leads to a call for the oppon recursive call to Play.   a complex task and  rch  for  Plan.  The  an 2 or Plan 3 and  nent\u2019s move and a   Figure 12: Tic-tac-toe HT  TN  This set up with 7 plans with 3 ta for Tic-tac-toe which considers all pos player  with only 457 games, 281 of  w and  0  where  the  second  opponent  w reduction over the 255, 168 possible g tree. These reductions can be very use computing  capabilities  but  also  we  pr that planning can be very efficient if d trees  by  applying  reasoning  very  reasoning.   arget strategies creates a tree  ssible moves for the second  which X wins 176 are draw  wins.  This  is  a  significant  ames with a complete game  eful for devices with limited  rove  a  very  important  point  designing meaningful game  similar  to  human  player   me  tree  are  also  possible  if  d, in other words if we drop  moves of the opponent.   Further  improvements  to  the  gam the opponents moves are also planned all the meaningless and symmetrical m 6. Game AI in Monopoly  6.1 Overview of the AI Imp The  AI  agent  is  responsible  for  players in the game. The core principle a Game Tree with all the sensible move make  from  the  current  point  of  time minimax  algorithm  the  agent  selects  t would  bring  the  computer  player  mo with  the  highest  probability.  Building  that would be big enough to consider  is  obstructed  by  the  vastness  of  poss with all the possible random landings  nodes  of  the  game  tree  exponentially tackle  this  problem  the  AI  agents  discussed technologies: Case Based Re The  technologies  are  employed  First the agent searches the CBR datab largest similarity  with the current state associated  with  a  playing  strategy.  Th that the planner needs to build plans f consecutive player  moves that bring th way only moves that are part of that str being a small fraction of the overall po edges of the game tree at each level dec At each level of the game tree the of  a  single  player.  After  the  strateg considered the response to those strate by the opponent(s). The move of the  probability  distribution  of  the  dice  as  player.  A  more general strategy  needs opponent\u2019s  (human  player)  moves  sin the  expertise  of  the  opponent.  This  ge more plausible moves than the focused After  covering  all  opponents  t deducting  a  feature  move  of  the  com CBR  selected  plan  strategy.  After  strategies  and  reaching  a  reasonable  s into  account  the  memory  limits  an probabilities  that  the  move  is  possible the dice the building of the Game Tre algorithm  searches  the  Game  Tree  favorable  move  for  the  AI  player  usi The process is repeated each time the A  plementation the  moves  of  the  artificial  e of the AI agent is building  es that all the players would  e  forward.  Then  using  the  the  move  that  in  the  future  ost  favorable  game  position  a  Game  Tree  in  this  game  sufficient number of moves  sible  moves  in  combination  of the dice. The number of  y  grows  at  each  level.  To  incorporates  two  already   easoning and AI Planning.   in  the  following  manner.  base to find the case with the  e of the board. This case is  he  strategy  consists  of  goal  for, and the plans consist of  he player to that goal. This  rategy are considered, those  ossible moves the number of  creases immensely.  e model considers the moves  gies  of  the  AI  player  are  egies needs to be considered  opponent(s) depends of the  well  as  the  strategy  of  the  s to be implemented for the  nce  we  cannot  be  aware  of  eneral  strategy  would  bring  d strategy of the AI player.   the  agent  comes  back  to  mputer  player  by  using  the  creating  several  loops  of  size  of  a  Game  Tree  taking  nd  the  rapidly  decreasing  e  due  to  the  distribution  of  ee stops. Then the minimax  and  decides  on  the  most  ing  the  minimax  algorithm.  AI player is up.   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f300  DIMEA 2008  On  the  other  hand  the  MonopolySolution  class  holds  the  three  particular  attributes  that  are  needed  for  the  planning,  the  planning Domain, State and TaskList.   The  game  is  implemented  by  using  the  Model-View- Controller  software  development  pattern.  The  controller  is  responsible  for  implementing  the  game  rules  and  handling  all  of  the  events  in  the  game  like  roll  of  dice,  input  commands  for  trading,  auctioning  and  etc  from  the  players.  The  View  layer  is  responsible  for  displaying  the  board  and  all  of  the  input  widgets  on  to  the  game  screen,  and  the  models  are  data  structures  representing the game state [Figure 14].   Buying,  auctioning  and  trading  game  moves  are  always  accompanied  by  return  of  investment  calculations  in  making  the  plans. These calculations represent adaptation of the more general  planning  associated  with  the  cases  in  the  CBR  database.  These  adaptations  are  necessary  due  to  the  fact  that  the  cases  do  not  identically  correspond  to  the  situation  on  the  table.  In  addition  calculating the game position value of each node of the game tree  is  done  by  heuristic  functions  incorporate  economic  calculations of net present value, cash, and strategic layout and so  on.  For  example  railroads  in  monopoly  are  known  to  be  strategically  effective  because  they  bring  constant  income  even  though  the  income  can  be  smaller  than  building  on  other  properties.   6.2 Details on the CBR Implementation   that   The  implementation  of  the  CBR  is  by  using  the  JColibri2  platform.    JColibri2  is  an  object-oriented  framework  in  Java  for  building  CBR  systems  that  is  an  evolution  of  previous  work  on  knowledge intensive CBR [14].    For this implementation we need to look into three particular  classes  of  the  JColibri2  platform.  The  StandardCBRApplication,  Connector,  CBRQuery.  For  a  JColibri2  implementation  the  StandardCBRApplication interface needs to be implemented.    The CBR cycle executed accepts an instance of CBRQuery.  This  class  represents  a  CBR  query  to  the  CBR  database.  The  description  component  (instance  of  CaseComponent)  represents  the description of the case that will be looked up in the database.  All  the  solutions  case  CaseComponent interface.   implementing   cases   and   are   The  JColibri2  platform  connects  to  the  CBR  database  via  a  Connector  class.  Each  connector  implements  all  the  necessary  methods for accessing the database, retrieval of cases, storing and  deletion  of  cases.  This  implementation  uses  a  custom  XML  structure  for  holding  the  CBR  cases.  Since  the  game  will  not  update  the  CBR  database  only  read  it,  a  XML  solution  satisfies  the needs. The XML file to a certain extent is similar to the XML  representation  of  the  board.  We  are  interested  in  finding  one  CBRCase that is the most similar case to the situation in the game  at  the  time  of  the  search.  This  procedure  is  done  in  the  cycle  method of the CBRApplication. The JColibri2 CBR comparison is  done by Nearest Neighbor (NN) search method.    JColibri2  offers  implementations  for  NN  search  algorithms  of  simple  attributes.  These  implementations  are  called  local  similarities.  For  complex  attributes  like  in  our  case  global  customized similarity mechanisms need to be implemented.   The  MonopolyDescription  class  [Figure  13]  is  basically  a  serialization of the  GameState. It holds all the information about  the state of the board, the players, their amount of cash etc.    Figure 13: Class diagram of the Monopoly Case component  models   Figure 14: Class diagram of the Monopoly models   6.2.1 Complex Similarity representation in CBR   implementing   The  similarity  measurement  part  of  the  Nearest  Neighbor  implemented  by  the  algorithm  JColibri2  is  LocalSimiralrityFunction  the  GlobalSimiralityFunction  and  interface. A local similarity function is applied to simple attributes  by the NN algorithm, and a global similarity function is applied to  compound  attributes.  In  the  case  of  our  implementation  the  attributes  of  the  MonopolyDescription  are  compound  attributes  describing  the  state  of  the  board,  number  of  players,  amount  of  cash  for  every  player  and  etc.  Since  MonopolyDescription  is  a  custom  CaseComponent  a  global  similarity  function  needs  to  be  implemented  to  accurately  find  the  distance  between  different  CBR cases.   The similarity mechanism is inseparable core element of the  CBR  system.  This  mechanism  represents  how  the  CBR  decides  which  strategy  is  best  suited  for  the  particular  situation  by   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \fInteractive and Adaptable Media  301  calculating  the  distance  or  similarity  to  other  cases  in  the  database.    For  the  monopoly  implementation  we  need  to  consider  several  basic  strategies.  Monopoly  is  based  on  investing  in  properties and receiving revenues from those investments. One of  the basic strategies of the game is to build a set of properties that  will  bring  constant  income  larger  than  the  one  of  the  opponents.  So in time the opponents will have to declare bankruptcy. But on  the other hand over investment can lead to too stretched resources  with  low  income  that  will  eventually  drove  the  player  to  bankruptcy.  To  decide  on  these  two  we  need  a  clear  separation  into two groups of cases in the CBR database. The first group of  cases will represent a situation on the board where the player has  significant  income  per  loop  formed  of  one  or  more  color  group  properties, maybe railroads, some buildings on them and so on. It  is  important  to  note  that  in  this  case  the  player  is  better  situated  than his opponents so he only needs to survive long enough to win  the  game.  In  the  other  group  of  cases  either  the  opponent  is  not  well  positioned on  the board  or  its  opponents  are better  situated.  In  this  case  further  investments  are  necessary  to  improve  the  situation  so  the  player  can  have  a  chance  of  winning  in  the  long  run.    These metrics can be owning color groups, valuing groups of  railroads, evaluating the other opponents as well, and considering  the amount of cash. As it is obvious in monopoly the number of  streets is not as nearly as important as the combination of streets  the  player  owns.  It  is  also  important  to  note  that  one  CBR  case  does not hold only a single strategy in place, but its solution can  have multiple different strategic goals. For example one CBR case  might simultaneously say buy this land to form a color group but  also  trade  some  other  unimportant  property  to  increase  cash  amount.    The cases do not represent all possible combinations of board  positions. They are only representation of typical game scenarios.  The CBR Case solutions do not give exact instructions in general  but  rather  strategic  goals.  For  example  one  CBR  Solution  might  say trade the streets that  you only have one of  each  for the ones  that you have two of that color already. Then the planner based on  the situation on the board needs to decompose this high level task  to a low level operations. Like offer \"Mediterranean Avenue\" for  \"Reading Railroad\" and offer $50. The exact amounts and actual  streets are left to the planer to evaluate.    The monopoly CBR database is currently in development on  a  monopoly  clone  game  called  Spaceopoly.  The  cases  are  architected  based  on  human  player  experience  and  knowledge.  There is a plan of making a number of slightly different strategies  that  differ  on  the  style  of  playing  and  then  running  simulation  tests that would determine the particular validity of each database  as  well  as  validity  of  certain  segments  of  the  strategy  or  even  particular cases in the database.    JSHOP2  uses  ordered  task  decomposition  in  reducing  the  HTN to list of primitive tasks  which form the plans.  An ordered  task decomposition planner is an HTN planner that plans for tasks  in  the  same  order  that  they  will  be  executed.  This  reduces  the  complexity  of  reasoning  by  removing  a  great  deal  of  uncertainty  about  the  world,  which  makes  it  easy  to  incorporate  substantial  expressive  power  into  the  planning  algorithm.  In  addition  to  the  usual HTN methods and operators, the planners can make use of  axioms,  can  do  mixed  symbolic/numeric  conditions,  and  can  do  external function calls.    In  order  for  the  JSHOP2  planer  to  generate  plans  it  needs  tree  crucial  components:  Domain,  State  and  Tasks.  The  Domain  defines  all  the  functionalities  that  the  particular  domain  offers.  These  are  simple  and  complex  tasks.  The  complex  tasks  also  called methods create the hierarchy with the fact that they can be  evaluated  by  simple  tasks  of  other  complex  tasks.  This  is  how  a  hierarchical structure of tasks is formed. The problem reduction is  done by reducing the high level complex tasks to simpler until all  the tasks are primitive. The list of primitive tasks forms the plan.   The  State  represents  the  state  of  the  system.  It  is  a  simple  database of facts that represent the state of the system. The State  is  necessary  to  determine  the  way  the  problems  or  tasks  are  reduced  to  their  primitive  level.  The  reduction  is  done  by  satisfying  different  prerequisites  set  in  the  methods;  these  prerequisites  are  defined  in  the  state.  The  Tasks  are  high  level  tasks or methods defined in the Domain. The planner based on the  State and the goals selects one or more high level tasks that need  to be reduced to plans [Figure  15].   Tasks  Core Planner   State  Plan  Figure 15: Diagram of a Planner   The  plans  then  generate  the  game  moves.  The  number  of  moves  generated  by  the  plans  is  just  a  fraction  of  the  possible  moves  at  that  point.  This  reduces  the  game  tree  providing  the  opportunity to generate smaller and deeper game trees and making  more efficient decisions in general.    7. Conclusion   Even  though  the  results  from  the  CBR  database  are  not  complete  at  this  time  partial  strategies  are  implemented  as  cases  and  recognized  during  game  play  by  the  CBR  system.  These  smaller  local  strategies  coupled  with  more  global  higher  level  strategies  that  are  particularly  important  at  the  beginning  of  the  game  would  form  a  complete  CBR  database  and  represent  a  knowledge engineered style of playing of the AI player.    The  actual  execution  of  the  strategies  will  not  differ  from  strategy to strategy since the plan execution is more related to the  structure and rules of the game than to the actual playing strategy.   The AI Planning approach is a proven method by the tic-tac- toe  experiment  and  is  suitable  for  implementing  the  strategies  associated with the CBR cases.   6.3 Details on the Planning Implementation   For  the  purpose  of  planning  this  implementation  uses  a  the  JSHOP2  planner.  The  Java  Simple  modification  of  Hierarchical  Ordered  Planner  2  is  a  domain  independent  HTN  planning system [15].    This  approach  in  general  benefits  from  both  technologies,  CBR  as  well  as  AI  Planning  and  comprises  an  elegant  solution.  Even  though  AI  Planning  can  be  enough  as  a  single  technology  for  some  simpler  problems  like  tic-tac-toe  the  complexity  of  Monopoly would mean that the Planner would have to incorporate   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f302  DIMEA 2008  large and complex domain and a very big state model. The CBR  application helps reduce this complexity by focusing the planning  on  smaller  domain  of  the  game.  Basically  the  CBR  reduces  the  overall  goal  of  the  play  (wining  the  game)  to  smaller  more  concrete  goals  suitable  to  the  particular  state  of  the  game,  thus  reducing  the  need  for  global  planning  strategies  and  complex  planning domain.    Furthermore  this  symbiosis  of  technologies  gives  way  for  more precise and finely tuned strategies which can be difficult to  include into global plan for the whole game. One simple example  for  the  Monopoly  game  would  be  this:  Sometimes  it\u2019s  better  to  stay  in  jail  because  rolling  double  increases  the  probability  of  landing  on  some  field  (two,  four,  six,  eight,  ten  or  twelve  steps  from  the  jail)  that  can  be  of  great  importance  to  the  rest  of  the  game.  These  and  similar  small  local  strategies  can  be  easily  recognized by similar cases in the CBR database.    In  other  words  the  system  is  flexible  enough  so  that  new  strategies can be incorporated easily missing strategies can be also  recognized by the distance metrics as well as wrong assumptions  in the strategies can be easily recognized.   One other important property of  the system is that is highly  configurable.  The  game  its  self  can  be  diversely  different  depending  on  the  configuration  of  the  board.  Even  though  the  platform  is  restricted  to  Monopoly  type  of  games,  changing  the  layout  and  values  of  the  fields  effectively  brings  completely  different  properties  of  the  game.  In  addition  the  CBR  database  represents the entire experience  of the AI Player. It can be filled  with rich set of strategies or even configured with different flavors  of difficulties of play, this of course coupled with the domain of  the planner which can differ from a case to a case as well.    8. Future Work   Further  exploration  of  this  technology  would  go  towards  complete  implementation  of  an  AI  aware  agent  for  monopoly.  Initial  results  from  the  local  cases  with  more  specific  strategies  show CBR as a capable tool for representing expertise in playing  the  game.  Completing  the  more  general  strategies  and  coupling  them  with  the  planning  domain  will  give  precise  results  on  the  benefits from this architecture.   There is also need for exploring the planning of strategies of  opponents.  This  task  is  to  some  extent  different  because  we  cannot  always  expect  the  opponent  to  select  the  best  move  we  think.  In  the  Tic-tac-toe  example  all  possible  moves  of  the  opponent  were  taken  into  consideration,  if  we  used  the  same  planner  for  the  opponent  only  tie  games  would  result  from  the  game tree. In other words mistakes of the players also need to be  considered.    The CBR Platform brings other functionalities well worth of  exploring as well. The revision stage of the JColibri2 platform is  basically capable of fine tuning strategies or even developing new  strategies  for  the  games.  A  well  written  underlying  AI  planning  model  with a  capable  feedback of the  game tree evaluation back  to  the  CBR  revision  capability  can  be  an  interesting  concept  in  automatic experience acquisition for the AI model.   There  are  also  many  other  fields  were  combined  CBR  and  planning  approach  can  be  incorporated  into  a  problem  solution.  This combination is analogous in a big extent to a human way of   reasoning.  People  in  addition  to  logic  of  reasoning  in  situations  with  lack  of  information  rely  to  planning  strategies  and  prior  experience,  exactly  the  intuition  behind  CBR  \u2013  AI  Planning  architecture.    9. ACKNOWLEDGMENTS   We  would  like  to  thank  Prof.  Sofia  Tsekeridou  for  her  involvement  in  the  valuable  discussions  we  had  on  the  topic  of  CBR.   10. REFERENCES  [1] Minimax. Wikipedia. [Online] [Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Minimax.   [2] Von Neumann, J: Zur theorie der gesellschaftsspiele Math.   Annalen. 100 (1928) 295-320   [3] Automated Planning. Wikipedia. [Online] [Cited: April 23,   2008.] http://en.wikipedia.org/wiki/Automated_planning.   [4] Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based  Strategy Game with Plan Adaptation and Ontology-based  retrieval.  [5] K. Erol, J. Hendler, and D. Nau (1994). Semantics for   hierarchical task-network planning. Technical Report TR-94- 31, UMIACS.   [6] Smith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning  approach decrarer play in contract bridge. Computational  Intelligence. 1996, Vol. 12, 1.   [7] One Jump Ahead: Challenging Human Supremacy in   Checkers. J.Schaeffer. s.l. : Springer-Verlag, 1997.   [8]  IBM. How Deep Blue works. [Online] 1997. [Cited: April  23, 2008.]  http://www.research.ibm.com/deepblue/meet/html/d.3.2.html  [9] Ghallab, Malik, Nau, Dana and Traverso, Paolo.  Automated Planning theory and practice. s.l. : Morgan  Kaufmann Publishers, May 2004. ISBN 1-55860-856-7.   [10] Case Based Reasoning. Experiences, Lessons and Future.  Leake, David. s.l. : AAAI Press. MIT Press., 1997.   [11] Applying case-based reasoning: techniques for enterprise   systems. Watson, I. San Francisco, CA, USA : Morgan  Kaufmann Publishers Inc., 1998.   [12] Plaza, A. Aamodt and E. Case-based reasoning:   Foundational issues, methodological. AI Communications.  1994, 7(i).   [13] Tic-tac-toe. Wikipedia. [Online] [Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Tic-tac-toe.   [14] D\u00edaz-Agudo, B. and Gonz\u00e1lez-Calero, P. A. An   architecture for knowledge intensive CBR systems. Advances  in Case-Based Reasoning \u2013 (EWCBR\u201900). New York :  Springer-Verlag, Berlin Heidelberg, 2000.   [15] Ilghami, Okhtay and Nau, Dana S. A General Approach to   Synthesize Problem-Specific Planners. 2003.   3rd International Conference on Digital Interactive Media in Entertainment and Arts",
  "references": [
    "1] Minimax. Wikipedia. ",
    "Online] ",
    "Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Minimax.   ",
    "2] Von Neumann, J: Zur theorie der gesellschaftsspiele Math.   Annalen. 100 (1928) 295-320   ",
    "3] Automated Planning. Wikipedia. ",
    "Online] ",
    "Cited: April 23,   2008.] http://en.wikipedia.org/wiki/Automated_planning.   ",
    "4] Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based  Strategy Game with Plan Adaptation and Ontology-based  retrieval.  ",
    "5] K. Erol, J. Hendler, and D. Nau (1994). Semantics for   hierarchical task-network planning. Technical Report TR-94- 31, UMIACS.   ",
    "6] Smith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning  approach decrarer play in contract bridge. Computational  Intelligence. 1996, Vol. 12, 1.   ",
    "7] One Jump Ahead: Challenging Human Supremacy in   Checkers. J.Schaeffer. s.l. : Springer-Verlag, 1997.   ",
    "8]  IBM. How Deep Blue works. ",
    "Online] 1997. ",
    "Cited: April  23, 2008.]  http://www.research.ibm.com/deepblue/meet/html/d.3.2.html  ",
    "9] Ghallab, Malik, Nau, Dana and Traverso, Paolo.  Automated Planning theory and practice. s.l. : Morgan  Kaufmann Publishers, May 2004. ISBN 1-55860-856-7.   ",
    "10] Case Based Reasoning. Experiences, Lessons and Future.  Leake, David. s.l. : AAAI Press. MIT Press., 1997.   ",
    "11] Applying case-based reasoning: techniques for enterprise   systems. Watson, I. San Francisco, CA, USA : Morgan  Kaufmann Publishers Inc., 1998.   ",
    "12] Plaza, A. Aamodt and E. Case-based reasoning:   Foundational issues, methodological. AI Communications.  1994, 7(i).   ",
    "13] Tic-tac-toe. Wikipedia. ",
    "Online] ",
    "Cited: April 23, 2008.]   http://en.wikipedia.org/wiki/Tic-tac-toe.   ",
    "14] D\u00edaz-Agudo, B. and Gonz\u00e1lez-Calero, P. A. An   architecture for knowledge intensive CBR systems. Advances  in Case-Based Reasoning \u2013 (EWCBR\u201900). New York :  Springer-Verlag, Berlin Heidelberg, 2000.   ",
    "15] Ilghami, Okhtay and Nau, Dana S. A General Approach to   Synthesize Problem-Specific Planners. 2003.   3rd International Conference on Digital Interactive Media in Entertainment and Arts  \f"
  ],
  "url": "https://drive.google.com/uc?id=1asTkg4Uf6_MHZeqZ642nPzp-IsQ4mTKM",
  "date": "2024-01-09 18:22:51",
  "is_published": false
}
{
  "title": "Framing the News: From Human Perception to Large Language Model Inferences",
  "authors": [
    "David Alonso del Barrio",
    "Idiap Research Institute",
    "Daniel Gatica-Perez",
    "Idiap Research Institute and EPFL"
  ],
  "institutions": [
    "Switzerland",
    "Switzerland"
  ],
  "abstract": "Identifying the frames of news is important to understand the arti- cles\u2019 vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of jour- nalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to under- stand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No- Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Nat- ural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT- 3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.",
  "keywords": [
    "Covid-19 no-vax",
    " news framing",
    " GPT-3",
    " prompt-engineering",
    " trans-",
    "formers",
    " large language models"
  ],
  "article": "In recent years, there has been a proliferation in the use of concepts such as data journalism, computational journalism, and computer- assisted reporting [15] [29], which all share the vision of bridging journalism and technology. The progress made in NLP has been gradually integrated into the journalistic field [5][8][54]. More specifically, machine learning models based on transformers have been integrated in the media sector in different tasks [41] such as the creation of headlines with generative languages models [17], summarization of news articles [28][27], false news detection [49], and topic modeling and sentiment analysis [25]. The development of large language models such as GPT-3 [9], BLOOM [51] or ChatGPT show a clear trend towards human-machine interaction becoming easier and more intuitive, opening up a wide range of research possibilities. At the same time, the use of these models is also associated with a lack of transparency regarding how these models work, but efforts are being made to bring some transparency to where they cannot [35]. Based on the premises that these models open up a wide range of research directions [7], and that at the same time (and needless to say) they are not the solution to all problems, can be potentially useful, while acknowledging and systematically documenting their limitations [56]. More specifically, the aim of this work is to analyze the performance of GPT-3.5 for a specific use case, namely the analysis of frames in news, from an empirical point of view, with the objective of shedding light on a potential use of generative models in journalistic tasks.  Frame analysis is a concept from journalism, which consists of studying the way in which news stories are presented on an issue, and what aspects are emphasized: Is a merely informative vision given in an article? Or is it intended to leave a moral lesson? Is a news article being presented from an economic point of view? Or from a more human, emotional angle? The examples above correspond to different frames with which an article can be written. The concept of news framing has been studied in computing as a step beyond topic modeling and sentiment analysis, and for this purpose, in recent years, pre-trained language models have been used for fine-tuning the classification process of these frames [60] [10], but the emergence of generative models opens the possibility of doing prompt-engineering of these classification tasks, instead of the fine-tuning approach investigated so far.  Our work aims to address this research gap by posing the fol-  lowing research questions:  RQ1: What are the main frames in the news headlines about the anti-vaccine movement, as reported in newspapers across 5 European countries?  627\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  RQ2: Can prompt engineering be used for classification of head-  lines according to frames?  By addressing the above research questions, our work makes the  following contributions:  Contribution 1. We implemented a process to do human an- notation of the main frame of 1786 headlines of articles about the Covid-19 no-vax movement, as reported in 19 newspapers from 5 European countries (France, Italy, Spain, Switzerland and United Kingdom.) At the headline level, we found that the predominant frame was human interest, where this frame corresponds to a per- sonification of an event, either through a statement by a person, or the explanation of a specific event that happened to a person. Furthermore, we found a large number of headlines annotated as containing no frame, as they simply present information without entering into evaluations. We also found that for all the countries involved, the distribution of frame types was very similar, i.e., hu- man interest and no frame are the two predominant frames. Finally, the generated annotations allowed to subsequently study the per- formance of a large language model.  Contribution 2. We studied the performance of GPT-3.5 on the task of frame classification of headlines. In addition to using the fine-tuning approach from previous literature, we propose an alternative approach for frame classification that requires no labeled data for training, namely prompt-engineering using GPT-3.5. The results show that fine-tuning with GPT-3.5 produces 72% accuracy (slightly higher than other smaller models), and that the prompt- engineering approach results in lower performance (49% accuracy.) Our analysis also shows that the subjectivity of the human labeling task has an effect on the obtained accufracy.  The paper is organized as follows. In Section 2, we discuss related work. In Section 3, we describe the news dataset. In Section 4, we describe the methodology for both human labeling and machine classification of news frames. We present and discuss results for RQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide conclusions in Section 7.  2 RELATED WORK Framing has been a concept widely studied in journalism, with a definition that is rooted in the study of this domain [23]: \u201cTo frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a par- ticular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.\u201d  For frame recognition, there are two main approaches: the induc- tive approach [16], where one can extract the frames after reading the article, and the deductive approach [38], where a predefined list of frames exists and the goal is to interpret if any of them ap- pears in the article. In the deductive case, there are generic frames and subject-specific frames, and the way to detect them typically involves reading and identifying one frame at a time, or through answers to yes/no questions that represent the frames. Semetko et al. [52] used 5 types of generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) based on previous literature, and they defined a list of 20 yes/no questions to detect frames in articles. For instance, the questions about morality are the following: \"Does the story contain any moral  message? Does the story make reference to morality, God, and other religious tenets? Does the story offer specific social prescriptions about how to behave?\", and so on for each of the frame types. This categorization of frames has been used in various topics such as climate change [18] [19], vaccine hesitance [13], or immigration [34].  We now compare the two approaches on a common topic, such as Covid-19. Ebrahim et al. [21] followed an inductive approach in which the frames were not predefined but emerged from the text (e.g., deadly spread, stay home, what if, the cost of Covid-19) using headlines as the unit of analysis. In contrast, the deductive approach has studied very different labels. El-Behary et al. [22] followed the method of yes/no questions, but in addition to the 5 generic frames presented before, they also used blame frame and fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5 generic frames with yes/no questions, while Catal\u00e1n-Matamoros et al. [14] used the 5 frames and read the headline and subheadline to decide the main frame. Table 1 summarizes some of the the existing approaches. This previous work showed how frame labels can be different, and also that frame analysis has been done at both headline and article levels. These two approaches (inductive and deductive) that originated in journalism have since been replicated in the computing literature.  We decided to follow the deductive approach because a prede- fined list of frames allows to compare among topics, countries, previous literature, and also because they represent a fixed list of labels for machine classification models. Furthermore, the induc- tive approach tends to be more specific to a topic, and from the computing viewpoint, past work has tried to justify topic modeling as a technique to extract frames from articles.  Yl\u00e4-Antitila et al. [60] proposed topic modeling as a frame ex- traction technique. They argued that topics can be interpreted as frames if three requirements are met: frames are operationalized as connections between concepts; subject-specific data is selected; and topics are adequately validated as frames, for which they suggested a practical procedure. This approach was based on the choice of a specific topic (e.g., climate change) and the use of Latent Dirichlet Allocation (LDA) as a technique to extract a number of subtopics. In a second phase, a qualitative study of the top 10 words of each subtopic was performed, and the different subtopics were elimi- nated or grouped, reducing the number and establishing a tentative description. In a third phase, the top 10 articles belonging to that frame/topic were taken, and if the description of the topic fitted at least 8 of the 10 articles, that topic/frame remained. The frames found in this article were: green growth, emission cuts, negotiations and treaties, environmental risk, cost of carbon emissions, Chinese emissions, economics of energy production, climate change, en- vironmental activism, North-South burden sharing, state leaders negotiating, and citizen participation.  From Entman\u2019s definition of frame [23], it seems that the deduc- tive approach is more refined than the inductive approach (which seems to resemble the detection of sub-themes.) For example, with regard to climate change, there are stories on how people have been affected by climate change from an emotional point of view, thus personalizing the problem. In this case, we could categorize the corresponding frame as human interest, as the writer of the article is selecting \"some aspects of a perceived reality and make them  628\fFraming the News: From Human Perception to Large Language Model Inferences  more salient\". The language subtleties with which news articles are presented cannot be captured with basic topic modeling.  Isoaho et al.[30] held the position that while the benefits of scale and scope in topic modeling were clear, there were also a number of problems, namely that topic outputs do not correspond to the methodological definition of frames, and thus topic modeling remained an incomplete method for frame analysis. Topic modeling, in the practice of journalistic research, is a useful technique to deal with the large datasets that are available, yet is often not enough to do more thorough analyses [31]. In our work, we clearly notice that frame analysis is not topic modeling. For example, two documents could be about the same topic, say Covid-19 vaccination, but one article could emphasize the number of deaths after vaccination, while the other emphasized the role of the vaccine as a solution to the epidemic.  We also consider that the larger the number of possible frame types, the more likely it is to end up doing topic modeling instead of frame analysis. Using a deductive approach, Dallas et al. [12] created a dataset with articles about polemic topics such as immigration, same sex marriage, or smoking, and they defined 15 types of frames: \"economic, capacity and resources, morality, fairness and equality, legality, constitutionality and jurisprudence, policy prescription and evaluation, crime and punishment, security and defense, health and safety, quality of life, cultural identity, political, external regulation and reputation, other\". In this case, they authors did not use a list of questions. Instead, for each article, annotators were asked to identify any of the 15 framing dimensions present in the article and to label text blurbs that cued them (based on the definitions of each of the frame dimensions) and decide the main frame of each article. In our case, we followed the idea of detecting the main frame by reading the text instead of answering questions, but instead of using the 15 frames proposed in [12] , we used the 5 generic frames proposed in [52].  A final decision in our work was the type of text to analyze, whether headlines or whole article. For this decision, the chosen classification method was also going to be important. For example, Khanehzar et al. [33] used traditional approaches such as SVMs as baseline, and demonstrated the improvement in frame classifica- tion with the use of pre-trained languages models such as BERT, RoBERTa and XLNet, following a fine-tuning approach, setting as input text a maximum of 256 tokens (although the maximum number of input tokens in these models is 512 tokens.) Liu et al. [37] classified news headlines about the gun problem in the United States, arguing for the choice of headlines as a unit of analysis based on previous journalism literature [6], [44], that advocated for the importance and influence of headlines on readers and the subsequent perception of articles. From a computational viewpoint, using headlines is also an advantage, since you avoid the 512 token limitation in BERT-based models. Therefore, we decided to work with headlines about a controversial issue, namely the Covid-19 no-vax movement.  Continuing with the question of the methods used for classi- fication, much work has been developed in prompt engineering, especially since the release of GPT-3. Liu et al.[36] presented a good overview of the work done on this new NLP paradigm, not only explaining the concept of prompt engineering, but also the differ- ent strategies that can be followed both in the design of prompts,  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  Table 1: Summary of deductive approaches for frame analysis  Ref Frames  Goal  Technique  To label frames of full articles  [12] 15 generic frames: \"Economic\", \"Capac- ity and resources\", \"Morality\", \"Fair- ness and equality\", \"Legality, constitu- tionality and jurisprudence\", \"Policy prescription and evaluation\", \"Crime and punishment\", \"Security and de- fense\", \"Health and safety\", \"Quality of life\", \"Cultural identity\", \"Public opin- ion\", \"Political\", \"External regulation and reputation\", \"Other\".  [33] 15 generic frames  Classification  [52] 5 generic frames: \"human interest\", \"conflict\", \"morality\", \"attribution of responsibility\", and \"economic conse- quences\".  \u201cSociety/Culture\u201d, ,  [37] 9 specific frames:\u201cPolitics\u201d, \u201cPublic opinion\u201d, and \u201c2nd \u201cEconomic consequences\u201d Amendment\u201d (Gun Rights), \u201cGun control/regulation\u201d, \u201cMental health\u201d, \u201cSchool/Public space safety\u201d, and \u201cRace/Ethnicity\u201d.  [22] 5 generic frames + blame frame and  fear frame 5 generic frames  [1]  To label frames of full articles  To label frames of full articles/ Classification  To label frames of full articles To label frames of full articles  [50] 5 generic frames + pandemic frames  [14] 5 generic frames, journalistic role and  pandemic frames  To label frames of full articles To label frames of full articles  the Reading article, full the annotator defines the main frame  BERT based models Yes/No ques- tions.  the Reading full article, the annotator the defines main frame. BERT based models Yes/No ques- tions. Reading the article, full the annotator defines the main frame. Yes/No ques- tions. Reading the headline and subheadline, the annotator defines the main frame.  Number samples  of  20000 articles  12000 articles  2600 articles and 1522 tv news stories  2990 headlines  1170 articles  6713 articles  2742 articles  131 headlines + subheadlines  the potential applications, and the challenges to face when using this approach. Prompt engineering applications include knowledge probing [46], information extraction [53], NLP reasoning [57], ques- tion answering [32], text generation [20], multi-modal learning [58], and text classification [24], the latter being the prompt-engineering use case in our work. Puri et al.[45] presented a very interesting idea that we apply to our classification task. This consists of pro- viding the language model with natural language descriptions of classification tasks as input, and training it to generate the correct answer in natural language via a language modeling objective. It is a zero-shot learning approach, in which no examples are used to explain the task to the model. Radford et al. [48] demonstrated that language models can learn tasks without any explicit supervision. We have followed this approach to find an alternative way to do frame analysis.  As mentioned before, the emergence of giant models like GPT-3, BLOOM, and ChatGPT are a very active research topic. To the best of our knowledge, on one hand our work extends the computational analysis of news related to the covid-19 no-vax movement, which illustrates the influence of the press on the ways societies think about relevant issues [40], [59], and on the other hand it adds to the literature of human-machine interaction, regarding the design of GPT-3 prompts for classification tasks [39], [2].  3 DATA: EUROPEAN COVID-19 NEWS  DATASET  We used part of the European Covid-19 News dataset collected in our recent work [3]. This dataset contains 51320 articles on Covid- 19 vaccination from 19 newspapers from 5 different countries: Italy,  629\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  France, Spain, Switzerland and UK. The articles cover a time period of 22 months, from January 2020 to October 2021. All content was translated into English to be able to work in a common language. The dataset was used for various analyses, such as name entity recognition, sentiment analysis, and subtopic modeling, to under- stand how Covid-19 vaccination was reported in Europe through the print media (in digital format.) The subtopic modeling analysis revealed a subsample of articles on the no-vax movement, which is the one we have used in this paper. We took the headlines of the articles associated with the no-vax movement, selecting all articles containing any of the keywords in Table 2 in the headline or in the main text. This corresponds to a total of 1786 headlines.  Table 2: Keywords used to identify no-vax articles  NO VAX TOPIC  \"anti-vaxxers\", \"anti-vaccine\", \"anti-vaxx\", \"anti-corona\", \"no-vax\", \"no vax\",\"anti-vaccin\"  Keywords  In Table 3, we show the number of headlines per country and newspaper. France is the country with the most no-vax articles in the corpus, with 523 articles, followed by Italy with 508. However, note that there are 6 newspapers from France, while only 2 from Italy. Corriere della Sera is the newspaper that dealt most frequently with the subject (429 articles), while The Telegraph is the second one (206 articles). The total number of articles normalized by the number of newspapers per country is also shown in the last column of the Table. Using these normalized values, the ranking is Italy, UK, France, Switzerland, and Spain.  Table 3: Number of headlines by newspaper and country  COUNTRY  NEWSPAPER  HEADLINES  TOTAL (NORM. TOTAL)  FRANCE  ITALY  SPAIN  SWITZERLAND  UNITED KINGDOM  La Croix Le Monde Les Echos Liberation Lyon Capitale Ouest France Corriere della Sera Il Sole 24 Ore 20 minutos ABC El Diario El Mundo El Espa\u00f1ol La Vanguardia 24 heures La Libert\u00e9 Le Temps The Irish News The Telegraph  94 125 49 97 8 150 429 79 27 50 32 77 22 95 97 22 111 16 206  523 (87.1)  508 (254.0)  303 (50.5)  230 (76.6)  222 (111.0)  1786  4 METHODOLOGY 4.1 Human labeling of news frames To carry out the labeling of the frames in our corpus of headlines, we first designed a codebook, which contained the definitions of each of the frame types and a couple of examples of each type, as well as a definition of the corpus subject matter and definitions of the concept of frame analysis, so that the annotators could understand the task to be performed. The codebook follows the proposed by  [52] with 5 generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) plus one additional \u2019no-frame\u2019 category. Two researchers were engaged to annotate a sample of the collected newspaper articles following a three-phase training procedure.  In the first phase, annotators had to read the codebook and get familiar with the task. In the second phase, they were asked to identify the main frame in the same subset of 50 headlines. At the end of the second phase, the intercoder reliability (ICR) was 0.58 between the 2 annotators. We analyzed those cases where there were discrepancies, and observed that in some cases, there was not a unique main frame, because both annotators had valid arguments to select one of the frames. In other cases, the discrepancies were due to slight misunderstanding of the definitions. In the third phase, the annotators coded again 50 headlines, and the ICR increased to was 0.66. We realized that the possibility of having two frames remained. They discussed the cases in which they had disagreed, and if the other person\u2019s arguments were considered valid, it could be said that there were two frames. After this three-phase training procedure, annotators were ready to annotate the dataset independently. We divided the dataset into two equal parts, and each person annotated 893 headlines.  4.2 Fine-tuning GPT-3.5 and BERT-based  models  With the annotated dataset, we investigated two NLP approaches: the first one involves fine-tuning a pre-trained model; the second one is prompt engineering. Pre-trained language models have been  Figure 1: Pre-train, fine-tune, prompt  trained with large text strings based on two unsupervised tasks, next sentence prediction and masked language model. Figure 1 summarizes these techniques.  In the first approach, a model with a fixed architecture is pre- trained as a language model (LM), predicting the likelihood of the observed textual data. This can be done due to the availability of large, raw text data needed to train LMs. This learning process can produce general purpose features of the modeled language. The learning process produces robust, general-purpose features of the language being modeled. The above pre-trained LM is then adapted to different downstream tasks, by introducing additional parameters and adjusting them using task-specific objective functions. In this approach, the focus was primarily on goal engineering, designing the training targets used in both the pre-training and the fine-tuning stages [36].  630\fFraming the News: From Human Perception to Large Language Model Inferences  We present an example to illustrate the idea. Imagine that the task is sentiment analysis, and we have a dataset with sentences and their associated sentiment, and a pre-trained model, which is a saved neural network trained with a much larger dataset. For that pre-trained model to address the target task, we unfreeze a few of the top layers of the saved model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows to \"fine-tune\" the higher-order feature representations in the base model to make them more relevant for the sentiment analysis task. In this way, instead of having to obtain a very large dataset with target labels to train a model, we can reuse the pre- trained model and use a much smaller train dataset. We use a part of our dataset as examples for the model to learn the task, while the other part of the dataset is used to evaluate model performance. Previous works related to frame classification in the computing literature have used fine-tuning, BERT-based models. In our work, we have done the same as a baseline, but we aimed to go one step further and also produce results using fine-tuning of GPT-3.5.  4.3 Prompt-engineering with GPT-3.5 Model fine-tuning has been widely used, but with the emergence of generative models such as GPT-3, another way to approach classification tasks has appeared. The idea is to use the pre-trained model directly and convert the task to be performed into a format as close as possible to the tasks for which it has been pre-trained. That is, if the model has been pre-trained from next word prediction as in the case of GPT-3, classification can be done by defining a prompt, where the input to the model is an incomplete sentence, and the model must complete it with a word or several words, just as it has been trained. This avoids having to use part of the already labeled dataset to teach the task to be performed to the model, and a previous labeling is not needed [36].  In this approach, instead of adapting pre-trained LMs to down- stream tasks via objective engineering, downstream tasks are re- formulated to look more like those solved during the original LM training with the help of a textual prompt. For example, when recog- nizing the emotion of a social media post, \u201cI missed the bus today.\u201d, we may continue with a prompt \u201cI felt so _\u201d, and ask the LM to fill the blank with an emotion-bearing word. Or if we choose the prompt \u201cEnglish: I missed the bus today. French: _\u201d), an LM may be able to fill in the blank with a French translation. In this way, by selecting the appropriate prompts, we can influence the model behavior so that the pre-trained LM itself can be used to predict the desired output, even without any additional task-specific training [36].  We use this emerging NLP approach to classify frames at headline level. We are not aware of previous uses of this strategy to classify frames as we propose here. The idea is the following. Prompt engi- neering consists of giving a prompt to the model, and understands that prompt as an incomplete sentence. To do prompt engineer- ing with our dataset, we needed to define an appropriate prompt that would produce the headline frames as output. We defined sev- eral experiments with the Playground of GPT-3, in order to find the best prompt for our task. In our initial experiments, we fol- lowed existing approaches in prompt engineering to do sentiment analysis, where the individual answer was an adjective, and this  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  adjective was matched with a sentiment. In a similar fashion, we decided to build a thesaurus of adjectives that define each of the frames. For instance, the human interest frame could be \u2019interest- ing\u2019, \u2019emotional\u2019, \u2019personal\u2019, \u2019human\u2019. The conflict frame could be: \u2019conflictive\u2019, \u2019bellicose\u2019, \u2019troublesome\u2019, \u2019rowdy\u2019, \u2019quarrelsome\u2019, \u2019trou- blemaker\u2019, \u2019agitator\u2019, etc. After the list of adjectives was defined, we needed to define the prompt in order to get, as an answer, one of the adjectives in our thesaurus to match them with the frame. We used the GPT-3 playground using the headline as input and asking for the frame as output, but the strategy did not work. In our final experiment, instead of giving the headline as input, we gave the definitions of each type of frame plus the headline, and we asked the model to choose between the different types of frames as output. In this way, the output of the model was directly one of the frames, and we avoided the step of matching adjectives with frames. An example is shown in Figure 2.  Figure 2: GPT-3.5 for frame inference: input and output  For the GPT-3 configuration 1, there are 3 main concepts:  \u2022 TEMPERATURE [0-1]. This parameter controls randomness,  lowering it results in less random completions.  \u2022 TOP_P [0-1]. This parameter controls diversity via nucleus  sampling.  \u2022 MAX_TOKENS[1-4000]. This parameter indicates the maxi-  mum number of tokens to generate,  \u2022 MODEL. GPT-3 offer four main models with different levels of power, suitable for different tasks. Davinci is the most capable model, and Ada is the fastest.  After testing with the GPT-3 playground and varying different hyper-parameters to assess performance, we set the temperature to 0, since the higher the temperature the more random the response. Furthermore, the Top-p parameter was set to 1, as it would likely get a set of the most likely words for the model to choose from. The maximum number of tokens was set to 2; in this way, the model is asked to choose between one of the responses. As a model, we used the one with the best performance at the time of experimental design, which was TEXT-DAVINCI-003, recognized as GPT 3.5.  5 RESULTS: HUMAN LABELING OF FRAMES  IN NO-VAX NEWS HEADLINES (RQ1)  In this section, we present and discuss the results of the analysis related to our first RQ.  Figure 3 shows the distribution of frames per country at headline level, with human interest and no-frame being the predominant   631\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  ones. Attribution of responsibility is the third one except in Switzer- land, where the corresponding frame is conflict. Finally, morality and economic are the least represented in the dataset for every country.  Figure 4: Non-normalized monthly distribution of frames.  detail the negative and positive sentiment of each frame category, we observed a few trends:  \u2022 Attribution of responsibility: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Switzerland, and the United Kingdom. \u2022 Conflict: Negative sentiment represents 20-35% of the cases. \u2022 Economic: Predominantly neutral, with only negative tone in Italy and UK (in the latter case, all headlines with this frame were considered negative.)  \u2022 Human interest: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Spain, and Switzerland.  \u2022 Morality: Predominantly neutral, with negative tone in Italy,  Switzerland, and the United Kingdom, \u2022 No frame: 20-30% of negative content.  Figure 5: Sentiment of headline by frame and by country  Figure 3: Non-normalized distribution of frames per country  The monthly distribution of frames aggregated for all countries is shown in Fig. 4. We can see two big peaks, the first one in January 2021 and the second one in August 2021. In all countries, the vac- cination process started at the end of December 2020, so it makes sense that the no-vax movement started to be more predominant in the news in January 2021. Human interest is the most predominant frame. Manual inspection shows that this is because the headlines are about personal cases of people who are pro- or anti- vaccine. Attribution of responsibility is also present. Manual inspection in- dicates that local politicians and health authorities had to make decisions about who could be vaccinated at the beginning of the process. The second peak at the end of summer 2021 coincided with the health pass (also called Covid passport in some countries), and we can observe a peak in the curve corresponding to the con- flict frame, reflecting the demonstrations against the measure of mandatory health passes taken by country governments.  In Figure 5, we compare the sentiment per frame and per country, to understand if there were any major differences. The sentiment analysis labels were obtained using BERT-sent from the Hugging Face package [47], used in our previous work (please refer to our original analysis in [3] for details.) We normalized the results be- tween 0 and 1 to compare frames between countries. We see that the sentiment is predominantly neutral (in blue). Examining in more  632\fFraming the News: From Human Perception to Large Language Model Inferences  Regarding the results of the annotation process, the fact that the distribution of the 6 frame types is relatively similar between coun- tries suggests that the anti-vaccine movement issue was treated in a similar way in these countries. The fact that human interest is the most dominant frame indicates that this issue was treated from a more human and emotional approach, with headlines about personal experiences, celebrities giving their opinion about vacci- nation, and politicians defending vaccine policies. Moreover, the reason for many headlines being classified as no-frame is partly due to how data was selected. We chose articles that contained words related to no-vax, either in the headline or in the article. This resulted in many headlines not containing anything specific related to no-vax, while the no-vax content was actually included in the main text of the corresponding articles.  It is worth mentioning that prior to obtaining the results, we had expected that attribution of responsibility would be among the most prominent frames, since governments took many measures such as mandatory health pass requirements to access certain sites; we had also expected that the conflict frame would be prominent, since there were many demonstrations in Europe. In reality, however, these frames categories were not reflected as frequently at the headline level.  Regarding the analysis at the temporal level, it is clear that certain events were captured by the press, such as the start of vaccination or the mandatory vaccination passport.  Finally, the sentiment analysis of the different frames shows that the predominant tone in all of them is neutral or negative, with very similar trends between countries. This association between senti- ment analysis and frames has been discussed in previous literature [11] [43].  6 RESULTS: GPT-3.5 FOR FRAME  CLASSIFICATION OF HEADLINES (RQ2) Here, we present and discuss the results related to our second RQ.  6.1 Fine-tuning GPT-3.5 Table 4 shows the results of the 6-class classification task using 5-cross validation. Three models were used: GPT-3.5 and two BERT- based models. We observe that, on average, GPT-3.5 performs better than the BERT-based models. This is somehow expected as GPT- 3.5 is a much larger model. Overall, in the case of fine-tuning, the best performance for the six-class frame classification task is 72% accuracy, which is promising, with an improvement over previous models based on BERT. Yet, it should be noted that the performance differences are modest (2% improvement between GPT-3.5 and RoBERTa).  Table 4: Classification results for six-class frame classifica- tion and 5-fold cross validation  ACCURACY  BERT RoBERTa GPT3  0  0.68 0.70 0.75  1  0.69 0.72 0.70  2  0.72 0.72 0.72  3  0.64 0.67 0.71  4  0.70 0.71 0.71  AVERAGE  0.67 0.70 0.72  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  On the other hand, BERT is open-source, while GPT-3 has an economic cost as the use of the model is not free, which monetarily limits the number of experiments that can be performed with it, as well as the different configurations one can explore to improve performance. This is important because much of the improvement in performance requires empirical explorations of model parameters More specifically, the cost of an experiment for each of the folds has a cost of 4 dollars (at the time of writing this paper.) This represents a limitation in practice.  Furthermore, GPT-3 has a significant carbon footprint. Similarly, for prompt engineering (discussed in the next subsection), choosing the right prompt (i.e., the words that best define the task so that the model is able to perform adequately) is also based on trial and error. This also has an impact on carbon footprint. In connection with this topic, Strubell et al.[55] argue that improvements in the accu- racy of models depend on the availability of large computational resources, which involve large economic and environmental costs. A criticism has been made as \u2019the rich get richer\u2019, in the sense that not all research groups have sufficient infrastructure resources and access to funding needed to use these models and improve their performance. Also in relation to this analysis, the work of Bender et al. [4] evaluates the costs and risks of the use of large language models, stating that researchers should be aware of the impact that these models have on the environment, and assess whether the benefits outweigh the risks. The work in [4] provides a very telling example, where people living in the Maldives or Sudan are affected by floods and pay the environmental price of training English LLMs, when similar models have not been produced for languages like Dhivehi or Sudanese Arab. In short, there is a need to establish ways to use this technological development responsibly, and it all starts with being aware of the risks it presents.  6.2 Prompt-engineering with GPT-3.5 For each headline, we got the frame that the model considered the most likely, and we compared these GPT-3.5 inferences with the frames labeled by the annotators. The agreement between model and annotator was of 49%. Analyzing the results, and specifically looking at the cases where the annotator and GPT-3.5 disagreed, we discovered that according to the frame definitions, the model in some cases proposed a frame that indeed made sense. This ob- servation, together with our previous experience in the annotation process, where headlines could have more than one valid frame, led us to design a second post-hoc experiment. We took all the headlines where each of the two annotators had disagreed with GPT-3.5, and we asked the annotators to state whether they would agree (or not) with each GPT-inferred label for a given headline. It is important to emphasize that the annotators did not know the origin of that label, i.e., they did not know if it was the label they had originally assigned, or if it was a random one. In this way, we could quantify how GPT-3.5 worked according to valid arguments provided by the annotators. In this post-hoc experiment, the model agreed in 76% of cases with the annotators.  Looking at the results of the classification models, the 49% accu- racy of the prompt-engineering approach can be considered low, yet we consider that it is a valid avenue for further investigation, as in the second post-hoc analysis, we found that the model agrees  633\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  with human annotators in 76% of the cases. Clearly, framing in- volves aspects of subjectivity [42]. Much of what we do as people has a subjective component, influenced by how we feel or how we express opinions.  News reading is never fully objective, and the annotators en- gaged in the frame classification task, influenced by their personal state of mind, experience, and culture, may perceive information differently. Monarch affirms that \"for simple tasks, like binary labels on objective tasks, the statistics are fairly straightforward to decide which is the \u2018correct\u2019 label when different annotators disagree. But for subjective tasks, or even objective tasks with continuous data, there are no simple heuristics for deciding what the correct label should be\" [42].  Subjectivity is involved in both the generation and perception of information: the assumption that there is only one frame is com- plicated by the point of view of the reader. In the case of news, the information sender (the journalist) has an intention, but the receiver (the reader) plays a role and is influenced by it. In psychology, this is known as the lens model of interpersonal communication, where the sender has certain objectives, but the receiver can interpret or re-interpret what the sender wants to say, with more or less accuracy [26].  Following this discussion on subjectivity, the question arose as to what would happen if, instead of headlines, we used the complete article as a source of analysis. We wondered if longer text could make the frame labeling task clearer than when using headlines. Yet another possible hypothesis is that having to read longer texts could lead to the same subject being presented from different angles. Please recall that in the existing literature discussed in Section 2, both headlines and full articles have been used from frame analysis (see Table 1.) This remains as an issue for future work.  7 CONCLUSIONS In this paper, we first presented an analysis of human-generated news frames on the covid-19 no-vax movement in Europe, and then studied different approaches using large language models for automatic inference of frames. We conclude by answering the two research questions we posed:  RQ1: What are the main frames in the news headlines about the covid-19 anti-vaccine movement in 5 European countries? After annotating the headlines, we found that of the 1786 headlines, the predominant frame is human interest (45.3% of cases), which presents a news item with an emotional angle, putting a face to a problem or situation. We also found that a substantial proportion of headlines were annotated as not presenting any frame (40.2% of cases). Finally, the other frame types are found more infrequently. RQ2: Can prompt engineering be used for classification of head- lines according to frames? We first used fine-tuning of a number of language models, and found that GPT-3.5 produced classification ac- curacy of 72% on a six-frame classification task. This represented a modest 2% improvement over BERT-based models, at a significantly larger environmental cost. We then presented a new way of classi- fying frames using prompts. At the headline level, inferences made with GPT-3.5 reached 49% of agreement with human-generated frame labels. In many cases, the GPT-3.5 model inferred frame types that were considered as valid choices by human annotators,  and in an post-doc experiment, the human-machine agreement reached 76%. These results have opened several new directions for future work.  ACKNOWLEDGMENTS",
  "references": [
    "1] Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online media: Quantitative framing analysis on Detik. com\u2019s coverage of Covid-19. Jurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153\u2013170.  ",
    "2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. 2021. RAFT: A real-world few-shot text classification benchmark. arXiv preprint arXiv:2109.14076 (2021).  ",
    "3] David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe\u2019s Press (2022), 35\u201343.  Cover Covid-19 Vaccination News? A Five-Country Analysis. https://doi.org/10.1145/3512732.3533588  ",
    "4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? (2021), 610\u2013623.  ",
    "5] Santosh Kumar Biswal and Nikhil Kumar Gouda. 2020. Artificial intelligence in journalism: A boon or bane? In Optimization in machine learning and applications. Springer, 155\u2013167.  ",
    "6] Erik Bleich, Hannah Stonebraker, Hasher Nisar, and Rana Abdelhamid. 2015. Media portrayals of minorities: Muslims in British newspaper headlines, 2001\u2013 2012. Journal of Ethnic and Migration Studies 41, 6 (2015), 942\u2013962.  ",
    "7] Michael Bommarito and Daniel Martin Katz. 2022. GPT Takes the Bar Exam.  https://doi.org/10.48550/ARXIV.2212.14402  ",
    "8] Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe, Michel Dupagne, and Ching-Hua Chuan. 2019. Artificial intelligence and jour- nalism. Journalism & Mass Communication Quarterly 96, 3 (2019), 673\u2013695. ",
    "9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901.  ",
    "10] Bj\u00f6rn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, and Claes H De Vreese. 2014. Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis. Communication Methods and Measures 8, 3 (2014), 190\u2013206.  ",
    "11] Bjorn Burscher, Rens Vliegenthart, and Claes H de Vreese. 2016. Frames beyond words: Applying cluster and sentiment analysis to news coverage of the nuclear power issue. Social Science Computer Review 34, 5 (2016), 530\u2013545.  ",
    "12] Dallas Card, Amber Boydstun, Justin Gross, Philip Resnik, and Noah Smith. 2015. The Media Frames Corpus: Annotations of Frames Across Issues. 2 (01 2015), 438\u2013444. https://doi.org/10.3115/v1/P15-2072  ",
    "13] Daniel Catalan-Matamoros and Carlos El\u00edas. 2020. Vaccine hesitancy in the age of coronavirus and fake news: analysis of journalistic sources in the Spanish quality press. International Journal of Environmental Research and Public Health 17, 21 (2020), 8136.  ",
    "14] Daniel Catal\u00e1n-Matamoros and Carmen Pe\u00f1afiel-Saiz. 2019. Media and mistrust of vaccines: a content analysis of press headlines. Revista latina de comunicaci\u00f3n social 74 (2019), 786\u2013802.  ",
    "15] Mark Coddington. 2015. Clarifying journalism\u2019s quantitative turn: A typology for evaluating data journalism, computational journalism, and computer-assisted reporting. Digital journalism 3, 3 (2015), 331\u2013348.  ",
    "16] Stephen D Cooper. 2010. The oppositional framing of bloggers. In Doing News  Framing Analysis. Routledge, 151\u2013172.  ",
    "17] Robert Dale. 2021. GPT-3: What\u2019s it good for? Natural Language Engineering 27,  1 (2021), 113\u2013118.  ",
    "18] Astrid Dirikx and Dave Gelders. 2010.  To frame is to explain: A de- ductive frame-analysis of Dutch and French climate change coverage dur- ing the annual UN Conferences of the Parties. Public Understanding of Science 19, 6 (2010), 732\u2013742. https://doi.org/10.1177/0963662509352044 arXiv:https://doi.org/10.1177/0963662509352044 PMID: 21560546.  ",
    "19] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A deductive frame- analysis of Dutch and French climate change coverage during the annual UN Conferences of the Parties. Public understanding of science 19, 6 (2010), 732\u2013742.  634\fFraming the News: From Human Perception to Large Language Model Inferences  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  ",
    "20] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2020. Gsum: A general framework for guided neural abstractive summarization. arXiv preprint arXiv:2010.08014 (2020).  ",
    "21] Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA Gesondheid (Online) 27 (2022), 1\u20138.  ",
    "22] Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021). ",
    "23] Robert M Entman. 1993. Framing: Towards clarification of a fractured paradigm.  McQuail\u2019s reader in mass communication theory 390 (1993), 397.  ",
    "24] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723 (2020). ",
    "25] Piyush Ghasiya and Koji Okamura. 2021. Investigating COVID-19 news across four nations: a topic modeling and sentiment analysis approach. Ieee Access 9 (2021), 36645\u201336656.  ",
    "26] Robert Gifford. 1994. A Lens-Mapping Framework for Understanding the En- coding and Decoding of Interpersonal Dispositions in Nonverbal Behavior. Journal of Personality and Social Psychology 66 (02 1994), 398\u2013412. https: //doi.org/10.1037//0022-3514.66.2.398  ",
    "27] Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based transformer architectures for long document summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 1792\u20131810.  ",
    "28] Anushka Gupta, Diksha Chugh, Rahul Katarya, et al. 2022. Automated news summarization using transformers. In Sustainable Advanced Computing. Springer, 249\u2013259.  ",
    "29] Alfred Hermida and Mary Lynn Young. 2017. Finding the data unicorn: A hierar- chy of hybridity in data and computational journalism. Digital Journalism 5, 2 (2017), 159\u2013176.  ",
    "30] Karoliina Isoaho, Daria Gritsenko, and Eetu M\u00e4kel\u00e4. 2021. Topic modeling and text analysis for qualitative policy research. Policy Studies Journal 49, 1 (2021), 300\u2013324.  ",
    "31] Carina Jacobi, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative analysis of large amounts of journalistic texts using topic modelling. Digital journalism 4, 1 (2016), 89\u2013106.  ",
    "32] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computa- tional Linguistics 8 (2020), 423\u2013438.  ",
    "33] Shima Khanehzar, Andrew Turpin, and Gosia Miko\u0142ajczak. 2019. Modeling  ",
    "46] Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with  mixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021).  ",
    "47] Rabindra Lamsal. 2021. Sentiment Analysis of English Tweets with BERTsent. https://huggingface.co/rabindralamsal/finetuned-bertweet-sentiment-analysis. ",
    "48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.  ",
    "49] Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, and Ahad Ali. 2022. Fake News Classification using transformer based enhanced LSTM and BERT. International Journal of Cognitive Computing in Engineering 3 (2022), 98\u2013105. https://doi.org/10.1016/j.ijcce.2022.03.003  ",
    "50] Frida V Rodelo. 2021. Framing of the Covid-19 pandemic and its organizational  predictors. Cuadernos. info 50 (2021), 91\u2013112.  ",
    "51] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).  ",
    "52] Holli Semetko and Patti Valkenburg. 2000. Framing European Politics: A Content Analysis of Press and Television News. Journal of Communication 50 (06 2000), 93 \u2013 109. https://doi.org/10.1111/j.1460-2466.2000.tb02843.x  ",
    "53] Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Em- manouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. arXiv preprint arXiv:2104.08768 (2021).  ",
    "54] Efstathios Sidiropoulos and Andreas Veglis. 2017. Computer Supported Collab- orative Work trends on Media Organizations: Mixing Qualitative and Quan- titative Approaches. Studies in Media and Communication 5 (04 2017), 63. https://doi.org/10.11114/smc.v5i1.2279  ",
    "55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019). ",
    "56] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503 (2021).  ",
    "57] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning.  arXiv preprint arXiv:1806.02847 (2018).  ",
    "58] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems 34 (2021), 200\u2013212.  ",
    "59] Sandra A Vannoy and Prashant Palvia. 2010. The social influence model of  technology adoption. Commun. ACM 53, 6 (2010), 149\u2013153.  ",
    "60] Tuukka Yl\u00e4-Anttila, Veikko Eranti, and Anna Kukkonen. 2022. Topic modeling for frame analysis: A study of media debates on climate change in India and USA. Global Media and Communication 18, 1 (2022), 91\u2013112.  Political Framing Across Policy Issues and Contexts. In ALTA.  ",
    "34] Jeesun Kim and Wayne Wanta. 2018. News framing of the US immigration debate during election years: Focus on generic frames. The Communication Review 21, 2 (2018), 89\u2013115.  ",
    "35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi- hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).  ",
    "36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. (2021). https://doi.org/10. 48550/ARXIV.2107.13586  ",
    "37] Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and Derry Tanti Wijaya. 2019. Detecting frames in news headlines and its application to analyzing news framing trends surrounding US gun violence. In Proceedings of the 23rd conference on computational natural language learning (CoNLL).  ",
    "38] J\u00f6rg Matthes and Matthias Kohring. 2008. The Content Analysis of Media Frames: Toward Improving Reliability and Validity. Journal of Communication 58 (06 2008). https://doi.org/10.1111/j.1460-2466.2008.00384.x  ",
    "39] Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, and David E Losada. 2022. Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI. In Proceedings of the 4th Conference on Conversational User Interfaces. 1\u20136.  ",
    "40] Stuart E Middleton, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Social computing for verifying social media content in breaking news. IEEE Internet Computing 22, 2 (2018), 83\u201389.  ",
    "41] Marko Milosavljevi\u0107 and Igor Vobi\u010d. 2021.  \u2018Our task is to demystify fears\u2019: Analysing newsroom management of automation in journalism. Journalism 22, 9 (2021), 2203\u20132221.  ",
    "42] R. Monarch. 2021. Human-in-the-Loop Machine Learning: Active Learning and Annotation for Human-centered AI. Manning. https://books.google.ch/books? id=LCh0zQEACAAJ  ",
    "43] Tom Nicholls and Pepper D Culpepper. 2021. Computational identification of media frames: Strengths, weaknesses, and opportunities. Political Communication 38, 1-2 (2021), 159\u2013181.  ",
    "44] Zhongdang Pan and Gerald M Kosicki. 1993. Framing analysis: An approach to  news discourse. Political communication 10, 1 (1993), 55\u201375.  ",
    "45] Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative  language models. arXiv preprint arXiv:1912.10165 (2019).  635\f"
  ],
  "url": "https://drive.google.com/uc?id=1vmiWI3NUHORkl-g7D9WA8Xh7YbTOHfmB",
  "date": "2024-01-09 21:21:44",
  "is_published": false
}
{
  "title": "Framing the News: From Human Perception to Large Language Model Inferences",
  "authors": [
    "David Alonso del Barrio",
    "Idiap Research Institute",
    "Daniel Gatica-Perez",
    "Idiap Research Institute and EPFL"
  ],
  "institutions": [
    "Switzerland",
    "Switzerland"
  ],
  "abstract": "Identifying the frames of news is important to understand the arti- cles\u2019 vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of jour- nalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to under- stand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No- Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Nat- ural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT- 3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.",
  "keywords": [
    "Covid-19 no-vax",
    " news framing",
    " GPT-3",
    " prompt-engineering",
    " trans-",
    "formers",
    " large language models"
  ],
  "article": "In recent years, there has been a proliferation in the use of concepts such as data journalism, computational journalism, and computer- assisted reporting [15] [29], which all share the vision of bridging journalism and technology. The progress made in NLP has been gradually integrated into the journalistic field [5][8][54]. More specifically, machine learning models based on transformers have been integrated in the media sector in different tasks [41] such as the creation of headlines with generative languages models [17], summarization of news articles [28][27], false news detection [49], and topic modeling and sentiment analysis [25]. The development of large language models such as GPT-3 [9], BLOOM [51] or ChatGPT show a clear trend towards human-machine interaction becoming easier and more intuitive, opening up a wide range of research possibilities. At the same time, the use of these models is also associated with a lack of transparency regarding how these models work, but efforts are being made to bring some transparency to where they cannot [35]. Based on the premises that these models open up a wide range of research directions [7], and that at the same time (and needless to say) they are not the solution to all problems, can be potentially useful, while acknowledging and systematically documenting their limitations [56]. More specifically, the aim of this work is to analyze the performance of GPT-3.5 for a specific use case, namely the analysis of frames in news, from an empirical point of view, with the objective of shedding light on a potential use of generative models in journalistic tasks.  Frame analysis is a concept from journalism, which consists of studying the way in which news stories are presented on an issue, and what aspects are emphasized: Is a merely informative vision given in an article? Or is it intended to leave a moral lesson? Is a news article being presented from an economic point of view? Or from a more human, emotional angle? The examples above correspond to different frames with which an article can be written. The concept of news framing has been studied in computing as a step beyond topic modeling and sentiment analysis, and for this purpose, in recent years, pre-trained language models have been used for fine-tuning the classification process of these frames [60] [10], but the emergence of generative models opens the possibility of doing prompt-engineering of these classification tasks, instead of the fine-tuning approach investigated so far.  Our work aims to address this research gap by posing the fol-  lowing research questions:  RQ1: What are the main frames in the news headlines about the anti-vaccine movement, as reported in newspapers across 5 European countries?  627\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  RQ2: Can prompt engineering be used for classification of head-  lines according to frames?  By addressing the above research questions, our work makes the  following contributions:  Contribution 1. We implemented a process to do human an- notation of the main frame of 1786 headlines of articles about the Covid-19 no-vax movement, as reported in 19 newspapers from 5 European countries (France, Italy, Spain, Switzerland and United Kingdom.) At the headline level, we found that the predominant frame was human interest, where this frame corresponds to a per- sonification of an event, either through a statement by a person, or the explanation of a specific event that happened to a person. Furthermore, we found a large number of headlines annotated as containing no frame, as they simply present information without entering into evaluations. We also found that for all the countries involved, the distribution of frame types was very similar, i.e., hu- man interest and no frame are the two predominant frames. Finally, the generated annotations allowed to subsequently study the per- formance of a large language model.  Contribution 2. We studied the performance of GPT-3.5 on the task of frame classification of headlines. In addition to using the fine-tuning approach from previous literature, we propose an alternative approach for frame classification that requires no labeled data for training, namely prompt-engineering using GPT-3.5. The results show that fine-tuning with GPT-3.5 produces 72% accuracy (slightly higher than other smaller models), and that the prompt- engineering approach results in lower performance (49% accuracy.) Our analysis also shows that the subjectivity of the human labeling task has an effect on the obtained accufracy.  The paper is organized as follows. In Section 2, we discuss related work. In Section 3, we describe the news dataset. In Section 4, we describe the methodology for both human labeling and machine classification of news frames. We present and discuss results for RQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide conclusions in Section 7.  2 RELATED WORK Framing has been a concept widely studied in journalism, with a definition that is rooted in the study of this domain [23]: \u201cTo frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a par- ticular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.\u201d  For frame recognition, there are two main approaches: the induc- tive approach [16], where one can extract the frames after reading the article, and the deductive approach [38], where a predefined list of frames exists and the goal is to interpret if any of them ap- pears in the article. In the deductive case, there are generic frames and subject-specific frames, and the way to detect them typically involves reading and identifying one frame at a time, or through answers to yes/no questions that represent the frames. Semetko et al. [52] used 5 types of generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) based on previous literature, and they defined a list of 20 yes/no questions to detect frames in articles. For instance, the questions about morality are the following: \"Does the story contain any moral  message? Does the story make reference to morality, God, and other religious tenets? Does the story offer specific social prescriptions about how to behave?\", and so on for each of the frame types. This categorization of frames has been used in various topics such as climate change [18] [19], vaccine hesitance [13], or immigration [34].  We now compare the two approaches on a common topic, such as Covid-19. Ebrahim et al. [21] followed an inductive approach in which the frames were not predefined but emerged from the text (e.g., deadly spread, stay home, what if, the cost of Covid-19) using headlines as the unit of analysis. In contrast, the deductive approach has studied very different labels. El-Behary et al. [22] followed the method of yes/no questions, but in addition to the 5 generic frames presented before, they also used blame frame and fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5 generic frames with yes/no questions, while Catal\u00e1n-Matamoros et al. [14] used the 5 frames and read the headline and subheadline to decide the main frame. Table 1 summarizes some of the the existing approaches. This previous work showed how frame labels can be different, and also that frame analysis has been done at both headline and article levels. These two approaches (inductive and deductive) that originated in journalism have since been replicated in the computing literature.  We decided to follow the deductive approach because a prede- fined list of frames allows to compare among topics, countries, previous literature, and also because they represent a fixed list of labels for machine classification models. Furthermore, the induc- tive approach tends to be more specific to a topic, and from the computing viewpoint, past work has tried to justify topic modeling as a technique to extract frames from articles.  Yl\u00e4-Antitila et al. [60] proposed topic modeling as a frame ex- traction technique. They argued that topics can be interpreted as frames if three requirements are met: frames are operationalized as connections between concepts; subject-specific data is selected; and topics are adequately validated as frames, for which they suggested a practical procedure. This approach was based on the choice of a specific topic (e.g., climate change) and the use of Latent Dirichlet Allocation (LDA) as a technique to extract a number of subtopics. In a second phase, a qualitative study of the top 10 words of each subtopic was performed, and the different subtopics were elimi- nated or grouped, reducing the number and establishing a tentative description. In a third phase, the top 10 articles belonging to that frame/topic were taken, and if the description of the topic fitted at least 8 of the 10 articles, that topic/frame remained. The frames found in this article were: green growth, emission cuts, negotiations and treaties, environmental risk, cost of carbon emissions, Chinese emissions, economics of energy production, climate change, en- vironmental activism, North-South burden sharing, state leaders negotiating, and citizen participation.  From Entman\u2019s definition of frame [23], it seems that the deduc- tive approach is more refined than the inductive approach (which seems to resemble the detection of sub-themes.) For example, with regard to climate change, there are stories on how people have been affected by climate change from an emotional point of view, thus personalizing the problem. In this case, we could categorize the corresponding frame as human interest, as the writer of the article is selecting \"some aspects of a perceived reality and make them  628\fFraming the News: From Human Perception to Large Language Model Inferences  more salient\". The language subtleties with which news articles are presented cannot be captured with basic topic modeling.  Isoaho et al.[30] held the position that while the benefits of scale and scope in topic modeling were clear, there were also a number of problems, namely that topic outputs do not correspond to the methodological definition of frames, and thus topic modeling remained an incomplete method for frame analysis. Topic modeling, in the practice of journalistic research, is a useful technique to deal with the large datasets that are available, yet is often not enough to do more thorough analyses [31]. In our work, we clearly notice that frame analysis is not topic modeling. For example, two documents could be about the same topic, say Covid-19 vaccination, but one article could emphasize the number of deaths after vaccination, while the other emphasized the role of the vaccine as a solution to the epidemic.  We also consider that the larger the number of possible frame types, the more likely it is to end up doing topic modeling instead of frame analysis. Using a deductive approach, Dallas et al. [12] created a dataset with articles about polemic topics such as immigration, same sex marriage, or smoking, and they defined 15 types of frames: \"economic, capacity and resources, morality, fairness and equality, legality, constitutionality and jurisprudence, policy prescription and evaluation, crime and punishment, security and defense, health and safety, quality of life, cultural identity, political, external regulation and reputation, other\". In this case, they authors did not use a list of questions. Instead, for each article, annotators were asked to identify any of the 15 framing dimensions present in the article and to label text blurbs that cued them (based on the definitions of each of the frame dimensions) and decide the main frame of each article. In our case, we followed the idea of detecting the main frame by reading the text instead of answering questions, but instead of using the 15 frames proposed in [12] , we used the 5 generic frames proposed in [52].  A final decision in our work was the type of text to analyze, whether headlines or whole article. For this decision, the chosen classification method was also going to be important. For example, Khanehzar et al. [33] used traditional approaches such as SVMs as baseline, and demonstrated the improvement in frame classifica- tion with the use of pre-trained languages models such as BERT, RoBERTa and XLNet, following a fine-tuning approach, setting as input text a maximum of 256 tokens (although the maximum number of input tokens in these models is 512 tokens.) Liu et al. [37] classified news headlines about the gun problem in the United States, arguing for the choice of headlines as a unit of analysis based on previous journalism literature [6], [44], that advocated for the importance and influence of headlines on readers and the subsequent perception of articles. From a computational viewpoint, using headlines is also an advantage, since you avoid the 512 token limitation in BERT-based models. Therefore, we decided to work with headlines about a controversial issue, namely the Covid-19 no-vax movement.  Continuing with the question of the methods used for classi- fication, much work has been developed in prompt engineering, especially since the release of GPT-3. Liu et al.[36] presented a good overview of the work done on this new NLP paradigm, not only explaining the concept of prompt engineering, but also the differ- ent strategies that can be followed both in the design of prompts,  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  Table 1: Summary of deductive approaches for frame analysis  Ref Frames  Goal  Technique  To label frames of full articles  [12] 15 generic frames: \"Economic\", \"Capac- ity and resources\", \"Morality\", \"Fair- ness and equality\", \"Legality, constitu- tionality and jurisprudence\", \"Policy prescription and evaluation\", \"Crime and punishment\", \"Security and de- fense\", \"Health and safety\", \"Quality of life\", \"Cultural identity\", \"Public opin- ion\", \"Political\", \"External regulation and reputation\", \"Other\".  [33] 15 generic frames  Classification  [52] 5 generic frames: \"human interest\", \"conflict\", \"morality\", \"attribution of responsibility\", and \"economic conse- quences\".  \u201cSociety/Culture\u201d, ,  [37] 9 specific frames:\u201cPolitics\u201d, \u201cPublic opinion\u201d, and \u201c2nd \u201cEconomic consequences\u201d Amendment\u201d (Gun Rights), \u201cGun control/regulation\u201d, \u201cMental health\u201d, \u201cSchool/Public space safety\u201d, and \u201cRace/Ethnicity\u201d.  [22] 5 generic frames + blame frame and  fear frame 5 generic frames  [1]  To label frames of full articles  To label frames of full articles/ Classification  To label frames of full articles To label frames of full articles  [50] 5 generic frames + pandemic frames  [14] 5 generic frames, journalistic role and  pandemic frames  To label frames of full articles To label frames of full articles  the Reading article, full the annotator defines the main frame  BERT based models Yes/No ques- tions.  the Reading full article, the annotator the defines main frame. BERT based models Yes/No ques- tions. Reading the article, full the annotator defines the main frame. Yes/No ques- tions. Reading the headline and subheadline, the annotator defines the main frame.  Number samples  of  20000 articles  12000 articles  2600 articles and 1522 tv news stories  2990 headlines  1170 articles  6713 articles  2742 articles  131 headlines + subheadlines  the potential applications, and the challenges to face when using this approach. Prompt engineering applications include knowledge probing [46], information extraction [53], NLP reasoning [57], ques- tion answering [32], text generation [20], multi-modal learning [58], and text classification [24], the latter being the prompt-engineering use case in our work. Puri et al.[45] presented a very interesting idea that we apply to our classification task. This consists of pro- viding the language model with natural language descriptions of classification tasks as input, and training it to generate the correct answer in natural language via a language modeling objective. It is a zero-shot learning approach, in which no examples are used to explain the task to the model. Radford et al. [48] demonstrated that language models can learn tasks without any explicit supervision. We have followed this approach to find an alternative way to do frame analysis.  As mentioned before, the emergence of giant models like GPT-3, BLOOM, and ChatGPT are a very active research topic. To the best of our knowledge, on one hand our work extends the computational analysis of news related to the covid-19 no-vax movement, which illustrates the influence of the press on the ways societies think about relevant issues [40], [59], and on the other hand it adds to the literature of human-machine interaction, regarding the design of GPT-3 prompts for classification tasks [39], [2].  3 DATA: EUROPEAN COVID-19 NEWS  DATASET  We used part of the European Covid-19 News dataset collected in our recent work [3]. This dataset contains 51320 articles on Covid- 19 vaccination from 19 newspapers from 5 different countries: Italy,  629\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  France, Spain, Switzerland and UK. The articles cover a time period of 22 months, from January 2020 to October 2021. All content was translated into English to be able to work in a common language. The dataset was used for various analyses, such as name entity recognition, sentiment analysis, and subtopic modeling, to under- stand how Covid-19 vaccination was reported in Europe through the print media (in digital format.) The subtopic modeling analysis revealed a subsample of articles on the no-vax movement, which is the one we have used in this paper. We took the headlines of the articles associated with the no-vax movement, selecting all articles containing any of the keywords in Table 2 in the headline or in the main text. This corresponds to a total of 1786 headlines.  Table 2: Keywords used to identify no-vax articles  NO VAX TOPIC  \"anti-vaxxers\", \"anti-vaccine\", \"anti-vaxx\", \"anti-corona\", \"no-vax\", \"no vax\",\"anti-vaccin\"  Keywords  In Table 3, we show the number of headlines per country and newspaper. France is the country with the most no-vax articles in the corpus, with 523 articles, followed by Italy with 508. However, note that there are 6 newspapers from France, while only 2 from Italy. Corriere della Sera is the newspaper that dealt most frequently with the subject (429 articles), while The Telegraph is the second one (206 articles). The total number of articles normalized by the number of newspapers per country is also shown in the last column of the Table. Using these normalized values, the ranking is Italy, UK, France, Switzerland, and Spain.  Table 3: Number of headlines by newspaper and country  COUNTRY  NEWSPAPER  HEADLINES  TOTAL (NORM. TOTAL)  FRANCE  ITALY  SPAIN  SWITZERLAND  UNITED KINGDOM  La Croix Le Monde Les Echos Liberation Lyon Capitale Ouest France Corriere della Sera Il Sole 24 Ore 20 minutos ABC El Diario El Mundo El Espa\u00f1ol La Vanguardia 24 heures La Libert\u00e9 Le Temps The Irish News The Telegraph  94 125 49 97 8 150 429 79 27 50 32 77 22 95 97 22 111 16 206  523 (87.1)  508 (254.0)  303 (50.5)  230 (76.6)  222 (111.0)  1786  4 METHODOLOGY 4.1 Human labeling of news frames To carry out the labeling of the frames in our corpus of headlines, we first designed a codebook, which contained the definitions of each of the frame types and a couple of examples of each type, as well as a definition of the corpus subject matter and definitions of the concept of frame analysis, so that the annotators could understand the task to be performed. The codebook follows the proposed by  [52] with 5 generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) plus one additional \u2019no-frame\u2019 category. Two researchers were engaged to annotate a sample of the collected newspaper articles following a three-phase training procedure.  In the first phase, annotators had to read the codebook and get familiar with the task. In the second phase, they were asked to identify the main frame in the same subset of 50 headlines. At the end of the second phase, the intercoder reliability (ICR) was 0.58 between the 2 annotators. We analyzed those cases where there were discrepancies, and observed that in some cases, there was not a unique main frame, because both annotators had valid arguments to select one of the frames. In other cases, the discrepancies were due to slight misunderstanding of the definitions. In the third phase, the annotators coded again 50 headlines, and the ICR increased to was 0.66. We realized that the possibility of having two frames remained. They discussed the cases in which they had disagreed, and if the other person\u2019s arguments were considered valid, it could be said that there were two frames. After this three-phase training procedure, annotators were ready to annotate the dataset independently. We divided the dataset into two equal parts, and each person annotated 893 headlines.  4.2 Fine-tuning GPT-3.5 and BERT-based  models  With the annotated dataset, we investigated two NLP approaches: the first one involves fine-tuning a pre-trained model; the second one is prompt engineering. Pre-trained language models have been  Figure 1: Pre-train, fine-tune, prompt  trained with large text strings based on two unsupervised tasks, next sentence prediction and masked language model. Figure 1 summarizes these techniques.  In the first approach, a model with a fixed architecture is pre- trained as a language model (LM), predicting the likelihood of the observed textual data. This can be done due to the availability of large, raw text data needed to train LMs. This learning process can produce general purpose features of the modeled language. The learning process produces robust, general-purpose features of the language being modeled. The above pre-trained LM is then adapted to different downstream tasks, by introducing additional parameters and adjusting them using task-specific objective functions. In this approach, the focus was primarily on goal engineering, designing the training targets used in both the pre-training and the fine-tuning stages [36].  630\fFraming the News: From Human Perception to Large Language Model Inferences  We present an example to illustrate the idea. Imagine that the task is sentiment analysis, and we have a dataset with sentences and their associated sentiment, and a pre-trained model, which is a saved neural network trained with a much larger dataset. For that pre-trained model to address the target task, we unfreeze a few of the top layers of the saved model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows to \"fine-tune\" the higher-order feature representations in the base model to make them more relevant for the sentiment analysis task. In this way, instead of having to obtain a very large dataset with target labels to train a model, we can reuse the pre- trained model and use a much smaller train dataset. We use a part of our dataset as examples for the model to learn the task, while the other part of the dataset is used to evaluate model performance. Previous works related to frame classification in the computing literature have used fine-tuning, BERT-based models. In our work, we have done the same as a baseline, but we aimed to go one step further and also produce results using fine-tuning of GPT-3.5.  4.3 Prompt-engineering with GPT-3.5 Model fine-tuning has been widely used, but with the emergence of generative models such as GPT-3, another way to approach classification tasks has appeared. The idea is to use the pre-trained model directly and convert the task to be performed into a format as close as possible to the tasks for which it has been pre-trained. That is, if the model has been pre-trained from next word prediction as in the case of GPT-3, classification can be done by defining a prompt, where the input to the model is an incomplete sentence, and the model must complete it with a word or several words, just as it has been trained. This avoids having to use part of the already labeled dataset to teach the task to be performed to the model, and a previous labeling is not needed [36].  In this approach, instead of adapting pre-trained LMs to down- stream tasks via objective engineering, downstream tasks are re- formulated to look more like those solved during the original LM training with the help of a textual prompt. For example, when recog- nizing the emotion of a social media post, \u201cI missed the bus today.\u201d, we may continue with a prompt \u201cI felt so _\u201d, and ask the LM to fill the blank with an emotion-bearing word. Or if we choose the prompt \u201cEnglish: I missed the bus today. French: _\u201d), an LM may be able to fill in the blank with a French translation. In this way, by selecting the appropriate prompts, we can influence the model behavior so that the pre-trained LM itself can be used to predict the desired output, even without any additional task-specific training [36].  We use this emerging NLP approach to classify frames at headline level. We are not aware of previous uses of this strategy to classify frames as we propose here. The idea is the following. Prompt engi- neering consists of giving a prompt to the model, and understands that prompt as an incomplete sentence. To do prompt engineer- ing with our dataset, we needed to define an appropriate prompt that would produce the headline frames as output. We defined sev- eral experiments with the Playground of GPT-3, in order to find the best prompt for our task. In our initial experiments, we fol- lowed existing approaches in prompt engineering to do sentiment analysis, where the individual answer was an adjective, and this  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  adjective was matched with a sentiment. In a similar fashion, we decided to build a thesaurus of adjectives that define each of the frames. For instance, the human interest frame could be \u2019interest- ing\u2019, \u2019emotional\u2019, \u2019personal\u2019, \u2019human\u2019. The conflict frame could be: \u2019conflictive\u2019, \u2019bellicose\u2019, \u2019troublesome\u2019, \u2019rowdy\u2019, \u2019quarrelsome\u2019, \u2019trou- blemaker\u2019, \u2019agitator\u2019, etc. After the list of adjectives was defined, we needed to define the prompt in order to get, as an answer, one of the adjectives in our thesaurus to match them with the frame. We used the GPT-3 playground using the headline as input and asking for the frame as output, but the strategy did not work. In our final experiment, instead of giving the headline as input, we gave the definitions of each type of frame plus the headline, and we asked the model to choose between the different types of frames as output. In this way, the output of the model was directly one of the frames, and we avoided the step of matching adjectives with frames. An example is shown in Figure 2.  Figure 2: GPT-3.5 for frame inference: input and output  For the GPT-3 configuration 1, there are 3 main concepts:  \u2022 TEMPERATURE [0-1]. This parameter controls randomness,  lowering it results in less random completions.  \u2022 TOP_P [0-1]. This parameter controls diversity via nucleus  sampling.  \u2022 MAX_TOKENS[1-4000]. This parameter indicates the maxi-  mum number of tokens to generate,  \u2022 MODEL. GPT-3 offer four main models with different levels of power, suitable for different tasks. Davinci is the most capable model, and Ada is the fastest.  After testing with the GPT-3 playground and varying different hyper-parameters to assess performance, we set the temperature to 0, since the higher the temperature the more random the response. Furthermore, the Top-p parameter was set to 1, as it would likely get a set of the most likely words for the model to choose from. The maximum number of tokens was set to 2; in this way, the model is asked to choose between one of the responses. As a model, we used the one with the best performance at the time of experimental design, which was TEXT-DAVINCI-003, recognized as GPT 3.5.  5 RESULTS: HUMAN LABELING OF FRAMES  IN NO-VAX NEWS HEADLINES (RQ1)  In this section, we present and discuss the results of the analysis related to our first RQ.  Figure 3 shows the distribution of frames per country at headline level, with human interest and no-frame being the predominant   631\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  ones. Attribution of responsibility is the third one except in Switzer- land, where the corresponding frame is conflict. Finally, morality and economic are the least represented in the dataset for every country.  Figure 4: Non-normalized monthly distribution of frames.  detail the negative and positive sentiment of each frame category, we observed a few trends:  \u2022 Attribution of responsibility: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Switzerland, and the United Kingdom. \u2022 Conflict: Negative sentiment represents 20-35% of the cases. \u2022 Economic: Predominantly neutral, with only negative tone in Italy and UK (in the latter case, all headlines with this frame were considered negative.)  \u2022 Human interest: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Spain, and Switzerland.  \u2022 Morality: Predominantly neutral, with negative tone in Italy,  Switzerland, and the United Kingdom, \u2022 No frame: 20-30% of negative content.  Figure 5: Sentiment of headline by frame and by country  Figure 3: Non-normalized distribution of frames per country  The monthly distribution of frames aggregated for all countries is shown in Fig. 4. We can see two big peaks, the first one in January 2021 and the second one in August 2021. In all countries, the vac- cination process started at the end of December 2020, so it makes sense that the no-vax movement started to be more predominant in the news in January 2021. Human interest is the most predominant frame. Manual inspection shows that this is because the headlines are about personal cases of people who are pro- or anti- vaccine. Attribution of responsibility is also present. Manual inspection in- dicates that local politicians and health authorities had to make decisions about who could be vaccinated at the beginning of the process. The second peak at the end of summer 2021 coincided with the health pass (also called Covid passport in some countries), and we can observe a peak in the curve corresponding to the con- flict frame, reflecting the demonstrations against the measure of mandatory health passes taken by country governments.  In Figure 5, we compare the sentiment per frame and per country, to understand if there were any major differences. The sentiment analysis labels were obtained using BERT-sent from the Hugging Face package [47], used in our previous work (please refer to our original analysis in [3] for details.) We normalized the results be- tween 0 and 1 to compare frames between countries. We see that the sentiment is predominantly neutral (in blue). Examining in more  632\fFraming the News: From Human Perception to Large Language Model Inferences  Regarding the results of the annotation process, the fact that the distribution of the 6 frame types is relatively similar between coun- tries suggests that the anti-vaccine movement issue was treated in a similar way in these countries. The fact that human interest is the most dominant frame indicates that this issue was treated from a more human and emotional approach, with headlines about personal experiences, celebrities giving their opinion about vacci- nation, and politicians defending vaccine policies. Moreover, the reason for many headlines being classified as no-frame is partly due to how data was selected. We chose articles that contained words related to no-vax, either in the headline or in the article. This resulted in many headlines not containing anything specific related to no-vax, while the no-vax content was actually included in the main text of the corresponding articles.  It is worth mentioning that prior to obtaining the results, we had expected that attribution of responsibility would be among the most prominent frames, since governments took many measures such as mandatory health pass requirements to access certain sites; we had also expected that the conflict frame would be prominent, since there were many demonstrations in Europe. In reality, however, these frames categories were not reflected as frequently at the headline level.  Regarding the analysis at the temporal level, it is clear that certain events were captured by the press, such as the start of vaccination or the mandatory vaccination passport.  Finally, the sentiment analysis of the different frames shows that the predominant tone in all of them is neutral or negative, with very similar trends between countries. This association between senti- ment analysis and frames has been discussed in previous literature [11] [43].  6 RESULTS: GPT-3.5 FOR FRAME  CLASSIFICATION OF HEADLINES (RQ2) Here, we present and discuss the results related to our second RQ.  6.1 Fine-tuning GPT-3.5 Table 4 shows the results of the 6-class classification task using 5-cross validation. Three models were used: GPT-3.5 and two BERT- based models. We observe that, on average, GPT-3.5 performs better than the BERT-based models. This is somehow expected as GPT- 3.5 is a much larger model. Overall, in the case of fine-tuning, the best performance for the six-class frame classification task is 72% accuracy, which is promising, with an improvement over previous models based on BERT. Yet, it should be noted that the performance differences are modest (2% improvement between GPT-3.5 and RoBERTa).  Table 4: Classification results for six-class frame classifica- tion and 5-fold cross validation  ACCURACY  BERT RoBERTa GPT3  0  0.68 0.70 0.75  1  0.69 0.72 0.70  2  0.72 0.72 0.72  3  0.64 0.67 0.71  4  0.70 0.71 0.71  AVERAGE  0.67 0.70 0.72  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  On the other hand, BERT is open-source, while GPT-3 has an economic cost as the use of the model is not free, which monetarily limits the number of experiments that can be performed with it, as well as the different configurations one can explore to improve performance. This is important because much of the improvement in performance requires empirical explorations of model parameters More specifically, the cost of an experiment for each of the folds has a cost of 4 dollars (at the time of writing this paper.) This represents a limitation in practice.  Furthermore, GPT-3 has a significant carbon footprint. Similarly, for prompt engineering (discussed in the next subsection), choosing the right prompt (i.e., the words that best define the task so that the model is able to perform adequately) is also based on trial and error. This also has an impact on carbon footprint. In connection with this topic, Strubell et al.[55] argue that improvements in the accu- racy of models depend on the availability of large computational resources, which involve large economic and environmental costs. A criticism has been made as \u2019the rich get richer\u2019, in the sense that not all research groups have sufficient infrastructure resources and access to funding needed to use these models and improve their performance. Also in relation to this analysis, the work of Bender et al. [4] evaluates the costs and risks of the use of large language models, stating that researchers should be aware of the impact that these models have on the environment, and assess whether the benefits outweigh the risks. The work in [4] provides a very telling example, where people living in the Maldives or Sudan are affected by floods and pay the environmental price of training English LLMs, when similar models have not been produced for languages like Dhivehi or Sudanese Arab. In short, there is a need to establish ways to use this technological development responsibly, and it all starts with being aware of the risks it presents.  6.2 Prompt-engineering with GPT-3.5 For each headline, we got the frame that the model considered the most likely, and we compared these GPT-3.5 inferences with the frames labeled by the annotators. The agreement between model and annotator was of 49%. Analyzing the results, and specifically looking at the cases where the annotator and GPT-3.5 disagreed, we discovered that according to the frame definitions, the model in some cases proposed a frame that indeed made sense. This ob- servation, together with our previous experience in the annotation process, where headlines could have more than one valid frame, led us to design a second post-hoc experiment. We took all the headlines where each of the two annotators had disagreed with GPT-3.5, and we asked the annotators to state whether they would agree (or not) with each GPT-inferred label for a given headline. It is important to emphasize that the annotators did not know the origin of that label, i.e., they did not know if it was the label they had originally assigned, or if it was a random one. In this way, we could quantify how GPT-3.5 worked according to valid arguments provided by the annotators. In this post-hoc experiment, the model agreed in 76% of cases with the annotators.  Looking at the results of the classification models, the 49% accu- racy of the prompt-engineering approach can be considered low, yet we consider that it is a valid avenue for further investigation, as in the second post-hoc analysis, we found that the model agrees  633\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  with human annotators in 76% of the cases. Clearly, framing in- volves aspects of subjectivity [42]. Much of what we do as people has a subjective component, influenced by how we feel or how we express opinions.  News reading is never fully objective, and the annotators en- gaged in the frame classification task, influenced by their personal state of mind, experience, and culture, may perceive information differently. Monarch affirms that \"for simple tasks, like binary labels on objective tasks, the statistics are fairly straightforward to decide which is the \u2018correct\u2019 label when different annotators disagree. But for subjective tasks, or even objective tasks with continuous data, there are no simple heuristics for deciding what the correct label should be\" [42].  Subjectivity is involved in both the generation and perception of information: the assumption that there is only one frame is com- plicated by the point of view of the reader. In the case of news, the information sender (the journalist) has an intention, but the receiver (the reader) plays a role and is influenced by it. In psychology, this is known as the lens model of interpersonal communication, where the sender has certain objectives, but the receiver can interpret or re-interpret what the sender wants to say, with more or less accuracy [26].  Following this discussion on subjectivity, the question arose as to what would happen if, instead of headlines, we used the complete article as a source of analysis. We wondered if longer text could make the frame labeling task clearer than when using headlines. Yet another possible hypothesis is that having to read longer texts could lead to the same subject being presented from different angles. Please recall that in the existing literature discussed in Section 2, both headlines and full articles have been used from frame analysis (see Table 1.) This remains as an issue for future work.  7 CONCLUSIONS In this paper, we first presented an analysis of human-generated news frames on the covid-19 no-vax movement in Europe, and then studied different approaches using large language models for automatic inference of frames. We conclude by answering the two research questions we posed:  RQ1: What are the main frames in the news headlines about the covid-19 anti-vaccine movement in 5 European countries? After annotating the headlines, we found that of the 1786 headlines, the predominant frame is human interest (45.3% of cases), which presents a news item with an emotional angle, putting a face to a problem or situation. We also found that a substantial proportion of headlines were annotated as not presenting any frame (40.2% of cases). Finally, the other frame types are found more infrequently. RQ2: Can prompt engineering be used for classification of head- lines according to frames? We first used fine-tuning of a number of language models, and found that GPT-3.5 produced classification ac- curacy of 72% on a six-frame classification task. This represented a modest 2% improvement over BERT-based models, at a significantly larger environmental cost. We then presented a new way of classi- fying frames using prompts. At the headline level, inferences made with GPT-3.5 reached 49% of agreement with human-generated frame labels. In many cases, the GPT-3.5 model inferred frame types that were considered as valid choices by human annotators,  and in an post-doc experiment, the human-machine agreement reached 76%. These results have opened several new directions for future work.  ACKNOWLEDGMENTS",
  "references": [
    "1] Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online media: Quantitative framing analysis on Detik. com\u2019s coverage of Covid-19. Jurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153\u2013170.  ",
    "2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. 2021. RAFT: A real-world few-shot text classification benchmark. arXiv preprint arXiv:2109.14076 (2021).  ",
    "3] David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe\u2019s Press (2022), 35\u201343.  Cover Covid-19 Vaccination News? A Five-Country Analysis. https://doi.org/10.1145/3512732.3533588  ",
    "4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? (2021), 610\u2013623.  ",
    "5] Santosh Kumar Biswal and Nikhil Kumar Gouda. 2020. Artificial intelligence in journalism: A boon or bane? In Optimization in machine learning and applications. Springer, 155\u2013167.  ",
    "6] Erik Bleich, Hannah Stonebraker, Hasher Nisar, and Rana Abdelhamid. 2015. Media portrayals of minorities: Muslims in British newspaper headlines, 2001\u2013 2012. Journal of Ethnic and Migration Studies 41, 6 (2015), 942\u2013962.  ",
    "7] Michael Bommarito and Daniel Martin Katz. 2022. GPT Takes the Bar Exam.  https://doi.org/10.48550/ARXIV.2212.14402  ",
    "8] Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe, Michel Dupagne, and Ching-Hua Chuan. 2019. Artificial intelligence and jour- nalism. Journalism & Mass Communication Quarterly 96, 3 (2019), 673\u2013695. ",
    "9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901.  ",
    "10] Bj\u00f6rn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, and Claes H De Vreese. 2014. Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis. Communication Methods and Measures 8, 3 (2014), 190\u2013206.  ",
    "11] Bjorn Burscher, Rens Vliegenthart, and Claes H de Vreese. 2016. Frames beyond words: Applying cluster and sentiment analysis to news coverage of the nuclear power issue. Social Science Computer Review 34, 5 (2016), 530\u2013545.  ",
    "12] Dallas Card, Amber Boydstun, Justin Gross, Philip Resnik, and Noah Smith. 2015. The Media Frames Corpus: Annotations of Frames Across Issues. 2 (01 2015), 438\u2013444. https://doi.org/10.3115/v1/P15-2072  ",
    "13] Daniel Catalan-Matamoros and Carlos El\u00edas. 2020. Vaccine hesitancy in the age of coronavirus and fake news: analysis of journalistic sources in the Spanish quality press. International Journal of Environmental Research and Public Health 17, 21 (2020), 8136.  ",
    "14] Daniel Catal\u00e1n-Matamoros and Carmen Pe\u00f1afiel-Saiz. 2019. Media and mistrust of vaccines: a content analysis of press headlines. Revista latina de comunicaci\u00f3n social 74 (2019), 786\u2013802.  ",
    "15] Mark Coddington. 2015. Clarifying journalism\u2019s quantitative turn: A typology for evaluating data journalism, computational journalism, and computer-assisted reporting. Digital journalism 3, 3 (2015), 331\u2013348.  ",
    "16] Stephen D Cooper. 2010. The oppositional framing of bloggers. In Doing News  Framing Analysis. Routledge, 151\u2013172.  ",
    "17] Robert Dale. 2021. GPT-3: What\u2019s it good for? Natural Language Engineering 27,  1 (2021), 113\u2013118.  ",
    "18] Astrid Dirikx and Dave Gelders. 2010.  To frame is to explain: A de- ductive frame-analysis of Dutch and French climate change coverage dur- ing the annual UN Conferences of the Parties. Public Understanding of Science 19, 6 (2010), 732\u2013742. https://doi.org/10.1177/0963662509352044 arXiv:https://doi.org/10.1177/0963662509352044 PMID: 21560546.  ",
    "19] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A deductive frame- analysis of Dutch and French climate change coverage during the annual UN Conferences of the Parties. Public understanding of science 19, 6 (2010), 732\u2013742.  634\fFraming the News: From Human Perception to Large Language Model Inferences  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  ",
    "20] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2020. Gsum: A general framework for guided neural abstractive summarization. arXiv preprint arXiv:2010.08014 (2020).  ",
    "21] Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA Gesondheid (Online) 27 (2022), 1\u20138.  ",
    "22] Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021). ",
    "23] Robert M Entman. 1993. Framing: Towards clarification of a fractured paradigm.  McQuail\u2019s reader in mass communication theory 390 (1993), 397.  ",
    "24] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723 (2020). ",
    "25] Piyush Ghasiya and Koji Okamura. 2021. Investigating COVID-19 news across four nations: a topic modeling and sentiment analysis approach. Ieee Access 9 (2021), 36645\u201336656.  ",
    "26] Robert Gifford. 1994. A Lens-Mapping Framework for Understanding the En- coding and Decoding of Interpersonal Dispositions in Nonverbal Behavior. Journal of Personality and Social Psychology 66 (02 1994), 398\u2013412. https: //doi.org/10.1037//0022-3514.66.2.398  ",
    "27] Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based transformer architectures for long document summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 1792\u20131810.  ",
    "28] Anushka Gupta, Diksha Chugh, Rahul Katarya, et al. 2022. Automated news summarization using transformers. In Sustainable Advanced Computing. Springer, 249\u2013259.  ",
    "29] Alfred Hermida and Mary Lynn Young. 2017. Finding the data unicorn: A hierar- chy of hybridity in data and computational journalism. Digital Journalism 5, 2 (2017), 159\u2013176.  ",
    "30] Karoliina Isoaho, Daria Gritsenko, and Eetu M\u00e4kel\u00e4. 2021. Topic modeling and text analysis for qualitative policy research. Policy Studies Journal 49, 1 (2021), 300\u2013324.  ",
    "31] Carina Jacobi, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative analysis of large amounts of journalistic texts using topic modelling. Digital journalism 4, 1 (2016), 89\u2013106.  ",
    "32] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computa- tional Linguistics 8 (2020), 423\u2013438.  ",
    "33] Shima Khanehzar, Andrew Turpin, and Gosia Miko\u0142ajczak. 2019. Modeling  ",
    "46] Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with  mixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021).  ",
    "47] Rabindra Lamsal. 2021. Sentiment Analysis of English Tweets with BERTsent. https://huggingface.co/rabindralamsal/finetuned-bertweet-sentiment-analysis. ",
    "48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.  ",
    "49] Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, and Ahad Ali. 2022. Fake News Classification using transformer based enhanced LSTM and BERT. International Journal of Cognitive Computing in Engineering 3 (2022), 98\u2013105. https://doi.org/10.1016/j.ijcce.2022.03.003  ",
    "50] Frida V Rodelo. 2021. Framing of the Covid-19 pandemic and its organizational  predictors. Cuadernos. info 50 (2021), 91\u2013112.  ",
    "51] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).  ",
    "52] Holli Semetko and Patti Valkenburg. 2000. Framing European Politics: A Content Analysis of Press and Television News. Journal of Communication 50 (06 2000), 93 \u2013 109. https://doi.org/10.1111/j.1460-2466.2000.tb02843.x  ",
    "53] Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Em- manouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. arXiv preprint arXiv:2104.08768 (2021).  ",
    "54] Efstathios Sidiropoulos and Andreas Veglis. 2017. Computer Supported Collab- orative Work trends on Media Organizations: Mixing Qualitative and Quan- titative Approaches. Studies in Media and Communication 5 (04 2017), 63. https://doi.org/10.11114/smc.v5i1.2279  ",
    "55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019). ",
    "56] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503 (2021).  ",
    "57] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning.  arXiv preprint arXiv:1806.02847 (2018).  ",
    "58] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems 34 (2021), 200\u2013212.  ",
    "59] Sandra A Vannoy and Prashant Palvia. 2010. The social influence model of  technology adoption. Commun. ACM 53, 6 (2010), 149\u2013153.  ",
    "60] Tuukka Yl\u00e4-Anttila, Veikko Eranti, and Anna Kukkonen. 2022. Topic modeling for frame analysis: A study of media debates on climate change in India and USA. Global Media and Communication 18, 1 (2022), 91\u2013112.  Political Framing Across Policy Issues and Contexts. In ALTA.  ",
    "34] Jeesun Kim and Wayne Wanta. 2018. News framing of the US immigration debate during election years: Focus on generic frames. The Communication Review 21, 2 (2018), 89\u2013115.  ",
    "35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi- hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).  ",
    "36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. (2021). https://doi.org/10. 48550/ARXIV.2107.13586  ",
    "37] Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and Derry Tanti Wijaya. 2019. Detecting frames in news headlines and its application to analyzing news framing trends surrounding US gun violence. In Proceedings of the 23rd conference on computational natural language learning (CoNLL).  ",
    "38] J\u00f6rg Matthes and Matthias Kohring. 2008. The Content Analysis of Media Frames: Toward Improving Reliability and Validity. Journal of Communication 58 (06 2008). https://doi.org/10.1111/j.1460-2466.2008.00384.x  ",
    "39] Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, and David E Losada. 2022. Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI. In Proceedings of the 4th Conference on Conversational User Interfaces. 1\u20136.  ",
    "40] Stuart E Middleton, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Social computing for verifying social media content in breaking news. IEEE Internet Computing 22, 2 (2018), 83\u201389.  ",
    "41] Marko Milosavljevi\u0107 and Igor Vobi\u010d. 2021.  \u2018Our task is to demystify fears\u2019: Analysing newsroom management of automation in journalism. Journalism 22, 9 (2021), 2203\u20132221.  ",
    "42] R. Monarch. 2021. Human-in-the-Loop Machine Learning: Active Learning and Annotation for Human-centered AI. Manning. https://books.google.ch/books? id=LCh0zQEACAAJ  ",
    "43] Tom Nicholls and Pepper D Culpepper. 2021. Computational identification of media frames: Strengths, weaknesses, and opportunities. Political Communication 38, 1-2 (2021), 159\u2013181.  ",
    "44] Zhongdang Pan and Gerald M Kosicki. 1993. Framing analysis: An approach to  news discourse. Political communication 10, 1 (1993), 55\u201375.  ",
    "45] Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative  language models. arXiv preprint arXiv:1912.10165 (2019).  635\f"
  ],
  "url": "https://drive.google.com/uc?id=1vmiWI3NUHORkl-g7D9WA8Xh7YbTOHfmB",
  "date": "2024-01-09 21:23:22",
  "is_published": false
}
{
  "title": "Framing the News: From Human Perception to Large Language Model Inferences",
  "authors": [
    "David Alonso del Barrio",
    "Idiap Research Institute",
    "Daniel Gatica-Perez",
    "Idiap Research Institute and EPFL"
  ],
  "institutions": [
    "Switzerland",
    "Switzerland"
  ],
  "abstract": "Identifying the frames of news is important to understand the arti- cles\u2019 vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of jour- nalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to under- stand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No- Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Nat- ural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT- 3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.",
  "keywords": [
    "Covid-19 no-vax",
    " news framing",
    " GPT-3",
    " prompt-engineering",
    " trans-",
    "formers",
    " large language models"
  ],
  "article": "In recent years, there has been a proliferation in the use of concepts such as data journalism, computational journalism, and computer- assisted reporting [15] [29], which all share the vision of bridging journalism and technology. The progress made in NLP has been gradually integrated into the journalistic field [5][8][54]. More specifically, machine learning models based on transformers have been integrated in the media sector in different tasks [41] such as the creation of headlines with generative languages models [17], summarization of news articles [28][27], false news detection [49], and topic modeling and sentiment analysis [25]. The development of large language models such as GPT-3 [9], BLOOM [51] or ChatGPT show a clear trend towards human-machine interaction becoming easier and more intuitive, opening up a wide range of research possibilities. At the same time, the use of these models is also associated with a lack of transparency regarding how these models work, but efforts are being made to bring some transparency to where they cannot [35]. Based on the premises that these models open up a wide range of research directions [7], and that at the same time (and needless to say) they are not the solution to all problems, can be potentially useful, while acknowledging and systematically documenting their limitations [56]. More specifically, the aim of this work is to analyze the performance of GPT-3.5 for a specific use case, namely the analysis of frames in news, from an empirical point of view, with the objective of shedding light on a potential use of generative models in journalistic tasks.  Frame analysis is a concept from journalism, which consists of studying the way in which news stories are presented on an issue, and what aspects are emphasized: Is a merely informative vision given in an article? Or is it intended to leave a moral lesson? Is a news article being presented from an economic point of view? Or from a more human, emotional angle? The examples above correspond to different frames with which an article can be written. The concept of news framing has been studied in computing as a step beyond topic modeling and sentiment analysis, and for this purpose, in recent years, pre-trained language models have been used for fine-tuning the classification process of these frames [60] [10], but the emergence of generative models opens the possibility of doing prompt-engineering of these classification tasks, instead of the fine-tuning approach investigated so far.  Our work aims to address this research gap by posing the fol-  lowing research questions:  RQ1: What are the main frames in the news headlines about the anti-vaccine movement, as reported in newspapers across 5 European countries?  627\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  RQ2: Can prompt engineering be used for classification of head-  lines according to frames?  By addressing the above research questions, our work makes the  following contributions:  Contribution 1. We implemented a process to do human an- notation of the main frame of 1786 headlines of articles about the Covid-19 no-vax movement, as reported in 19 newspapers from 5 European countries (France, Italy, Spain, Switzerland and United Kingdom.) At the headline level, we found that the predominant frame was human interest, where this frame corresponds to a per- sonification of an event, either through a statement by a person, or the explanation of a specific event that happened to a person. Furthermore, we found a large number of headlines annotated as containing no frame, as they simply present information without entering into evaluations. We also found that for all the countries involved, the distribution of frame types was very similar, i.e., hu- man interest and no frame are the two predominant frames. Finally, the generated annotations allowed to subsequently study the per- formance of a large language model.  Contribution 2. We studied the performance of GPT-3.5 on the task of frame classification of headlines. In addition to using the fine-tuning approach from previous literature, we propose an alternative approach for frame classification that requires no labeled data for training, namely prompt-engineering using GPT-3.5. The results show that fine-tuning with GPT-3.5 produces 72% accuracy (slightly higher than other smaller models), and that the prompt- engineering approach results in lower performance (49% accuracy.) Our analysis also shows that the subjectivity of the human labeling task has an effect on the obtained accufracy.  The paper is organized as follows. In Section 2, we discuss related work. In Section 3, we describe the news dataset. In Section 4, we describe the methodology for both human labeling and machine classification of news frames. We present and discuss results for RQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide conclusions in Section 7.  2 RELATED WORK Framing has been a concept widely studied in journalism, with a definition that is rooted in the study of this domain [23]: \u201cTo frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a par- ticular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.\u201d  For frame recognition, there are two main approaches: the induc- tive approach [16], where one can extract the frames after reading the article, and the deductive approach [38], where a predefined list of frames exists and the goal is to interpret if any of them ap- pears in the article. In the deductive case, there are generic frames and subject-specific frames, and the way to detect them typically involves reading and identifying one frame at a time, or through answers to yes/no questions that represent the frames. Semetko et al. [52] used 5 types of generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) based on previous literature, and they defined a list of 20 yes/no questions to detect frames in articles. For instance, the questions about morality are the following: \"Does the story contain any moral  message? Does the story make reference to morality, God, and other religious tenets? Does the story offer specific social prescriptions about how to behave?\", and so on for each of the frame types. This categorization of frames has been used in various topics such as climate change [18] [19], vaccine hesitance [13], or immigration [34].  We now compare the two approaches on a common topic, such as Covid-19. Ebrahim et al. [21] followed an inductive approach in which the frames were not predefined but emerged from the text (e.g., deadly spread, stay home, what if, the cost of Covid-19) using headlines as the unit of analysis. In contrast, the deductive approach has studied very different labels. El-Behary et al. [22] followed the method of yes/no questions, but in addition to the 5 generic frames presented before, they also used blame frame and fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5 generic frames with yes/no questions, while Catal\u00e1n-Matamoros et al. [14] used the 5 frames and read the headline and subheadline to decide the main frame. Table 1 summarizes some of the the existing approaches. This previous work showed how frame labels can be different, and also that frame analysis has been done at both headline and article levels. These two approaches (inductive and deductive) that originated in journalism have since been replicated in the computing literature.  We decided to follow the deductive approach because a prede- fined list of frames allows to compare among topics, countries, previous literature, and also because they represent a fixed list of labels for machine classification models. Furthermore, the induc- tive approach tends to be more specific to a topic, and from the computing viewpoint, past work has tried to justify topic modeling as a technique to extract frames from articles.  Yl\u00e4-Antitila et al. [60] proposed topic modeling as a frame ex- traction technique. They argued that topics can be interpreted as frames if three requirements are met: frames are operationalized as connections between concepts; subject-specific data is selected; and topics are adequately validated as frames, for which they suggested a practical procedure. This approach was based on the choice of a specific topic (e.g., climate change) and the use of Latent Dirichlet Allocation (LDA) as a technique to extract a number of subtopics. In a second phase, a qualitative study of the top 10 words of each subtopic was performed, and the different subtopics were elimi- nated or grouped, reducing the number and establishing a tentative description. In a third phase, the top 10 articles belonging to that frame/topic were taken, and if the description of the topic fitted at least 8 of the 10 articles, that topic/frame remained. The frames found in this article were: green growth, emission cuts, negotiations and treaties, environmental risk, cost of carbon emissions, Chinese emissions, economics of energy production, climate change, en- vironmental activism, North-South burden sharing, state leaders negotiating, and citizen participation.  From Entman\u2019s definition of frame [23], it seems that the deduc- tive approach is more refined than the inductive approach (which seems to resemble the detection of sub-themes.) For example, with regard to climate change, there are stories on how people have been affected by climate change from an emotional point of view, thus personalizing the problem. In this case, we could categorize the corresponding frame as human interest, as the writer of the article is selecting \"some aspects of a perceived reality and make them  628\fFraming the News: From Human Perception to Large Language Model Inferences  more salient\". The language subtleties with which news articles are presented cannot be captured with basic topic modeling.  Isoaho et al.[30] held the position that while the benefits of scale and scope in topic modeling were clear, there were also a number of problems, namely that topic outputs do not correspond to the methodological definition of frames, and thus topic modeling remained an incomplete method for frame analysis. Topic modeling, in the practice of journalistic research, is a useful technique to deal with the large datasets that are available, yet is often not enough to do more thorough analyses [31]. In our work, we clearly notice that frame analysis is not topic modeling. For example, two documents could be about the same topic, say Covid-19 vaccination, but one article could emphasize the number of deaths after vaccination, while the other emphasized the role of the vaccine as a solution to the epidemic.  We also consider that the larger the number of possible frame types, the more likely it is to end up doing topic modeling instead of frame analysis. Using a deductive approach, Dallas et al. [12] created a dataset with articles about polemic topics such as immigration, same sex marriage, or smoking, and they defined 15 types of frames: \"economic, capacity and resources, morality, fairness and equality, legality, constitutionality and jurisprudence, policy prescription and evaluation, crime and punishment, security and defense, health and safety, quality of life, cultural identity, political, external regulation and reputation, other\". In this case, they authors did not use a list of questions. Instead, for each article, annotators were asked to identify any of the 15 framing dimensions present in the article and to label text blurbs that cued them (based on the definitions of each of the frame dimensions) and decide the main frame of each article. In our case, we followed the idea of detecting the main frame by reading the text instead of answering questions, but instead of using the 15 frames proposed in [12] , we used the 5 generic frames proposed in [52].  A final decision in our work was the type of text to analyze, whether headlines or whole article. For this decision, the chosen classification method was also going to be important. For example, Khanehzar et al. [33] used traditional approaches such as SVMs as baseline, and demonstrated the improvement in frame classifica- tion with the use of pre-trained languages models such as BERT, RoBERTa and XLNet, following a fine-tuning approach, setting as input text a maximum of 256 tokens (although the maximum number of input tokens in these models is 512 tokens.) Liu et al. [37] classified news headlines about the gun problem in the United States, arguing for the choice of headlines as a unit of analysis based on previous journalism literature [6], [44], that advocated for the importance and influence of headlines on readers and the subsequent perception of articles. From a computational viewpoint, using headlines is also an advantage, since you avoid the 512 token limitation in BERT-based models. Therefore, we decided to work with headlines about a controversial issue, namely the Covid-19 no-vax movement.  Continuing with the question of the methods used for classi- fication, much work has been developed in prompt engineering, especially since the release of GPT-3. Liu et al.[36] presented a good overview of the work done on this new NLP paradigm, not only explaining the concept of prompt engineering, but also the differ- ent strategies that can be followed both in the design of prompts,  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  Table 1: Summary of deductive approaches for frame analysis  Ref Frames  Goal  Technique  To label frames of full articles  [12] 15 generic frames: \"Economic\", \"Capac- ity and resources\", \"Morality\", \"Fair- ness and equality\", \"Legality, constitu- tionality and jurisprudence\", \"Policy prescription and evaluation\", \"Crime and punishment\", \"Security and de- fense\", \"Health and safety\", \"Quality of life\", \"Cultural identity\", \"Public opin- ion\", \"Political\", \"External regulation and reputation\", \"Other\".  [33] 15 generic frames  Classification  [52] 5 generic frames: \"human interest\", \"conflict\", \"morality\", \"attribution of responsibility\", and \"economic conse- quences\".  \u201cSociety/Culture\u201d, ,  [37] 9 specific frames:\u201cPolitics\u201d, \u201cPublic opinion\u201d, and \u201c2nd \u201cEconomic consequences\u201d Amendment\u201d (Gun Rights), \u201cGun control/regulation\u201d, \u201cMental health\u201d, \u201cSchool/Public space safety\u201d, and \u201cRace/Ethnicity\u201d.  [22] 5 generic frames + blame frame and  fear frame 5 generic frames  [1]  To label frames of full articles  To label frames of full articles/ Classification  To label frames of full articles To label frames of full articles  [50] 5 generic frames + pandemic frames  [14] 5 generic frames, journalistic role and  pandemic frames  To label frames of full articles To label frames of full articles  the Reading article, full the annotator defines the main frame  BERT based models Yes/No ques- tions.  the Reading full article, the annotator the defines main frame. BERT based models Yes/No ques- tions. Reading the article, full the annotator defines the main frame. Yes/No ques- tions. Reading the headline and subheadline, the annotator defines the main frame.  Number samples  of  20000 articles  12000 articles  2600 articles and 1522 tv news stories  2990 headlines  1170 articles  6713 articles  2742 articles  131 headlines + subheadlines  the potential applications, and the challenges to face when using this approach. Prompt engineering applications include knowledge probing [46], information extraction [53], NLP reasoning [57], ques- tion answering [32], text generation [20], multi-modal learning [58], and text classification [24], the latter being the prompt-engineering use case in our work. Puri et al.[45] presented a very interesting idea that we apply to our classification task. This consists of pro- viding the language model with natural language descriptions of classification tasks as input, and training it to generate the correct answer in natural language via a language modeling objective. It is a zero-shot learning approach, in which no examples are used to explain the task to the model. Radford et al. [48] demonstrated that language models can learn tasks without any explicit supervision. We have followed this approach to find an alternative way to do frame analysis.  As mentioned before, the emergence of giant models like GPT-3, BLOOM, and ChatGPT are a very active research topic. To the best of our knowledge, on one hand our work extends the computational analysis of news related to the covid-19 no-vax movement, which illustrates the influence of the press on the ways societies think about relevant issues [40], [59], and on the other hand it adds to the literature of human-machine interaction, regarding the design of GPT-3 prompts for classification tasks [39], [2].  3 DATA: EUROPEAN COVID-19 NEWS  DATASET  We used part of the European Covid-19 News dataset collected in our recent work [3]. This dataset contains 51320 articles on Covid- 19 vaccination from 19 newspapers from 5 different countries: Italy,  629\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  France, Spain, Switzerland and UK. The articles cover a time period of 22 months, from January 2020 to October 2021. All content was translated into English to be able to work in a common language. The dataset was used for various analyses, such as name entity recognition, sentiment analysis, and subtopic modeling, to under- stand how Covid-19 vaccination was reported in Europe through the print media (in digital format.) The subtopic modeling analysis revealed a subsample of articles on the no-vax movement, which is the one we have used in this paper. We took the headlines of the articles associated with the no-vax movement, selecting all articles containing any of the keywords in Table 2 in the headline or in the main text. This corresponds to a total of 1786 headlines.  Table 2: Keywords used to identify no-vax articles  NO VAX TOPIC  \"anti-vaxxers\", \"anti-vaccine\", \"anti-vaxx\", \"anti-corona\", \"no-vax\", \"no vax\",\"anti-vaccin\"  Keywords  In Table 3, we show the number of headlines per country and newspaper. France is the country with the most no-vax articles in the corpus, with 523 articles, followed by Italy with 508. However, note that there are 6 newspapers from France, while only 2 from Italy. Corriere della Sera is the newspaper that dealt most frequently with the subject (429 articles), while The Telegraph is the second one (206 articles). The total number of articles normalized by the number of newspapers per country is also shown in the last column of the Table. Using these normalized values, the ranking is Italy, UK, France, Switzerland, and Spain.  Table 3: Number of headlines by newspaper and country  COUNTRY  NEWSPAPER  HEADLINES  TOTAL (NORM. TOTAL)  FRANCE  ITALY  SPAIN  SWITZERLAND  UNITED KINGDOM  La Croix Le Monde Les Echos Liberation Lyon Capitale Ouest France Corriere della Sera Il Sole 24 Ore 20 minutos ABC El Diario El Mundo El Espa\u00f1ol La Vanguardia 24 heures La Libert\u00e9 Le Temps The Irish News The Telegraph  94 125 49 97 8 150 429 79 27 50 32 77 22 95 97 22 111 16 206  523 (87.1)  508 (254.0)  303 (50.5)  230 (76.6)  222 (111.0)  1786  4 METHODOLOGY 4.1 Human labeling of news frames To carry out the labeling of the frames in our corpus of headlines, we first designed a codebook, which contained the definitions of each of the frame types and a couple of examples of each type, as well as a definition of the corpus subject matter and definitions of the concept of frame analysis, so that the annotators could understand the task to be performed. The codebook follows the proposed by  [52] with 5 generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) plus one additional \u2019no-frame\u2019 category. Two researchers were engaged to annotate a sample of the collected newspaper articles following a three-phase training procedure.  In the first phase, annotators had to read the codebook and get familiar with the task. In the second phase, they were asked to identify the main frame in the same subset of 50 headlines. At the end of the second phase, the intercoder reliability (ICR) was 0.58 between the 2 annotators. We analyzed those cases where there were discrepancies, and observed that in some cases, there was not a unique main frame, because both annotators had valid arguments to select one of the frames. In other cases, the discrepancies were due to slight misunderstanding of the definitions. In the third phase, the annotators coded again 50 headlines, and the ICR increased to was 0.66. We realized that the possibility of having two frames remained. They discussed the cases in which they had disagreed, and if the other person\u2019s arguments were considered valid, it could be said that there were two frames. After this three-phase training procedure, annotators were ready to annotate the dataset independently. We divided the dataset into two equal parts, and each person annotated 893 headlines.  4.2 Fine-tuning GPT-3.5 and BERT-based  models  With the annotated dataset, we investigated two NLP approaches: the first one involves fine-tuning a pre-trained model; the second one is prompt engineering. Pre-trained language models have been  Figure 1: Pre-train, fine-tune, prompt  trained with large text strings based on two unsupervised tasks, next sentence prediction and masked language model. Figure 1 summarizes these techniques.  In the first approach, a model with a fixed architecture is pre- trained as a language model (LM), predicting the likelihood of the observed textual data. This can be done due to the availability of large, raw text data needed to train LMs. This learning process can produce general purpose features of the modeled language. The learning process produces robust, general-purpose features of the language being modeled. The above pre-trained LM is then adapted to different downstream tasks, by introducing additional parameters and adjusting them using task-specific objective functions. In this approach, the focus was primarily on goal engineering, designing the training targets used in both the pre-training and the fine-tuning stages [36].  630\fFraming the News: From Human Perception to Large Language Model Inferences  We present an example to illustrate the idea. Imagine that the task is sentiment analysis, and we have a dataset with sentences and their associated sentiment, and a pre-trained model, which is a saved neural network trained with a much larger dataset. For that pre-trained model to address the target task, we unfreeze a few of the top layers of the saved model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows to \"fine-tune\" the higher-order feature representations in the base model to make them more relevant for the sentiment analysis task. In this way, instead of having to obtain a very large dataset with target labels to train a model, we can reuse the pre- trained model and use a much smaller train dataset. We use a part of our dataset as examples for the model to learn the task, while the other part of the dataset is used to evaluate model performance. Previous works related to frame classification in the computing literature have used fine-tuning, BERT-based models. In our work, we have done the same as a baseline, but we aimed to go one step further and also produce results using fine-tuning of GPT-3.5.  4.3 Prompt-engineering with GPT-3.5 Model fine-tuning has been widely used, but with the emergence of generative models such as GPT-3, another way to approach classification tasks has appeared. The idea is to use the pre-trained model directly and convert the task to be performed into a format as close as possible to the tasks for which it has been pre-trained. That is, if the model has been pre-trained from next word prediction as in the case of GPT-3, classification can be done by defining a prompt, where the input to the model is an incomplete sentence, and the model must complete it with a word or several words, just as it has been trained. This avoids having to use part of the already labeled dataset to teach the task to be performed to the model, and a previous labeling is not needed [36].  In this approach, instead of adapting pre-trained LMs to down- stream tasks via objective engineering, downstream tasks are re- formulated to look more like those solved during the original LM training with the help of a textual prompt. For example, when recog- nizing the emotion of a social media post, \u201cI missed the bus today.\u201d, we may continue with a prompt \u201cI felt so _\u201d, and ask the LM to fill the blank with an emotion-bearing word. Or if we choose the prompt \u201cEnglish: I missed the bus today. French: _\u201d), an LM may be able to fill in the blank with a French translation. In this way, by selecting the appropriate prompts, we can influence the model behavior so that the pre-trained LM itself can be used to predict the desired output, even without any additional task-specific training [36].  We use this emerging NLP approach to classify frames at headline level. We are not aware of previous uses of this strategy to classify frames as we propose here. The idea is the following. Prompt engi- neering consists of giving a prompt to the model, and understands that prompt as an incomplete sentence. To do prompt engineer- ing with our dataset, we needed to define an appropriate prompt that would produce the headline frames as output. We defined sev- eral experiments with the Playground of GPT-3, in order to find the best prompt for our task. In our initial experiments, we fol- lowed existing approaches in prompt engineering to do sentiment analysis, where the individual answer was an adjective, and this  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  adjective was matched with a sentiment. In a similar fashion, we decided to build a thesaurus of adjectives that define each of the frames. For instance, the human interest frame could be \u2019interest- ing\u2019, \u2019emotional\u2019, \u2019personal\u2019, \u2019human\u2019. The conflict frame could be: \u2019conflictive\u2019, \u2019bellicose\u2019, \u2019troublesome\u2019, \u2019rowdy\u2019, \u2019quarrelsome\u2019, \u2019trou- blemaker\u2019, \u2019agitator\u2019, etc. After the list of adjectives was defined, we needed to define the prompt in order to get, as an answer, one of the adjectives in our thesaurus to match them with the frame. We used the GPT-3 playground using the headline as input and asking for the frame as output, but the strategy did not work. In our final experiment, instead of giving the headline as input, we gave the definitions of each type of frame plus the headline, and we asked the model to choose between the different types of frames as output. In this way, the output of the model was directly one of the frames, and we avoided the step of matching adjectives with frames. An example is shown in Figure 2.  Figure 2: GPT-3.5 for frame inference: input and output  For the GPT-3 configuration 1, there are 3 main concepts:  \u2022 TEMPERATURE [0-1]. This parameter controls randomness,  lowering it results in less random completions.  \u2022 TOP_P [0-1]. This parameter controls diversity via nucleus  sampling.  \u2022 MAX_TOKENS[1-4000]. This parameter indicates the maxi-  mum number of tokens to generate,  \u2022 MODEL. GPT-3 offer four main models with different levels of power, suitable for different tasks. Davinci is the most capable model, and Ada is the fastest.  After testing with the GPT-3 playground and varying different hyper-parameters to assess performance, we set the temperature to 0, since the higher the temperature the more random the response. Furthermore, the Top-p parameter was set to 1, as it would likely get a set of the most likely words for the model to choose from. The maximum number of tokens was set to 2; in this way, the model is asked to choose between one of the responses. As a model, we used the one with the best performance at the time of experimental design, which was TEXT-DAVINCI-003, recognized as GPT 3.5.  5 RESULTS: HUMAN LABELING OF FRAMES  IN NO-VAX NEWS HEADLINES (RQ1)  In this section, we present and discuss the results of the analysis related to our first RQ.  Figure 3 shows the distribution of frames per country at headline level, with human interest and no-frame being the predominant   631\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  ones. Attribution of responsibility is the third one except in Switzer- land, where the corresponding frame is conflict. Finally, morality and economic are the least represented in the dataset for every country.  Figure 4: Non-normalized monthly distribution of frames.  detail the negative and positive sentiment of each frame category, we observed a few trends:  \u2022 Attribution of responsibility: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Switzerland, and the United Kingdom. \u2022 Conflict: Negative sentiment represents 20-35% of the cases. \u2022 Economic: Predominantly neutral, with only negative tone in Italy and UK (in the latter case, all headlines with this frame were considered negative.)  \u2022 Human interest: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Spain, and Switzerland.  \u2022 Morality: Predominantly neutral, with negative tone in Italy,  Switzerland, and the United Kingdom, \u2022 No frame: 20-30% of negative content.  Figure 5: Sentiment of headline by frame and by country  Figure 3: Non-normalized distribution of frames per country  The monthly distribution of frames aggregated for all countries is shown in Fig. 4. We can see two big peaks, the first one in January 2021 and the second one in August 2021. In all countries, the vac- cination process started at the end of December 2020, so it makes sense that the no-vax movement started to be more predominant in the news in January 2021. Human interest is the most predominant frame. Manual inspection shows that this is because the headlines are about personal cases of people who are pro- or anti- vaccine. Attribution of responsibility is also present. Manual inspection in- dicates that local politicians and health authorities had to make decisions about who could be vaccinated at the beginning of the process. The second peak at the end of summer 2021 coincided with the health pass (also called Covid passport in some countries), and we can observe a peak in the curve corresponding to the con- flict frame, reflecting the demonstrations against the measure of mandatory health passes taken by country governments.  In Figure 5, we compare the sentiment per frame and per country, to understand if there were any major differences. The sentiment analysis labels were obtained using BERT-sent from the Hugging Face package [47], used in our previous work (please refer to our original analysis in [3] for details.) We normalized the results be- tween 0 and 1 to compare frames between countries. We see that the sentiment is predominantly neutral (in blue). Examining in more  632\fFraming the News: From Human Perception to Large Language Model Inferences  Regarding the results of the annotation process, the fact that the distribution of the 6 frame types is relatively similar between coun- tries suggests that the anti-vaccine movement issue was treated in a similar way in these countries. The fact that human interest is the most dominant frame indicates that this issue was treated from a more human and emotional approach, with headlines about personal experiences, celebrities giving their opinion about vacci- nation, and politicians defending vaccine policies. Moreover, the reason for many headlines being classified as no-frame is partly due to how data was selected. We chose articles that contained words related to no-vax, either in the headline or in the article. This resulted in many headlines not containing anything specific related to no-vax, while the no-vax content was actually included in the main text of the corresponding articles.  It is worth mentioning that prior to obtaining the results, we had expected that attribution of responsibility would be among the most prominent frames, since governments took many measures such as mandatory health pass requirements to access certain sites; we had also expected that the conflict frame would be prominent, since there were many demonstrations in Europe. In reality, however, these frames categories were not reflected as frequently at the headline level.  Regarding the analysis at the temporal level, it is clear that certain events were captured by the press, such as the start of vaccination or the mandatory vaccination passport.  Finally, the sentiment analysis of the different frames shows that the predominant tone in all of them is neutral or negative, with very similar trends between countries. This association between senti- ment analysis and frames has been discussed in previous literature [11] [43].  6 RESULTS: GPT-3.5 FOR FRAME  CLASSIFICATION OF HEADLINES (RQ2) Here, we present and discuss the results related to our second RQ.  6.1 Fine-tuning GPT-3.5 Table 4 shows the results of the 6-class classification task using 5-cross validation. Three models were used: GPT-3.5 and two BERT- based models. We observe that, on average, GPT-3.5 performs better than the BERT-based models. This is somehow expected as GPT- 3.5 is a much larger model. Overall, in the case of fine-tuning, the best performance for the six-class frame classification task is 72% accuracy, which is promising, with an improvement over previous models based on BERT. Yet, it should be noted that the performance differences are modest (2% improvement between GPT-3.5 and RoBERTa).  Table 4: Classification results for six-class frame classifica- tion and 5-fold cross validation  ACCURACY  BERT RoBERTa GPT3  0  0.68 0.70 0.75  1  0.69 0.72 0.70  2  0.72 0.72 0.72  3  0.64 0.67 0.71  4  0.70 0.71 0.71  AVERAGE  0.67 0.70 0.72  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  On the other hand, BERT is open-source, while GPT-3 has an economic cost as the use of the model is not free, which monetarily limits the number of experiments that can be performed with it, as well as the different configurations one can explore to improve performance. This is important because much of the improvement in performance requires empirical explorations of model parameters More specifically, the cost of an experiment for each of the folds has a cost of 4 dollars (at the time of writing this paper.) This represents a limitation in practice.  Furthermore, GPT-3 has a significant carbon footprint. Similarly, for prompt engineering (discussed in the next subsection), choosing the right prompt (i.e., the words that best define the task so that the model is able to perform adequately) is also based on trial and error. This also has an impact on carbon footprint. In connection with this topic, Strubell et al.[55] argue that improvements in the accu- racy of models depend on the availability of large computational resources, which involve large economic and environmental costs. A criticism has been made as \u2019the rich get richer\u2019, in the sense that not all research groups have sufficient infrastructure resources and access to funding needed to use these models and improve their performance. Also in relation to this analysis, the work of Bender et al. [4] evaluates the costs and risks of the use of large language models, stating that researchers should be aware of the impact that these models have on the environment, and assess whether the benefits outweigh the risks. The work in [4] provides a very telling example, where people living in the Maldives or Sudan are affected by floods and pay the environmental price of training English LLMs, when similar models have not been produced for languages like Dhivehi or Sudanese Arab. In short, there is a need to establish ways to use this technological development responsibly, and it all starts with being aware of the risks it presents.  6.2 Prompt-engineering with GPT-3.5 For each headline, we got the frame that the model considered the most likely, and we compared these GPT-3.5 inferences with the frames labeled by the annotators. The agreement between model and annotator was of 49%. Analyzing the results, and specifically looking at the cases where the annotator and GPT-3.5 disagreed, we discovered that according to the frame definitions, the model in some cases proposed a frame that indeed made sense. This ob- servation, together with our previous experience in the annotation process, where headlines could have more than one valid frame, led us to design a second post-hoc experiment. We took all the headlines where each of the two annotators had disagreed with GPT-3.5, and we asked the annotators to state whether they would agree (or not) with each GPT-inferred label for a given headline. It is important to emphasize that the annotators did not know the origin of that label, i.e., they did not know if it was the label they had originally assigned, or if it was a random one. In this way, we could quantify how GPT-3.5 worked according to valid arguments provided by the annotators. In this post-hoc experiment, the model agreed in 76% of cases with the annotators.  Looking at the results of the classification models, the 49% accu- racy of the prompt-engineering approach can be considered low, yet we consider that it is a valid avenue for further investigation, as in the second post-hoc analysis, we found that the model agrees  633\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  with human annotators in 76% of the cases. Clearly, framing in- volves aspects of subjectivity [42]. Much of what we do as people has a subjective component, influenced by how we feel or how we express opinions.  News reading is never fully objective, and the annotators en- gaged in the frame classification task, influenced by their personal state of mind, experience, and culture, may perceive information differently. Monarch affirms that \"for simple tasks, like binary labels on objective tasks, the statistics are fairly straightforward to decide which is the \u2018correct\u2019 label when different annotators disagree. But for subjective tasks, or even objective tasks with continuous data, there are no simple heuristics for deciding what the correct label should be\" [42].  Subjectivity is involved in both the generation and perception of information: the assumption that there is only one frame is com- plicated by the point of view of the reader. In the case of news, the information sender (the journalist) has an intention, but the receiver (the reader) plays a role and is influenced by it. In psychology, this is known as the lens model of interpersonal communication, where the sender has certain objectives, but the receiver can interpret or re-interpret what the sender wants to say, with more or less accuracy [26].  Following this discussion on subjectivity, the question arose as to what would happen if, instead of headlines, we used the complete article as a source of analysis. We wondered if longer text could make the frame labeling task clearer than when using headlines. Yet another possible hypothesis is that having to read longer texts could lead to the same subject being presented from different angles. Please recall that in the existing literature discussed in Section 2, both headlines and full articles have been used from frame analysis (see Table 1.) This remains as an issue for future work.  7 CONCLUSIONS In this paper, we first presented an analysis of human-generated news frames on the covid-19 no-vax movement in Europe, and then studied different approaches using large language models for automatic inference of frames. We conclude by answering the two research questions we posed:  RQ1: What are the main frames in the news headlines about the covid-19 anti-vaccine movement in 5 European countries? After annotating the headlines, we found that of the 1786 headlines, the predominant frame is human interest (45.3% of cases), which presents a news item with an emotional angle, putting a face to a problem or situation. We also found that a substantial proportion of headlines were annotated as not presenting any frame (40.2% of cases). Finally, the other frame types are found more infrequently. RQ2: Can prompt engineering be used for classification of head- lines according to frames? We first used fine-tuning of a number of language models, and found that GPT-3.5 produced classification ac- curacy of 72% on a six-frame classification task. This represented a modest 2% improvement over BERT-based models, at a significantly larger environmental cost. We then presented a new way of classi- fying frames using prompts. At the headline level, inferences made with GPT-3.5 reached 49% of agreement with human-generated frame labels. In many cases, the GPT-3.5 model inferred frame types that were considered as valid choices by human annotators,  and in an post-doc experiment, the human-machine agreement reached 76%. These results have opened several new directions for future work.  ACKNOWLEDGMENTS",
  "references": [
    "1] Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online media: Quantitative framing analysis on Detik. com\u2019s coverage of Covid-19. Jurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153\u2013170.  ",
    "2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. 2021. RAFT: A real-world few-shot text classification benchmark. arXiv preprint arXiv:2109.14076 (2021).  ",
    "3] David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe\u2019s Press (2022), 35\u201343.  Cover Covid-19 Vaccination News? A Five-Country Analysis. https://doi.org/10.1145/3512732.3533588  ",
    "4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? (2021), 610\u2013623.  ",
    "5] Santosh Kumar Biswal and Nikhil Kumar Gouda. 2020. Artificial intelligence in journalism: A boon or bane? In Optimization in machine learning and applications. Springer, 155\u2013167.  ",
    "6] Erik Bleich, Hannah Stonebraker, Hasher Nisar, and Rana Abdelhamid. 2015. Media portrayals of minorities: Muslims in British newspaper headlines, 2001\u2013 2012. Journal of Ethnic and Migration Studies 41, 6 (2015), 942\u2013962.  ",
    "7] Michael Bommarito and Daniel Martin Katz. 2022. GPT Takes the Bar Exam.  https://doi.org/10.48550/ARXIV.2212.14402  ",
    "8] Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe, Michel Dupagne, and Ching-Hua Chuan. 2019. Artificial intelligence and jour- nalism. Journalism & Mass Communication Quarterly 96, 3 (2019), 673\u2013695. ",
    "9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901.  ",
    "10] Bj\u00f6rn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, and Claes H De Vreese. 2014. Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis. Communication Methods and Measures 8, 3 (2014), 190\u2013206.  ",
    "11] Bjorn Burscher, Rens Vliegenthart, and Claes H de Vreese. 2016. Frames beyond words: Applying cluster and sentiment analysis to news coverage of the nuclear power issue. Social Science Computer Review 34, 5 (2016), 530\u2013545.  ",
    "12] Dallas Card, Amber Boydstun, Justin Gross, Philip Resnik, and Noah Smith. 2015. The Media Frames Corpus: Annotations of Frames Across Issues. 2 (01 2015), 438\u2013444. https://doi.org/10.3115/v1/P15-2072  ",
    "13] Daniel Catalan-Matamoros and Carlos El\u00edas. 2020. Vaccine hesitancy in the age of coronavirus and fake news: analysis of journalistic sources in the Spanish quality press. International Journal of Environmental Research and Public Health 17, 21 (2020), 8136.  ",
    "14] Daniel Catal\u00e1n-Matamoros and Carmen Pe\u00f1afiel-Saiz. 2019. Media and mistrust of vaccines: a content analysis of press headlines. Revista latina de comunicaci\u00f3n social 74 (2019), 786\u2013802.  ",
    "15] Mark Coddington. 2015. Clarifying journalism\u2019s quantitative turn: A typology for evaluating data journalism, computational journalism, and computer-assisted reporting. Digital journalism 3, 3 (2015), 331\u2013348.  ",
    "16] Stephen D Cooper. 2010. The oppositional framing of bloggers. In Doing News  Framing Analysis. Routledge, 151\u2013172.  ",
    "17] Robert Dale. 2021. GPT-3: What\u2019s it good for? Natural Language Engineering 27,  1 (2021), 113\u2013118.  ",
    "18] Astrid Dirikx and Dave Gelders. 2010.  To frame is to explain: A de- ductive frame-analysis of Dutch and French climate change coverage dur- ing the annual UN Conferences of the Parties. Public Understanding of Science 19, 6 (2010), 732\u2013742. https://doi.org/10.1177/0963662509352044 arXiv:https://doi.org/10.1177/0963662509352044 PMID: 21560546.  ",
    "19] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A deductive frame- analysis of Dutch and French climate change coverage during the annual UN Conferences of the Parties. Public understanding of science 19, 6 (2010), 732\u2013742.  634\fFraming the News: From Human Perception to Large Language Model Inferences  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  ",
    "20] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2020. Gsum: A general framework for guided neural abstractive summarization. arXiv preprint arXiv:2010.08014 (2020).  ",
    "21] Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA Gesondheid (Online) 27 (2022), 1\u20138.  ",
    "22] Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021). ",
    "23] Robert M Entman. 1993. Framing: Towards clarification of a fractured paradigm.  McQuail\u2019s reader in mass communication theory 390 (1993), 397.  ",
    "24] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723 (2020). ",
    "25] Piyush Ghasiya and Koji Okamura. 2021. Investigating COVID-19 news across four nations: a topic modeling and sentiment analysis approach. Ieee Access 9 (2021), 36645\u201336656.  ",
    "26] Robert Gifford. 1994. A Lens-Mapping Framework for Understanding the En- coding and Decoding of Interpersonal Dispositions in Nonverbal Behavior. Journal of Personality and Social Psychology 66 (02 1994), 398\u2013412. https: //doi.org/10.1037//0022-3514.66.2.398  ",
    "27] Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based transformer architectures for long document summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 1792\u20131810.  ",
    "28] Anushka Gupta, Diksha Chugh, Rahul Katarya, et al. 2022. Automated news summarization using transformers. In Sustainable Advanced Computing. Springer, 249\u2013259.  ",
    "29] Alfred Hermida and Mary Lynn Young. 2017. Finding the data unicorn: A hierar- chy of hybridity in data and computational journalism. Digital Journalism 5, 2 (2017), 159\u2013176.  ",
    "30] Karoliina Isoaho, Daria Gritsenko, and Eetu M\u00e4kel\u00e4. 2021. Topic modeling and text analysis for qualitative policy research. Policy Studies Journal 49, 1 (2021), 300\u2013324.  ",
    "31] Carina Jacobi, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative analysis of large amounts of journalistic texts using topic modelling. Digital journalism 4, 1 (2016), 89\u2013106.  ",
    "32] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computa- tional Linguistics 8 (2020), 423\u2013438.  ",
    "33] Shima Khanehzar, Andrew Turpin, and Gosia Miko\u0142ajczak. 2019. Modeling  ",
    "46] Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with  mixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021).  ",
    "47] Rabindra Lamsal. 2021. Sentiment Analysis of English Tweets with BERTsent. https://huggingface.co/rabindralamsal/finetuned-bertweet-sentiment-analysis. ",
    "48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.  ",
    "49] Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, and Ahad Ali. 2022. Fake News Classification using transformer based enhanced LSTM and BERT. International Journal of Cognitive Computing in Engineering 3 (2022), 98\u2013105. https://doi.org/10.1016/j.ijcce.2022.03.003  ",
    "50] Frida V Rodelo. 2021. Framing of the Covid-19 pandemic and its organizational  predictors. Cuadernos. info 50 (2021), 91\u2013112.  ",
    "51] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).  ",
    "52] Holli Semetko and Patti Valkenburg. 2000. Framing European Politics: A Content Analysis of Press and Television News. Journal of Communication 50 (06 2000), 93 \u2013 109. https://doi.org/10.1111/j.1460-2466.2000.tb02843.x  ",
    "53] Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Em- manouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. arXiv preprint arXiv:2104.08768 (2021).  ",
    "54] Efstathios Sidiropoulos and Andreas Veglis. 2017. Computer Supported Collab- orative Work trends on Media Organizations: Mixing Qualitative and Quan- titative Approaches. Studies in Media and Communication 5 (04 2017), 63. https://doi.org/10.11114/smc.v5i1.2279  ",
    "55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019). ",
    "56] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503 (2021).  ",
    "57] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning.  arXiv preprint arXiv:1806.02847 (2018).  ",
    "58] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems 34 (2021), 200\u2013212.  ",
    "59] Sandra A Vannoy and Prashant Palvia. 2010. The social influence model of  technology adoption. Commun. ACM 53, 6 (2010), 149\u2013153.  ",
    "60] Tuukka Yl\u00e4-Anttila, Veikko Eranti, and Anna Kukkonen. 2022. Topic modeling for frame analysis: A study of media debates on climate change in India and USA. Global Media and Communication 18, 1 (2022), 91\u2013112.  Political Framing Across Policy Issues and Contexts. In ALTA.  ",
    "34] Jeesun Kim and Wayne Wanta. 2018. News framing of the US immigration debate during election years: Focus on generic frames. The Communication Review 21, 2 (2018), 89\u2013115.  ",
    "35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi- hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).  ",
    "36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. (2021). https://doi.org/10. 48550/ARXIV.2107.13586  ",
    "37] Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and Derry Tanti Wijaya. 2019. Detecting frames in news headlines and its application to analyzing news framing trends surrounding US gun violence. In Proceedings of the 23rd conference on computational natural language learning (CoNLL).  ",
    "38] J\u00f6rg Matthes and Matthias Kohring. 2008. The Content Analysis of Media Frames: Toward Improving Reliability and Validity. Journal of Communication 58 (06 2008). https://doi.org/10.1111/j.1460-2466.2008.00384.x  ",
    "39] Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, and David E Losada. 2022. Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI. In Proceedings of the 4th Conference on Conversational User Interfaces. 1\u20136.  ",
    "40] Stuart E Middleton, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Social computing for verifying social media content in breaking news. IEEE Internet Computing 22, 2 (2018), 83\u201389.  ",
    "41] Marko Milosavljevi\u0107 and Igor Vobi\u010d. 2021.  \u2018Our task is to demystify fears\u2019: Analysing newsroom management of automation in journalism. Journalism 22, 9 (2021), 2203\u20132221.  ",
    "42] R. Monarch. 2021. Human-in-the-Loop Machine Learning: Active Learning and Annotation for Human-centered AI. Manning. https://books.google.ch/books? id=LCh0zQEACAAJ  ",
    "43] Tom Nicholls and Pepper D Culpepper. 2021. Computational identification of media frames: Strengths, weaknesses, and opportunities. Political Communication 38, 1-2 (2021), 159\u2013181.  ",
    "44] Zhongdang Pan and Gerald M Kosicki. 1993. Framing analysis: An approach to  news discourse. Political communication 10, 1 (1993), 55\u201375.  ",
    "45] Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative  language models. arXiv preprint arXiv:1912.10165 (2019).  635\f"
  ],
  "url": "https://drive.google.com/uc?id=1vmiWI3NUHORkl-g7D9WA8Xh7YbTOHfmB",
  "date": "2024-01-09 21:23:23",
  "is_published": false
}
{
  "title": "Framing the News: From Human Perception to Large Language Model Inferences",
  "authors": [
    "David Alonso del Barrio",
    "Idiap Research Institute",
    "Daniel Gatica-Perez",
    "Idiap Research Institute and EPFL"
  ],
  "institutions": [
    "Switzerland",
    "Switzerland"
  ],
  "abstract": "Identifying the frames of news is important to understand the arti- cles\u2019 vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of jour- nalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to under- stand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No- Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Nat- ural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT- 3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.",
  "keywords": [
    "Covid-19 no-vax",
    " news framing",
    " GPT-3",
    " prompt-engineering",
    " trans-",
    "formers",
    " large language models"
  ],
  "article": "In recent years, there has been a proliferation in the use of concepts such as data journalism, computational journalism, and computer- assisted reporting [15] [29], which all share the vision of bridging journalism and technology. The progress made in NLP has been gradually integrated into the journalistic field [5][8][54]. More specifically, machine learning models based on transformers have been integrated in the media sector in different tasks [41] such as the creation of headlines with generative languages models [17], summarization of news articles [28][27], false news detection [49], and topic modeling and sentiment analysis [25]. The development of large language models such as GPT-3 [9], BLOOM [51] or ChatGPT show a clear trend towards human-machine interaction becoming easier and more intuitive, opening up a wide range of research possibilities. At the same time, the use of these models is also associated with a lack of transparency regarding how these models work, but efforts are being made to bring some transparency to where they cannot [35]. Based on the premises that these models open up a wide range of research directions [7], and that at the same time (and needless to say) they are not the solution to all problems, can be potentially useful, while acknowledging and systematically documenting their limitations [56]. More specifically, the aim of this work is to analyze the performance of GPT-3.5 for a specific use case, namely the analysis of frames in news, from an empirical point of view, with the objective of shedding light on a potential use of generative models in journalistic tasks.  Frame analysis is a concept from journalism, which consists of studying the way in which news stories are presented on an issue, and what aspects are emphasized: Is a merely informative vision given in an article? Or is it intended to leave a moral lesson? Is a news article being presented from an economic point of view? Or from a more human, emotional angle? The examples above correspond to different frames with which an article can be written. The concept of news framing has been studied in computing as a step beyond topic modeling and sentiment analysis, and for this purpose, in recent years, pre-trained language models have been used for fine-tuning the classification process of these frames [60] [10], but the emergence of generative models opens the possibility of doing prompt-engineering of these classification tasks, instead of the fine-tuning approach investigated so far.  Our work aims to address this research gap by posing the fol-  lowing research questions:  RQ1: What are the main frames in the news headlines about the anti-vaccine movement, as reported in newspapers across 5 European countries?  627\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  RQ2: Can prompt engineering be used for classification of head-  lines according to frames?  By addressing the above research questions, our work makes the  following contributions:  Contribution 1. We implemented a process to do human an- notation of the main frame of 1786 headlines of articles about the Covid-19 no-vax movement, as reported in 19 newspapers from 5 European countries (France, Italy, Spain, Switzerland and United Kingdom.) At the headline level, we found that the predominant frame was human interest, where this frame corresponds to a per- sonification of an event, either through a statement by a person, or the explanation of a specific event that happened to a person. Furthermore, we found a large number of headlines annotated as containing no frame, as they simply present information without entering into evaluations. We also found that for all the countries involved, the distribution of frame types was very similar, i.e., hu- man interest and no frame are the two predominant frames. Finally, the generated annotations allowed to subsequently study the per- formance of a large language model.  Contribution 2. We studied the performance of GPT-3.5 on the task of frame classification of headlines. In addition to using the fine-tuning approach from previous literature, we propose an alternative approach for frame classification that requires no labeled data for training, namely prompt-engineering using GPT-3.5. The results show that fine-tuning with GPT-3.5 produces 72% accuracy (slightly higher than other smaller models), and that the prompt- engineering approach results in lower performance (49% accuracy.) Our analysis also shows that the subjectivity of the human labeling task has an effect on the obtained accufracy.  The paper is organized as follows. In Section 2, we discuss related work. In Section 3, we describe the news dataset. In Section 4, we describe the methodology for both human labeling and machine classification of news frames. We present and discuss results for RQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide conclusions in Section 7.  2 RELATED WORK Framing has been a concept widely studied in journalism, with a definition that is rooted in the study of this domain [23]: \u201cTo frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a par- ticular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.\u201d  For frame recognition, there are two main approaches: the induc- tive approach [16], where one can extract the frames after reading the article, and the deductive approach [38], where a predefined list of frames exists and the goal is to interpret if any of them ap- pears in the article. In the deductive case, there are generic frames and subject-specific frames, and the way to detect them typically involves reading and identifying one frame at a time, or through answers to yes/no questions that represent the frames. Semetko et al. [52] used 5 types of generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) based on previous literature, and they defined a list of 20 yes/no questions to detect frames in articles. For instance, the questions about morality are the following: \"Does the story contain any moral  message? Does the story make reference to morality, God, and other religious tenets? Does the story offer specific social prescriptions about how to behave?\", and so on for each of the frame types. This categorization of frames has been used in various topics such as climate change [18] [19], vaccine hesitance [13], or immigration [34].  We now compare the two approaches on a common topic, such as Covid-19. Ebrahim et al. [21] followed an inductive approach in which the frames were not predefined but emerged from the text (e.g., deadly spread, stay home, what if, the cost of Covid-19) using headlines as the unit of analysis. In contrast, the deductive approach has studied very different labels. El-Behary et al. [22] followed the method of yes/no questions, but in addition to the 5 generic frames presented before, they also used blame frame and fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5 generic frames with yes/no questions, while Catal\u00e1n-Matamoros et al. [14] used the 5 frames and read the headline and subheadline to decide the main frame. Table 1 summarizes some of the the existing approaches. This previous work showed how frame labels can be different, and also that frame analysis has been done at both headline and article levels. These two approaches (inductive and deductive) that originated in journalism have since been replicated in the computing literature.  We decided to follow the deductive approach because a prede- fined list of frames allows to compare among topics, countries, previous literature, and also because they represent a fixed list of labels for machine classification models. Furthermore, the induc- tive approach tends to be more specific to a topic, and from the computing viewpoint, past work has tried to justify topic modeling as a technique to extract frames from articles.  Yl\u00e4-Antitila et al. [60] proposed topic modeling as a frame ex- traction technique. They argued that topics can be interpreted as frames if three requirements are met: frames are operationalized as connections between concepts; subject-specific data is selected; and topics are adequately validated as frames, for which they suggested a practical procedure. This approach was based on the choice of a specific topic (e.g., climate change) and the use of Latent Dirichlet Allocation (LDA) as a technique to extract a number of subtopics. In a second phase, a qualitative study of the top 10 words of each subtopic was performed, and the different subtopics were elimi- nated or grouped, reducing the number and establishing a tentative description. In a third phase, the top 10 articles belonging to that frame/topic were taken, and if the description of the topic fitted at least 8 of the 10 articles, that topic/frame remained. The frames found in this article were: green growth, emission cuts, negotiations and treaties, environmental risk, cost of carbon emissions, Chinese emissions, economics of energy production, climate change, en- vironmental activism, North-South burden sharing, state leaders negotiating, and citizen participation.  From Entman\u2019s definition of frame [23], it seems that the deduc- tive approach is more refined than the inductive approach (which seems to resemble the detection of sub-themes.) For example, with regard to climate change, there are stories on how people have been affected by climate change from an emotional point of view, thus personalizing the problem. In this case, we could categorize the corresponding frame as human interest, as the writer of the article is selecting \"some aspects of a perceived reality and make them  628\fFraming the News: From Human Perception to Large Language Model Inferences  more salient\". The language subtleties with which news articles are presented cannot be captured with basic topic modeling.  Isoaho et al.[30] held the position that while the benefits of scale and scope in topic modeling were clear, there were also a number of problems, namely that topic outputs do not correspond to the methodological definition of frames, and thus topic modeling remained an incomplete method for frame analysis. Topic modeling, in the practice of journalistic research, is a useful technique to deal with the large datasets that are available, yet is often not enough to do more thorough analyses [31]. In our work, we clearly notice that frame analysis is not topic modeling. For example, two documents could be about the same topic, say Covid-19 vaccination, but one article could emphasize the number of deaths after vaccination, while the other emphasized the role of the vaccine as a solution to the epidemic.  We also consider that the larger the number of possible frame types, the more likely it is to end up doing topic modeling instead of frame analysis. Using a deductive approach, Dallas et al. [12] created a dataset with articles about polemic topics such as immigration, same sex marriage, or smoking, and they defined 15 types of frames: \"economic, capacity and resources, morality, fairness and equality, legality, constitutionality and jurisprudence, policy prescription and evaluation, crime and punishment, security and defense, health and safety, quality of life, cultural identity, political, external regulation and reputation, other\". In this case, they authors did not use a list of questions. Instead, for each article, annotators were asked to identify any of the 15 framing dimensions present in the article and to label text blurbs that cued them (based on the definitions of each of the frame dimensions) and decide the main frame of each article. In our case, we followed the idea of detecting the main frame by reading the text instead of answering questions, but instead of using the 15 frames proposed in [12] , we used the 5 generic frames proposed in [52].  A final decision in our work was the type of text to analyze, whether headlines or whole article. For this decision, the chosen classification method was also going to be important. For example, Khanehzar et al. [33] used traditional approaches such as SVMs as baseline, and demonstrated the improvement in frame classifica- tion with the use of pre-trained languages models such as BERT, RoBERTa and XLNet, following a fine-tuning approach, setting as input text a maximum of 256 tokens (although the maximum number of input tokens in these models is 512 tokens.) Liu et al. [37] classified news headlines about the gun problem in the United States, arguing for the choice of headlines as a unit of analysis based on previous journalism literature [6], [44], that advocated for the importance and influence of headlines on readers and the subsequent perception of articles. From a computational viewpoint, using headlines is also an advantage, since you avoid the 512 token limitation in BERT-based models. Therefore, we decided to work with headlines about a controversial issue, namely the Covid-19 no-vax movement.  Continuing with the question of the methods used for classi- fication, much work has been developed in prompt engineering, especially since the release of GPT-3. Liu et al.[36] presented a good overview of the work done on this new NLP paradigm, not only explaining the concept of prompt engineering, but also the differ- ent strategies that can be followed both in the design of prompts,  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  Table 1: Summary of deductive approaches for frame analysis  Ref Frames  Goal  Technique  To label frames of full articles  [12] 15 generic frames: \"Economic\", \"Capac- ity and resources\", \"Morality\", \"Fair- ness and equality\", \"Legality, constitu- tionality and jurisprudence\", \"Policy prescription and evaluation\", \"Crime and punishment\", \"Security and de- fense\", \"Health and safety\", \"Quality of life\", \"Cultural identity\", \"Public opin- ion\", \"Political\", \"External regulation and reputation\", \"Other\".  [33] 15 generic frames  Classification  [52] 5 generic frames: \"human interest\", \"conflict\", \"morality\", \"attribution of responsibility\", and \"economic conse- quences\".  \u201cSociety/Culture\u201d, ,  [37] 9 specific frames:\u201cPolitics\u201d, \u201cPublic opinion\u201d, and \u201c2nd \u201cEconomic consequences\u201d Amendment\u201d (Gun Rights), \u201cGun control/regulation\u201d, \u201cMental health\u201d, \u201cSchool/Public space safety\u201d, and \u201cRace/Ethnicity\u201d.  [22] 5 generic frames + blame frame and  fear frame 5 generic frames  [1]  To label frames of full articles  To label frames of full articles/ Classification  To label frames of full articles To label frames of full articles  [50] 5 generic frames + pandemic frames  [14] 5 generic frames, journalistic role and  pandemic frames  To label frames of full articles To label frames of full articles  the Reading article, full the annotator defines the main frame  BERT based models Yes/No ques- tions.  the Reading full article, the annotator the defines main frame. BERT based models Yes/No ques- tions. Reading the article, full the annotator defines the main frame. Yes/No ques- tions. Reading the headline and subheadline, the annotator defines the main frame.  Number samples  of  20000 articles  12000 articles  2600 articles and 1522 tv news stories  2990 headlines  1170 articles  6713 articles  2742 articles  131 headlines + subheadlines  the potential applications, and the challenges to face when using this approach. Prompt engineering applications include knowledge probing [46], information extraction [53], NLP reasoning [57], ques- tion answering [32], text generation [20], multi-modal learning [58], and text classification [24], the latter being the prompt-engineering use case in our work. Puri et al.[45] presented a very interesting idea that we apply to our classification task. This consists of pro- viding the language model with natural language descriptions of classification tasks as input, and training it to generate the correct answer in natural language via a language modeling objective. It is a zero-shot learning approach, in which no examples are used to explain the task to the model. Radford et al. [48] demonstrated that language models can learn tasks without any explicit supervision. We have followed this approach to find an alternative way to do frame analysis.  As mentioned before, the emergence of giant models like GPT-3, BLOOM, and ChatGPT are a very active research topic. To the best of our knowledge, on one hand our work extends the computational analysis of news related to the covid-19 no-vax movement, which illustrates the influence of the press on the ways societies think about relevant issues [40], [59], and on the other hand it adds to the literature of human-machine interaction, regarding the design of GPT-3 prompts for classification tasks [39], [2].  3 DATA: EUROPEAN COVID-19 NEWS  DATASET  We used part of the European Covid-19 News dataset collected in our recent work [3]. This dataset contains 51320 articles on Covid- 19 vaccination from 19 newspapers from 5 different countries: Italy,  629\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  France, Spain, Switzerland and UK. The articles cover a time period of 22 months, from January 2020 to October 2021. All content was translated into English to be able to work in a common language. The dataset was used for various analyses, such as name entity recognition, sentiment analysis, and subtopic modeling, to under- stand how Covid-19 vaccination was reported in Europe through the print media (in digital format.) The subtopic modeling analysis revealed a subsample of articles on the no-vax movement, which is the one we have used in this paper. We took the headlines of the articles associated with the no-vax movement, selecting all articles containing any of the keywords in Table 2 in the headline or in the main text. This corresponds to a total of 1786 headlines.  Table 2: Keywords used to identify no-vax articles  NO VAX TOPIC  \"anti-vaxxers\", \"anti-vaccine\", \"anti-vaxx\", \"anti-corona\", \"no-vax\", \"no vax\",\"anti-vaccin\"  Keywords  In Table 3, we show the number of headlines per country and newspaper. France is the country with the most no-vax articles in the corpus, with 523 articles, followed by Italy with 508. However, note that there are 6 newspapers from France, while only 2 from Italy. Corriere della Sera is the newspaper that dealt most frequently with the subject (429 articles), while The Telegraph is the second one (206 articles). The total number of articles normalized by the number of newspapers per country is also shown in the last column of the Table. Using these normalized values, the ranking is Italy, UK, France, Switzerland, and Spain.  Table 3: Number of headlines by newspaper and country  COUNTRY  NEWSPAPER  HEADLINES  TOTAL (NORM. TOTAL)  FRANCE  ITALY  SPAIN  SWITZERLAND  UNITED KINGDOM  La Croix Le Monde Les Echos Liberation Lyon Capitale Ouest France Corriere della Sera Il Sole 24 Ore 20 minutos ABC El Diario El Mundo El Espa\u00f1ol La Vanguardia 24 heures La Libert\u00e9 Le Temps The Irish News The Telegraph  94 125 49 97 8 150 429 79 27 50 32 77 22 95 97 22 111 16 206  523 (87.1)  508 (254.0)  303 (50.5)  230 (76.6)  222 (111.0)  1786  4 METHODOLOGY 4.1 Human labeling of news frames To carry out the labeling of the frames in our corpus of headlines, we first designed a codebook, which contained the definitions of each of the frame types and a couple of examples of each type, as well as a definition of the corpus subject matter and definitions of the concept of frame analysis, so that the annotators could understand the task to be performed. The codebook follows the proposed by  [52] with 5 generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) plus one additional \u2019no-frame\u2019 category. Two researchers were engaged to annotate a sample of the collected newspaper articles following a three-phase training procedure.  In the first phase, annotators had to read the codebook and get familiar with the task. In the second phase, they were asked to identify the main frame in the same subset of 50 headlines. At the end of the second phase, the intercoder reliability (ICR) was 0.58 between the 2 annotators. We analyzed those cases where there were discrepancies, and observed that in some cases, there was not a unique main frame, because both annotators had valid arguments to select one of the frames. In other cases, the discrepancies were due to slight misunderstanding of the definitions. In the third phase, the annotators coded again 50 headlines, and the ICR increased to was 0.66. We realized that the possibility of having two frames remained. They discussed the cases in which they had disagreed, and if the other person\u2019s arguments were considered valid, it could be said that there were two frames. After this three-phase training procedure, annotators were ready to annotate the dataset independently. We divided the dataset into two equal parts, and each person annotated 893 headlines.  4.2 Fine-tuning GPT-3.5 and BERT-based  models  With the annotated dataset, we investigated two NLP approaches: the first one involves fine-tuning a pre-trained model; the second one is prompt engineering. Pre-trained language models have been  Figure 1: Pre-train, fine-tune, prompt  trained with large text strings based on two unsupervised tasks, next sentence prediction and masked language model. Figure 1 summarizes these techniques.  In the first approach, a model with a fixed architecture is pre- trained as a language model (LM), predicting the likelihood of the observed textual data. This can be done due to the availability of large, raw text data needed to train LMs. This learning process can produce general purpose features of the modeled language. The learning process produces robust, general-purpose features of the language being modeled. The above pre-trained LM is then adapted to different downstream tasks, by introducing additional parameters and adjusting them using task-specific objective functions. In this approach, the focus was primarily on goal engineering, designing the training targets used in both the pre-training and the fine-tuning stages [36].  630\fFraming the News: From Human Perception to Large Language Model Inferences  We present an example to illustrate the idea. Imagine that the task is sentiment analysis, and we have a dataset with sentences and their associated sentiment, and a pre-trained model, which is a saved neural network trained with a much larger dataset. For that pre-trained model to address the target task, we unfreeze a few of the top layers of the saved model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows to \"fine-tune\" the higher-order feature representations in the base model to make them more relevant for the sentiment analysis task. In this way, instead of having to obtain a very large dataset with target labels to train a model, we can reuse the pre- trained model and use a much smaller train dataset. We use a part of our dataset as examples for the model to learn the task, while the other part of the dataset is used to evaluate model performance. Previous works related to frame classification in the computing literature have used fine-tuning, BERT-based models. In our work, we have done the same as a baseline, but we aimed to go one step further and also produce results using fine-tuning of GPT-3.5.  4.3 Prompt-engineering with GPT-3.5 Model fine-tuning has been widely used, but with the emergence of generative models such as GPT-3, another way to approach classification tasks has appeared. The idea is to use the pre-trained model directly and convert the task to be performed into a format as close as possible to the tasks for which it has been pre-trained. That is, if the model has been pre-trained from next word prediction as in the case of GPT-3, classification can be done by defining a prompt, where the input to the model is an incomplete sentence, and the model must complete it with a word or several words, just as it has been trained. This avoids having to use part of the already labeled dataset to teach the task to be performed to the model, and a previous labeling is not needed [36].  In this approach, instead of adapting pre-trained LMs to down- stream tasks via objective engineering, downstream tasks are re- formulated to look more like those solved during the original LM training with the help of a textual prompt. For example, when recog- nizing the emotion of a social media post, \u201cI missed the bus today.\u201d, we may continue with a prompt \u201cI felt so _\u201d, and ask the LM to fill the blank with an emotion-bearing word. Or if we choose the prompt \u201cEnglish: I missed the bus today. French: _\u201d), an LM may be able to fill in the blank with a French translation. In this way, by selecting the appropriate prompts, we can influence the model behavior so that the pre-trained LM itself can be used to predict the desired output, even without any additional task-specific training [36].  We use this emerging NLP approach to classify frames at headline level. We are not aware of previous uses of this strategy to classify frames as we propose here. The idea is the following. Prompt engi- neering consists of giving a prompt to the model, and understands that prompt as an incomplete sentence. To do prompt engineer- ing with our dataset, we needed to define an appropriate prompt that would produce the headline frames as output. We defined sev- eral experiments with the Playground of GPT-3, in order to find the best prompt for our task. In our initial experiments, we fol- lowed existing approaches in prompt engineering to do sentiment analysis, where the individual answer was an adjective, and this  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  adjective was matched with a sentiment. In a similar fashion, we decided to build a thesaurus of adjectives that define each of the frames. For instance, the human interest frame could be \u2019interest- ing\u2019, \u2019emotional\u2019, \u2019personal\u2019, \u2019human\u2019. The conflict frame could be: \u2019conflictive\u2019, \u2019bellicose\u2019, \u2019troublesome\u2019, \u2019rowdy\u2019, \u2019quarrelsome\u2019, \u2019trou- blemaker\u2019, \u2019agitator\u2019, etc. After the list of adjectives was defined, we needed to define the prompt in order to get, as an answer, one of the adjectives in our thesaurus to match them with the frame. We used the GPT-3 playground using the headline as input and asking for the frame as output, but the strategy did not work. In our final experiment, instead of giving the headline as input, we gave the definitions of each type of frame plus the headline, and we asked the model to choose between the different types of frames as output. In this way, the output of the model was directly one of the frames, and we avoided the step of matching adjectives with frames. An example is shown in Figure 2.  Figure 2: GPT-3.5 for frame inference: input and output  For the GPT-3 configuration 1, there are 3 main concepts:  \u2022 TEMPERATURE [0-1]. This parameter controls randomness,  lowering it results in less random completions.  \u2022 TOP_P [0-1]. This parameter controls diversity via nucleus  sampling.  \u2022 MAX_TOKENS[1-4000]. This parameter indicates the maxi-  mum number of tokens to generate,  \u2022 MODEL. GPT-3 offer four main models with different levels of power, suitable for different tasks. Davinci is the most capable model, and Ada is the fastest.  After testing with the GPT-3 playground and varying different hyper-parameters to assess performance, we set the temperature to 0, since the higher the temperature the more random the response. Furthermore, the Top-p parameter was set to 1, as it would likely get a set of the most likely words for the model to choose from. The maximum number of tokens was set to 2; in this way, the model is asked to choose between one of the responses. As a model, we used the one with the best performance at the time of experimental design, which was TEXT-DAVINCI-003, recognized as GPT 3.5.  5 RESULTS: HUMAN LABELING OF FRAMES  IN NO-VAX NEWS HEADLINES (RQ1)  In this section, we present and discuss the results of the analysis related to our first RQ.  Figure 3 shows the distribution of frames per country at headline level, with human interest and no-frame being the predominant   631\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  ones. Attribution of responsibility is the third one except in Switzer- land, where the corresponding frame is conflict. Finally, morality and economic are the least represented in the dataset for every country.  Figure 4: Non-normalized monthly distribution of frames.  detail the negative and positive sentiment of each frame category, we observed a few trends:  \u2022 Attribution of responsibility: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Switzerland, and the United Kingdom. \u2022 Conflict: Negative sentiment represents 20-35% of the cases. \u2022 Economic: Predominantly neutral, with only negative tone in Italy and UK (in the latter case, all headlines with this frame were considered negative.)  \u2022 Human interest: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Spain, and Switzerland.  \u2022 Morality: Predominantly neutral, with negative tone in Italy,  Switzerland, and the United Kingdom, \u2022 No frame: 20-30% of negative content.  Figure 5: Sentiment of headline by frame and by country  Figure 3: Non-normalized distribution of frames per country  The monthly distribution of frames aggregated for all countries is shown in Fig. 4. We can see two big peaks, the first one in January 2021 and the second one in August 2021. In all countries, the vac- cination process started at the end of December 2020, so it makes sense that the no-vax movement started to be more predominant in the news in January 2021. Human interest is the most predominant frame. Manual inspection shows that this is because the headlines are about personal cases of people who are pro- or anti- vaccine. Attribution of responsibility is also present. Manual inspection in- dicates that local politicians and health authorities had to make decisions about who could be vaccinated at the beginning of the process. The second peak at the end of summer 2021 coincided with the health pass (also called Covid passport in some countries), and we can observe a peak in the curve corresponding to the con- flict frame, reflecting the demonstrations against the measure of mandatory health passes taken by country governments.  In Figure 5, we compare the sentiment per frame and per country, to understand if there were any major differences. The sentiment analysis labels were obtained using BERT-sent from the Hugging Face package [47], used in our previous work (please refer to our original analysis in [3] for details.) We normalized the results be- tween 0 and 1 to compare frames between countries. We see that the sentiment is predominantly neutral (in blue). Examining in more  632\fFraming the News: From Human Perception to Large Language Model Inferences  Regarding the results of the annotation process, the fact that the distribution of the 6 frame types is relatively similar between coun- tries suggests that the anti-vaccine movement issue was treated in a similar way in these countries. The fact that human interest is the most dominant frame indicates that this issue was treated from a more human and emotional approach, with headlines about personal experiences, celebrities giving their opinion about vacci- nation, and politicians defending vaccine policies. Moreover, the reason for many headlines being classified as no-frame is partly due to how data was selected. We chose articles that contained words related to no-vax, either in the headline or in the article. This resulted in many headlines not containing anything specific related to no-vax, while the no-vax content was actually included in the main text of the corresponding articles.  It is worth mentioning that prior to obtaining the results, we had expected that attribution of responsibility would be among the most prominent frames, since governments took many measures such as mandatory health pass requirements to access certain sites; we had also expected that the conflict frame would be prominent, since there were many demonstrations in Europe. In reality, however, these frames categories were not reflected as frequently at the headline level.  Regarding the analysis at the temporal level, it is clear that certain events were captured by the press, such as the start of vaccination or the mandatory vaccination passport.  Finally, the sentiment analysis of the different frames shows that the predominant tone in all of them is neutral or negative, with very similar trends between countries. This association between senti- ment analysis and frames has been discussed in previous literature [11] [43].  6 RESULTS: GPT-3.5 FOR FRAME  CLASSIFICATION OF HEADLINES (RQ2) Here, we present and discuss the results related to our second RQ.  6.1 Fine-tuning GPT-3.5 Table 4 shows the results of the 6-class classification task using 5-cross validation. Three models were used: GPT-3.5 and two BERT- based models. We observe that, on average, GPT-3.5 performs better than the BERT-based models. This is somehow expected as GPT- 3.5 is a much larger model. Overall, in the case of fine-tuning, the best performance for the six-class frame classification task is 72% accuracy, which is promising, with an improvement over previous models based on BERT. Yet, it should be noted that the performance differences are modest (2% improvement between GPT-3.5 and RoBERTa).  Table 4: Classification results for six-class frame classifica- tion and 5-fold cross validation  ACCURACY  BERT RoBERTa GPT3  0  0.68 0.70 0.75  1  0.69 0.72 0.70  2  0.72 0.72 0.72  3  0.64 0.67 0.71  4  0.70 0.71 0.71  AVERAGE  0.67 0.70 0.72  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  On the other hand, BERT is open-source, while GPT-3 has an economic cost as the use of the model is not free, which monetarily limits the number of experiments that can be performed with it, as well as the different configurations one can explore to improve performance. This is important because much of the improvement in performance requires empirical explorations of model parameters More specifically, the cost of an experiment for each of the folds has a cost of 4 dollars (at the time of writing this paper.) This represents a limitation in practice.  Furthermore, GPT-3 has a significant carbon footprint. Similarly, for prompt engineering (discussed in the next subsection), choosing the right prompt (i.e., the words that best define the task so that the model is able to perform adequately) is also based on trial and error. This also has an impact on carbon footprint. In connection with this topic, Strubell et al.[55] argue that improvements in the accu- racy of models depend on the availability of large computational resources, which involve large economic and environmental costs. A criticism has been made as \u2019the rich get richer\u2019, in the sense that not all research groups have sufficient infrastructure resources and access to funding needed to use these models and improve their performance. Also in relation to this analysis, the work of Bender et al. [4] evaluates the costs and risks of the use of large language models, stating that researchers should be aware of the impact that these models have on the environment, and assess whether the benefits outweigh the risks. The work in [4] provides a very telling example, where people living in the Maldives or Sudan are affected by floods and pay the environmental price of training English LLMs, when similar models have not been produced for languages like Dhivehi or Sudanese Arab. In short, there is a need to establish ways to use this technological development responsibly, and it all starts with being aware of the risks it presents.  6.2 Prompt-engineering with GPT-3.5 For each headline, we got the frame that the model considered the most likely, and we compared these GPT-3.5 inferences with the frames labeled by the annotators. The agreement between model and annotator was of 49%. Analyzing the results, and specifically looking at the cases where the annotator and GPT-3.5 disagreed, we discovered that according to the frame definitions, the model in some cases proposed a frame that indeed made sense. This ob- servation, together with our previous experience in the annotation process, where headlines could have more than one valid frame, led us to design a second post-hoc experiment. We took all the headlines where each of the two annotators had disagreed with GPT-3.5, and we asked the annotators to state whether they would agree (or not) with each GPT-inferred label for a given headline. It is important to emphasize that the annotators did not know the origin of that label, i.e., they did not know if it was the label they had originally assigned, or if it was a random one. In this way, we could quantify how GPT-3.5 worked according to valid arguments provided by the annotators. In this post-hoc experiment, the model agreed in 76% of cases with the annotators.  Looking at the results of the classification models, the 49% accu- racy of the prompt-engineering approach can be considered low, yet we consider that it is a valid avenue for further investigation, as in the second post-hoc analysis, we found that the model agrees  633\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  with human annotators in 76% of the cases. Clearly, framing in- volves aspects of subjectivity [42]. Much of what we do as people has a subjective component, influenced by how we feel or how we express opinions.  News reading is never fully objective, and the annotators en- gaged in the frame classification task, influenced by their personal state of mind, experience, and culture, may perceive information differently. Monarch affirms that \"for simple tasks, like binary labels on objective tasks, the statistics are fairly straightforward to decide which is the \u2018correct\u2019 label when different annotators disagree. But for subjective tasks, or even objective tasks with continuous data, there are no simple heuristics for deciding what the correct label should be\" [42].  Subjectivity is involved in both the generation and perception of information: the assumption that there is only one frame is com- plicated by the point of view of the reader. In the case of news, the information sender (the journalist) has an intention, but the receiver (the reader) plays a role and is influenced by it. In psychology, this is known as the lens model of interpersonal communication, where the sender has certain objectives, but the receiver can interpret or re-interpret what the sender wants to say, with more or less accuracy [26].  Following this discussion on subjectivity, the question arose as to what would happen if, instead of headlines, we used the complete article as a source of analysis. We wondered if longer text could make the frame labeling task clearer than when using headlines. Yet another possible hypothesis is that having to read longer texts could lead to the same subject being presented from different angles. Please recall that in the existing literature discussed in Section 2, both headlines and full articles have been used from frame analysis (see Table 1.) This remains as an issue for future work.  7 CONCLUSIONS In this paper, we first presented an analysis of human-generated news frames on the covid-19 no-vax movement in Europe, and then studied different approaches using large language models for automatic inference of frames. We conclude by answering the two research questions we posed:  RQ1: What are the main frames in the news headlines about the covid-19 anti-vaccine movement in 5 European countries? After annotating the headlines, we found that of the 1786 headlines, the predominant frame is human interest (45.3% of cases), which presents a news item with an emotional angle, putting a face to a problem or situation. We also found that a substantial proportion of headlines were annotated as not presenting any frame (40.2% of cases). Finally, the other frame types are found more infrequently. RQ2: Can prompt engineering be used for classification of head- lines according to frames? We first used fine-tuning of a number of language models, and found that GPT-3.5 produced classification ac- curacy of 72% on a six-frame classification task. This represented a modest 2% improvement over BERT-based models, at a significantly larger environmental cost. We then presented a new way of classi- fying frames using prompts. At the headline level, inferences made with GPT-3.5 reached 49% of agreement with human-generated frame labels. In many cases, the GPT-3.5 model inferred frame types that were considered as valid choices by human annotators,  and in an post-doc experiment, the human-machine agreement reached 76%. These results have opened several new directions for future work.  ACKNOWLEDGMENTS",
  "references": [
    "1] Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online media: Quantitative framing analysis on Detik. com\u2019s coverage of Covid-19. Jurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153\u2013170.  ",
    "2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. 2021. RAFT: A real-world few-shot text classification benchmark. arXiv preprint arXiv:2109.14076 (2021).  ",
    "3] David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe\u2019s Press (2022), 35\u201343.  Cover Covid-19 Vaccination News? A Five-Country Analysis. https://doi.org/10.1145/3512732.3533588  ",
    "4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? (2021), 610\u2013623.  ",
    "5] Santosh Kumar Biswal and Nikhil Kumar Gouda. 2020. Artificial intelligence in journalism: A boon or bane? In Optimization in machine learning and applications. Springer, 155\u2013167.  ",
    "6] Erik Bleich, Hannah Stonebraker, Hasher Nisar, and Rana Abdelhamid. 2015. Media portrayals of minorities: Muslims in British newspaper headlines, 2001\u2013 2012. Journal of Ethnic and Migration Studies 41, 6 (2015), 942\u2013962.  ",
    "7] Michael Bommarito and Daniel Martin Katz. 2022. GPT Takes the Bar Exam.  https://doi.org/10.48550/ARXIV.2212.14402  ",
    "8] Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe, Michel Dupagne, and Ching-Hua Chuan. 2019. Artificial intelligence and jour- nalism. Journalism & Mass Communication Quarterly 96, 3 (2019), 673\u2013695. ",
    "9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901.  ",
    "10] Bj\u00f6rn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, and Claes H De Vreese. 2014. Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis. Communication Methods and Measures 8, 3 (2014), 190\u2013206.  ",
    "11] Bjorn Burscher, Rens Vliegenthart, and Claes H de Vreese. 2016. Frames beyond words: Applying cluster and sentiment analysis to news coverage of the nuclear power issue. Social Science Computer Review 34, 5 (2016), 530\u2013545.  ",
    "12] Dallas Card, Amber Boydstun, Justin Gross, Philip Resnik, and Noah Smith. 2015. The Media Frames Corpus: Annotations of Frames Across Issues. 2 (01 2015), 438\u2013444. https://doi.org/10.3115/v1/P15-2072  ",
    "13] Daniel Catalan-Matamoros and Carlos El\u00edas. 2020. Vaccine hesitancy in the age of coronavirus and fake news: analysis of journalistic sources in the Spanish quality press. International Journal of Environmental Research and Public Health 17, 21 (2020), 8136.  ",
    "14] Daniel Catal\u00e1n-Matamoros and Carmen Pe\u00f1afiel-Saiz. 2019. Media and mistrust of vaccines: a content analysis of press headlines. Revista latina de comunicaci\u00f3n social 74 (2019), 786\u2013802.  ",
    "15] Mark Coddington. 2015. Clarifying journalism\u2019s quantitative turn: A typology for evaluating data journalism, computational journalism, and computer-assisted reporting. Digital journalism 3, 3 (2015), 331\u2013348.  ",
    "16] Stephen D Cooper. 2010. The oppositional framing of bloggers. In Doing News  Framing Analysis. Routledge, 151\u2013172.  ",
    "17] Robert Dale. 2021. GPT-3: What\u2019s it good for? Natural Language Engineering 27,  1 (2021), 113\u2013118.  ",
    "18] Astrid Dirikx and Dave Gelders. 2010.  To frame is to explain: A de- ductive frame-analysis of Dutch and French climate change coverage dur- ing the annual UN Conferences of the Parties. Public Understanding of Science 19, 6 (2010), 732\u2013742. https://doi.org/10.1177/0963662509352044 arXiv:https://doi.org/10.1177/0963662509352044 PMID: 21560546.  ",
    "19] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A deductive frame- analysis of Dutch and French climate change coverage during the annual UN Conferences of the Parties. Public understanding of science 19, 6 (2010), 732\u2013742.  634\fFraming the News: From Human Perception to Large Language Model Inferences  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  ",
    "20] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2020. Gsum: A general framework for guided neural abstractive summarization. arXiv preprint arXiv:2010.08014 (2020).  ",
    "21] Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA Gesondheid (Online) 27 (2022), 1\u20138.  ",
    "22] Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021). ",
    "23] Robert M Entman. 1993. Framing: Towards clarification of a fractured paradigm.  McQuail\u2019s reader in mass communication theory 390 (1993), 397.  ",
    "24] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723 (2020). ",
    "25] Piyush Ghasiya and Koji Okamura. 2021. Investigating COVID-19 news across four nations: a topic modeling and sentiment analysis approach. Ieee Access 9 (2021), 36645\u201336656.  ",
    "26] Robert Gifford. 1994. A Lens-Mapping Framework for Understanding the En- coding and Decoding of Interpersonal Dispositions in Nonverbal Behavior. Journal of Personality and Social Psychology 66 (02 1994), 398\u2013412. https: //doi.org/10.1037//0022-3514.66.2.398  ",
    "27] Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based transformer architectures for long document summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 1792\u20131810.  ",
    "28] Anushka Gupta, Diksha Chugh, Rahul Katarya, et al. 2022. Automated news summarization using transformers. In Sustainable Advanced Computing. Springer, 249\u2013259.  ",
    "29] Alfred Hermida and Mary Lynn Young. 2017. Finding the data unicorn: A hierar- chy of hybridity in data and computational journalism. Digital Journalism 5, 2 (2017), 159\u2013176.  ",
    "30] Karoliina Isoaho, Daria Gritsenko, and Eetu M\u00e4kel\u00e4. 2021. Topic modeling and text analysis for qualitative policy research. Policy Studies Journal 49, 1 (2021), 300\u2013324.  ",
    "31] Carina Jacobi, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative analysis of large amounts of journalistic texts using topic modelling. Digital journalism 4, 1 (2016), 89\u2013106.  ",
    "32] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computa- tional Linguistics 8 (2020), 423\u2013438.  ",
    "33] Shima Khanehzar, Andrew Turpin, and Gosia Miko\u0142ajczak. 2019. Modeling  ",
    "46] Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with  mixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021).  ",
    "47] Rabindra Lamsal. 2021. Sentiment Analysis of English Tweets with BERTsent. https://huggingface.co/rabindralamsal/finetuned-bertweet-sentiment-analysis. ",
    "48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.  ",
    "49] Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, and Ahad Ali. 2022. Fake News Classification using transformer based enhanced LSTM and BERT. International Journal of Cognitive Computing in Engineering 3 (2022), 98\u2013105. https://doi.org/10.1016/j.ijcce.2022.03.003  ",
    "50] Frida V Rodelo. 2021. Framing of the Covid-19 pandemic and its organizational  predictors. Cuadernos. info 50 (2021), 91\u2013112.  ",
    "51] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).  ",
    "52] Holli Semetko and Patti Valkenburg. 2000. Framing European Politics: A Content Analysis of Press and Television News. Journal of Communication 50 (06 2000), 93 \u2013 109. https://doi.org/10.1111/j.1460-2466.2000.tb02843.x  ",
    "53] Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Em- manouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. arXiv preprint arXiv:2104.08768 (2021).  ",
    "54] Efstathios Sidiropoulos and Andreas Veglis. 2017. Computer Supported Collab- orative Work trends on Media Organizations: Mixing Qualitative and Quan- titative Approaches. Studies in Media and Communication 5 (04 2017), 63. https://doi.org/10.11114/smc.v5i1.2279  ",
    "55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019). ",
    "56] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503 (2021).  ",
    "57] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning.  arXiv preprint arXiv:1806.02847 (2018).  ",
    "58] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems 34 (2021), 200\u2013212.  ",
    "59] Sandra A Vannoy and Prashant Palvia. 2010. The social influence model of  technology adoption. Commun. ACM 53, 6 (2010), 149\u2013153.  ",
    "60] Tuukka Yl\u00e4-Anttila, Veikko Eranti, and Anna Kukkonen. 2022. Topic modeling for frame analysis: A study of media debates on climate change in India and USA. Global Media and Communication 18, 1 (2022), 91\u2013112.  Political Framing Across Policy Issues and Contexts. In ALTA.  ",
    "34] Jeesun Kim and Wayne Wanta. 2018. News framing of the US immigration debate during election years: Focus on generic frames. The Communication Review 21, 2 (2018), 89\u2013115.  ",
    "35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi- hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).  ",
    "36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. (2021). https://doi.org/10. 48550/ARXIV.2107.13586  ",
    "37] Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and Derry Tanti Wijaya. 2019. Detecting frames in news headlines and its application to analyzing news framing trends surrounding US gun violence. In Proceedings of the 23rd conference on computational natural language learning (CoNLL).  ",
    "38] J\u00f6rg Matthes and Matthias Kohring. 2008. The Content Analysis of Media Frames: Toward Improving Reliability and Validity. Journal of Communication 58 (06 2008). https://doi.org/10.1111/j.1460-2466.2008.00384.x  ",
    "39] Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, and David E Losada. 2022. Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI. In Proceedings of the 4th Conference on Conversational User Interfaces. 1\u20136.  ",
    "40] Stuart E Middleton, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Social computing for verifying social media content in breaking news. IEEE Internet Computing 22, 2 (2018), 83\u201389.  ",
    "41] Marko Milosavljevi\u0107 and Igor Vobi\u010d. 2021.  \u2018Our task is to demystify fears\u2019: Analysing newsroom management of automation in journalism. Journalism 22, 9 (2021), 2203\u20132221.  ",
    "42] R. Monarch. 2021. Human-in-the-Loop Machine Learning: Active Learning and Annotation for Human-centered AI. Manning. https://books.google.ch/books? id=LCh0zQEACAAJ  ",
    "43] Tom Nicholls and Pepper D Culpepper. 2021. Computational identification of media frames: Strengths, weaknesses, and opportunities. Political Communication 38, 1-2 (2021), 159\u2013181.  ",
    "44] Zhongdang Pan and Gerald M Kosicki. 1993. Framing analysis: An approach to  news discourse. Political communication 10, 1 (1993), 55\u201375.  ",
    "45] Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative  language models. arXiv preprint arXiv:1912.10165 (2019).  635\f"
  ],
  "url": "https://drive.google.com/uc?id=1vmiWI3NUHORkl-g7D9WA8Xh7YbTOHfmB",
  "date": "2024-01-09 21:23:30",
  "is_published": false
}
{
  "title": "Framing the News: From Human Perception to Large Language Model Inferences",
  "authors": [
    "David Alonso del Barrio",
    "Idiap Research Institute",
    "Daniel Gatica-Perez",
    "Idiap Research Institute and EPFL"
  ],
  "institutions": [
    "Switzerland",
    "Switzerland"
  ],
  "abstract": "Identifying the frames of news is important to understand the arti- cles\u2019 vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of jour- nalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to under- stand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No- Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Nat- ural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT- 3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.",
  "keywords": [
    "Covid-19 no-vax",
    " news framing",
    " GPT-3",
    " prompt-engineering",
    " trans-",
    "formers",
    " large language models"
  ],
  "article": "In recent years, there has been a proliferation in the use of concepts such as data journalism, computational journalism, and computer- assisted reporting [15] [29], which all share the vision of bridging journalism and technology. The progress made in NLP has been gradually integrated into the journalistic field [5][8][54]. More specifically, machine learning models based on transformers have been integrated in the media sector in different tasks [41] such as the creation of headlines with generative languages models [17], summarization of news articles [28][27], false news detection [49], and topic modeling and sentiment analysis [25]. The development of large language models such as GPT-3 [9], BLOOM [51] or ChatGPT show a clear trend towards human-machine interaction becoming easier and more intuitive, opening up a wide range of research possibilities. At the same time, the use of these models is also associated with a lack of transparency regarding how these models work, but efforts are being made to bring some transparency to where they cannot [35]. Based on the premises that these models open up a wide range of research directions [7], and that at the same time (and needless to say) they are not the solution to all problems, can be potentially useful, while acknowledging and systematically documenting their limitations [56]. More specifically, the aim of this work is to analyze the performance of GPT-3.5 for a specific use case, namely the analysis of frames in news, from an empirical point of view, with the objective of shedding light on a potential use of generative models in journalistic tasks.  Frame analysis is a concept from journalism, which consists of studying the way in which news stories are presented on an issue, and what aspects are emphasized: Is a merely informative vision given in an article? Or is it intended to leave a moral lesson? Is a news article being presented from an economic point of view? Or from a more human, emotional angle? The examples above correspond to different frames with which an article can be written. The concept of news framing has been studied in computing as a step beyond topic modeling and sentiment analysis, and for this purpose, in recent years, pre-trained language models have been used for fine-tuning the classification process of these frames [60] [10], but the emergence of generative models opens the possibility of doing prompt-engineering of these classification tasks, instead of the fine-tuning approach investigated so far.  Our work aims to address this research gap by posing the fol-  lowing research questions:  RQ1: What are the main frames in the news headlines about the anti-vaccine movement, as reported in newspapers across 5 European countries?  627\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  RQ2: Can prompt engineering be used for classification of head-  lines according to frames?  By addressing the above research questions, our work makes the  following contributions:  Contribution 1. We implemented a process to do human an- notation of the main frame of 1786 headlines of articles about the Covid-19 no-vax movement, as reported in 19 newspapers from 5 European countries (France, Italy, Spain, Switzerland and United Kingdom.) At the headline level, we found that the predominant frame was human interest, where this frame corresponds to a per- sonification of an event, either through a statement by a person, or the explanation of a specific event that happened to a person. Furthermore, we found a large number of headlines annotated as containing no frame, as they simply present information without entering into evaluations. We also found that for all the countries involved, the distribution of frame types was very similar, i.e., hu- man interest and no frame are the two predominant frames. Finally, the generated annotations allowed to subsequently study the per- formance of a large language model.  Contribution 2. We studied the performance of GPT-3.5 on the task of frame classification of headlines. In addition to using the fine-tuning approach from previous literature, we propose an alternative approach for frame classification that requires no labeled data for training, namely prompt-engineering using GPT-3.5. The results show that fine-tuning with GPT-3.5 produces 72% accuracy (slightly higher than other smaller models), and that the prompt- engineering approach results in lower performance (49% accuracy.) Our analysis also shows that the subjectivity of the human labeling task has an effect on the obtained accufracy.  The paper is organized as follows. In Section 2, we discuss related work. In Section 3, we describe the news dataset. In Section 4, we describe the methodology for both human labeling and machine classification of news frames. We present and discuss results for RQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide conclusions in Section 7.  2 RELATED WORK Framing has been a concept widely studied in journalism, with a definition that is rooted in the study of this domain [23]: \u201cTo frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a par- ticular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.\u201d  For frame recognition, there are two main approaches: the induc- tive approach [16], where one can extract the frames after reading the article, and the deductive approach [38], where a predefined list of frames exists and the goal is to interpret if any of them ap- pears in the article. In the deductive case, there are generic frames and subject-specific frames, and the way to detect them typically involves reading and identifying one frame at a time, or through answers to yes/no questions that represent the frames. Semetko et al. [52] used 5 types of generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) based on previous literature, and they defined a list of 20 yes/no questions to detect frames in articles. For instance, the questions about morality are the following: \"Does the story contain any moral  message? Does the story make reference to morality, God, and other religious tenets? Does the story offer specific social prescriptions about how to behave?\", and so on for each of the frame types. This categorization of frames has been used in various topics such as climate change [18] [19], vaccine hesitance [13], or immigration [34].  We now compare the two approaches on a common topic, such as Covid-19. Ebrahim et al. [21] followed an inductive approach in which the frames were not predefined but emerged from the text (e.g., deadly spread, stay home, what if, the cost of Covid-19) using headlines as the unit of analysis. In contrast, the deductive approach has studied very different labels. El-Behary et al. [22] followed the method of yes/no questions, but in addition to the 5 generic frames presented before, they also used blame frame and fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5 generic frames with yes/no questions, while Catal\u00e1n-Matamoros et al. [14] used the 5 frames and read the headline and subheadline to decide the main frame. Table 1 summarizes some of the the existing approaches. This previous work showed how frame labels can be different, and also that frame analysis has been done at both headline and article levels. These two approaches (inductive and deductive) that originated in journalism have since been replicated in the computing literature.  We decided to follow the deductive approach because a prede- fined list of frames allows to compare among topics, countries, previous literature, and also because they represent a fixed list of labels for machine classification models. Furthermore, the induc- tive approach tends to be more specific to a topic, and from the computing viewpoint, past work has tried to justify topic modeling as a technique to extract frames from articles.  Yl\u00e4-Antitila et al. [60] proposed topic modeling as a frame ex- traction technique. They argued that topics can be interpreted as frames if three requirements are met: frames are operationalized as connections between concepts; subject-specific data is selected; and topics are adequately validated as frames, for which they suggested a practical procedure. This approach was based on the choice of a specific topic (e.g., climate change) and the use of Latent Dirichlet Allocation (LDA) as a technique to extract a number of subtopics. In a second phase, a qualitative study of the top 10 words of each subtopic was performed, and the different subtopics were elimi- nated or grouped, reducing the number and establishing a tentative description. In a third phase, the top 10 articles belonging to that frame/topic were taken, and if the description of the topic fitted at least 8 of the 10 articles, that topic/frame remained. The frames found in this article were: green growth, emission cuts, negotiations and treaties, environmental risk, cost of carbon emissions, Chinese emissions, economics of energy production, climate change, en- vironmental activism, North-South burden sharing, state leaders negotiating, and citizen participation.  From Entman\u2019s definition of frame [23], it seems that the deduc- tive approach is more refined than the inductive approach (which seems to resemble the detection of sub-themes.) For example, with regard to climate change, there are stories on how people have been affected by climate change from an emotional point of view, thus personalizing the problem. In this case, we could categorize the corresponding frame as human interest, as the writer of the article is selecting \"some aspects of a perceived reality and make them  628\fFraming the News: From Human Perception to Large Language Model Inferences  more salient\". The language subtleties with which news articles are presented cannot be captured with basic topic modeling.  Isoaho et al.[30] held the position that while the benefits of scale and scope in topic modeling were clear, there were also a number of problems, namely that topic outputs do not correspond to the methodological definition of frames, and thus topic modeling remained an incomplete method for frame analysis. Topic modeling, in the practice of journalistic research, is a useful technique to deal with the large datasets that are available, yet is often not enough to do more thorough analyses [31]. In our work, we clearly notice that frame analysis is not topic modeling. For example, two documents could be about the same topic, say Covid-19 vaccination, but one article could emphasize the number of deaths after vaccination, while the other emphasized the role of the vaccine as a solution to the epidemic.  We also consider that the larger the number of possible frame types, the more likely it is to end up doing topic modeling instead of frame analysis. Using a deductive approach, Dallas et al. [12] created a dataset with articles about polemic topics such as immigration, same sex marriage, or smoking, and they defined 15 types of frames: \"economic, capacity and resources, morality, fairness and equality, legality, constitutionality and jurisprudence, policy prescription and evaluation, crime and punishment, security and defense, health and safety, quality of life, cultural identity, political, external regulation and reputation, other\". In this case, they authors did not use a list of questions. Instead, for each article, annotators were asked to identify any of the 15 framing dimensions present in the article and to label text blurbs that cued them (based on the definitions of each of the frame dimensions) and decide the main frame of each article. In our case, we followed the idea of detecting the main frame by reading the text instead of answering questions, but instead of using the 15 frames proposed in [12] , we used the 5 generic frames proposed in [52].  A final decision in our work was the type of text to analyze, whether headlines or whole article. For this decision, the chosen classification method was also going to be important. For example, Khanehzar et al. [33] used traditional approaches such as SVMs as baseline, and demonstrated the improvement in frame classifica- tion with the use of pre-trained languages models such as BERT, RoBERTa and XLNet, following a fine-tuning approach, setting as input text a maximum of 256 tokens (although the maximum number of input tokens in these models is 512 tokens.) Liu et al. [37] classified news headlines about the gun problem in the United States, arguing for the choice of headlines as a unit of analysis based on previous journalism literature [6], [44], that advocated for the importance and influence of headlines on readers and the subsequent perception of articles. From a computational viewpoint, using headlines is also an advantage, since you avoid the 512 token limitation in BERT-based models. Therefore, we decided to work with headlines about a controversial issue, namely the Covid-19 no-vax movement.  Continuing with the question of the methods used for classi- fication, much work has been developed in prompt engineering, especially since the release of GPT-3. Liu et al.[36] presented a good overview of the work done on this new NLP paradigm, not only explaining the concept of prompt engineering, but also the differ- ent strategies that can be followed both in the design of prompts,  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  Table 1: Summary of deductive approaches for frame analysis  Ref Frames  Goal  Technique  To label frames of full articles  [12] 15 generic frames: \"Economic\", \"Capac- ity and resources\", \"Morality\", \"Fair- ness and equality\", \"Legality, constitu- tionality and jurisprudence\", \"Policy prescription and evaluation\", \"Crime and punishment\", \"Security and de- fense\", \"Health and safety\", \"Quality of life\", \"Cultural identity\", \"Public opin- ion\", \"Political\", \"External regulation and reputation\", \"Other\".  [33] 15 generic frames  Classification  [52] 5 generic frames: \"human interest\", \"conflict\", \"morality\", \"attribution of responsibility\", and \"economic conse- quences\".  \u201cSociety/Culture\u201d, ,  [37] 9 specific frames:\u201cPolitics\u201d, \u201cPublic opinion\u201d, and \u201c2nd \u201cEconomic consequences\u201d Amendment\u201d (Gun Rights), \u201cGun control/regulation\u201d, \u201cMental health\u201d, \u201cSchool/Public space safety\u201d, and \u201cRace/Ethnicity\u201d.  [22] 5 generic frames + blame frame and  fear frame 5 generic frames  [1]  To label frames of full articles  To label frames of full articles/ Classification  To label frames of full articles To label frames of full articles  [50] 5 generic frames + pandemic frames  [14] 5 generic frames, journalistic role and  pandemic frames  To label frames of full articles To label frames of full articles  the Reading article, full the annotator defines the main frame  BERT based models Yes/No ques- tions.  the Reading full article, the annotator the defines main frame. BERT based models Yes/No ques- tions. Reading the article, full the annotator defines the main frame. Yes/No ques- tions. Reading the headline and subheadline, the annotator defines the main frame.  Number samples  of  20000 articles  12000 articles  2600 articles and 1522 tv news stories  2990 headlines  1170 articles  6713 articles  2742 articles  131 headlines + subheadlines  the potential applications, and the challenges to face when using this approach. Prompt engineering applications include knowledge probing [46], information extraction [53], NLP reasoning [57], ques- tion answering [32], text generation [20], multi-modal learning [58], and text classification [24], the latter being the prompt-engineering use case in our work. Puri et al.[45] presented a very interesting idea that we apply to our classification task. This consists of pro- viding the language model with natural language descriptions of classification tasks as input, and training it to generate the correct answer in natural language via a language modeling objective. It is a zero-shot learning approach, in which no examples are used to explain the task to the model. Radford et al. [48] demonstrated that language models can learn tasks without any explicit supervision. We have followed this approach to find an alternative way to do frame analysis.  As mentioned before, the emergence of giant models like GPT-3, BLOOM, and ChatGPT are a very active research topic. To the best of our knowledge, on one hand our work extends the computational analysis of news related to the covid-19 no-vax movement, which illustrates the influence of the press on the ways societies think about relevant issues [40], [59], and on the other hand it adds to the literature of human-machine interaction, regarding the design of GPT-3 prompts for classification tasks [39], [2].  3 DATA: EUROPEAN COVID-19 NEWS  DATASET  We used part of the European Covid-19 News dataset collected in our recent work [3]. This dataset contains 51320 articles on Covid- 19 vaccination from 19 newspapers from 5 different countries: Italy,  629\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  France, Spain, Switzerland and UK. The articles cover a time period of 22 months, from January 2020 to October 2021. All content was translated into English to be able to work in a common language. The dataset was used for various analyses, such as name entity recognition, sentiment analysis, and subtopic modeling, to under- stand how Covid-19 vaccination was reported in Europe through the print media (in digital format.) The subtopic modeling analysis revealed a subsample of articles on the no-vax movement, which is the one we have used in this paper. We took the headlines of the articles associated with the no-vax movement, selecting all articles containing any of the keywords in Table 2 in the headline or in the main text. This corresponds to a total of 1786 headlines.  Table 2: Keywords used to identify no-vax articles  NO VAX TOPIC  \"anti-vaxxers\", \"anti-vaccine\", \"anti-vaxx\", \"anti-corona\", \"no-vax\", \"no vax\",\"anti-vaccin\"  Keywords  In Table 3, we show the number of headlines per country and newspaper. France is the country with the most no-vax articles in the corpus, with 523 articles, followed by Italy with 508. However, note that there are 6 newspapers from France, while only 2 from Italy. Corriere della Sera is the newspaper that dealt most frequently with the subject (429 articles), while The Telegraph is the second one (206 articles). The total number of articles normalized by the number of newspapers per country is also shown in the last column of the Table. Using these normalized values, the ranking is Italy, UK, France, Switzerland, and Spain.  Table 3: Number of headlines by newspaper and country  COUNTRY  NEWSPAPER  HEADLINES  TOTAL (NORM. TOTAL)  FRANCE  ITALY  SPAIN  SWITZERLAND  UNITED KINGDOM  La Croix Le Monde Les Echos Liberation Lyon Capitale Ouest France Corriere della Sera Il Sole 24 Ore 20 minutos ABC El Diario El Mundo El Espa\u00f1ol La Vanguardia 24 heures La Libert\u00e9 Le Temps The Irish News The Telegraph  94 125 49 97 8 150 429 79 27 50 32 77 22 95 97 22 111 16 206  523 (87.1)  508 (254.0)  303 (50.5)  230 (76.6)  222 (111.0)  1786  4 METHODOLOGY 4.1 Human labeling of news frames To carry out the labeling of the frames in our corpus of headlines, we first designed a codebook, which contained the definitions of each of the frame types and a couple of examples of each type, as well as a definition of the corpus subject matter and definitions of the concept of frame analysis, so that the annotators could understand the task to be performed. The codebook follows the proposed by  [52] with 5 generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) plus one additional \u2019no-frame\u2019 category. Two researchers were engaged to annotate a sample of the collected newspaper articles following a three-phase training procedure.  In the first phase, annotators had to read the codebook and get familiar with the task. In the second phase, they were asked to identify the main frame in the same subset of 50 headlines. At the end of the second phase, the intercoder reliability (ICR) was 0.58 between the 2 annotators. We analyzed those cases where there were discrepancies, and observed that in some cases, there was not a unique main frame, because both annotators had valid arguments to select one of the frames. In other cases, the discrepancies were due to slight misunderstanding of the definitions. In the third phase, the annotators coded again 50 headlines, and the ICR increased to was 0.66. We realized that the possibility of having two frames remained. They discussed the cases in which they had disagreed, and if the other person\u2019s arguments were considered valid, it could be said that there were two frames. After this three-phase training procedure, annotators were ready to annotate the dataset independently. We divided the dataset into two equal parts, and each person annotated 893 headlines.  4.2 Fine-tuning GPT-3.5 and BERT-based  models  With the annotated dataset, we investigated two NLP approaches: the first one involves fine-tuning a pre-trained model; the second one is prompt engineering. Pre-trained language models have been  Figure 1: Pre-train, fine-tune, prompt  trained with large text strings based on two unsupervised tasks, next sentence prediction and masked language model. Figure 1 summarizes these techniques.  In the first approach, a model with a fixed architecture is pre- trained as a language model (LM), predicting the likelihood of the observed textual data. This can be done due to the availability of large, raw text data needed to train LMs. This learning process can produce general purpose features of the modeled language. The learning process produces robust, general-purpose features of the language being modeled. The above pre-trained LM is then adapted to different downstream tasks, by introducing additional parameters and adjusting them using task-specific objective functions. In this approach, the focus was primarily on goal engineering, designing the training targets used in both the pre-training and the fine-tuning stages [36].  630\fFraming the News: From Human Perception to Large Language Model Inferences  We present an example to illustrate the idea. Imagine that the task is sentiment analysis, and we have a dataset with sentences and their associated sentiment, and a pre-trained model, which is a saved neural network trained with a much larger dataset. For that pre-trained model to address the target task, we unfreeze a few of the top layers of the saved model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows to \"fine-tune\" the higher-order feature representations in the base model to make them more relevant for the sentiment analysis task. In this way, instead of having to obtain a very large dataset with target labels to train a model, we can reuse the pre- trained model and use a much smaller train dataset. We use a part of our dataset as examples for the model to learn the task, while the other part of the dataset is used to evaluate model performance. Previous works related to frame classification in the computing literature have used fine-tuning, BERT-based models. In our work, we have done the same as a baseline, but we aimed to go one step further and also produce results using fine-tuning of GPT-3.5.  4.3 Prompt-engineering with GPT-3.5 Model fine-tuning has been widely used, but with the emergence of generative models such as GPT-3, another way to approach classification tasks has appeared. The idea is to use the pre-trained model directly and convert the task to be performed into a format as close as possible to the tasks for which it has been pre-trained. That is, if the model has been pre-trained from next word prediction as in the case of GPT-3, classification can be done by defining a prompt, where the input to the model is an incomplete sentence, and the model must complete it with a word or several words, just as it has been trained. This avoids having to use part of the already labeled dataset to teach the task to be performed to the model, and a previous labeling is not needed [36].  In this approach, instead of adapting pre-trained LMs to down- stream tasks via objective engineering, downstream tasks are re- formulated to look more like those solved during the original LM training with the help of a textual prompt. For example, when recog- nizing the emotion of a social media post, \u201cI missed the bus today.\u201d, we may continue with a prompt \u201cI felt so _\u201d, and ask the LM to fill the blank with an emotion-bearing word. Or if we choose the prompt \u201cEnglish: I missed the bus today. French: _\u201d), an LM may be able to fill in the blank with a French translation. In this way, by selecting the appropriate prompts, we can influence the model behavior so that the pre-trained LM itself can be used to predict the desired output, even without any additional task-specific training [36].  We use this emerging NLP approach to classify frames at headline level. We are not aware of previous uses of this strategy to classify frames as we propose here. The idea is the following. Prompt engi- neering consists of giving a prompt to the model, and understands that prompt as an incomplete sentence. To do prompt engineer- ing with our dataset, we needed to define an appropriate prompt that would produce the headline frames as output. We defined sev- eral experiments with the Playground of GPT-3, in order to find the best prompt for our task. In our initial experiments, we fol- lowed existing approaches in prompt engineering to do sentiment analysis, where the individual answer was an adjective, and this  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  adjective was matched with a sentiment. In a similar fashion, we decided to build a thesaurus of adjectives that define each of the frames. For instance, the human interest frame could be \u2019interest- ing\u2019, \u2019emotional\u2019, \u2019personal\u2019, \u2019human\u2019. The conflict frame could be: \u2019conflictive\u2019, \u2019bellicose\u2019, \u2019troublesome\u2019, \u2019rowdy\u2019, \u2019quarrelsome\u2019, \u2019trou- blemaker\u2019, \u2019agitator\u2019, etc. After the list of adjectives was defined, we needed to define the prompt in order to get, as an answer, one of the adjectives in our thesaurus to match them with the frame. We used the GPT-3 playground using the headline as input and asking for the frame as output, but the strategy did not work. In our final experiment, instead of giving the headline as input, we gave the definitions of each type of frame plus the headline, and we asked the model to choose between the different types of frames as output. In this way, the output of the model was directly one of the frames, and we avoided the step of matching adjectives with frames. An example is shown in Figure 2.  Figure 2: GPT-3.5 for frame inference: input and output  For the GPT-3 configuration 1, there are 3 main concepts:  \u2022 TEMPERATURE [0-1]. This parameter controls randomness,  lowering it results in less random completions.  \u2022 TOP_P [0-1]. This parameter controls diversity via nucleus  sampling.  \u2022 MAX_TOKENS[1-4000]. This parameter indicates the maxi-  mum number of tokens to generate,  \u2022 MODEL. GPT-3 offer four main models with different levels of power, suitable for different tasks. Davinci is the most capable model, and Ada is the fastest.  After testing with the GPT-3 playground and varying different hyper-parameters to assess performance, we set the temperature to 0, since the higher the temperature the more random the response. Furthermore, the Top-p parameter was set to 1, as it would likely get a set of the most likely words for the model to choose from. The maximum number of tokens was set to 2; in this way, the model is asked to choose between one of the responses. As a model, we used the one with the best performance at the time of experimental design, which was TEXT-DAVINCI-003, recognized as GPT 3.5.  5 RESULTS: HUMAN LABELING OF FRAMES  IN NO-VAX NEWS HEADLINES (RQ1)  In this section, we present and discuss the results of the analysis related to our first RQ.  Figure 3 shows the distribution of frames per country at headline level, with human interest and no-frame being the predominant   631\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  ones. Attribution of responsibility is the third one except in Switzer- land, where the corresponding frame is conflict. Finally, morality and economic are the least represented in the dataset for every country.  Figure 4: Non-normalized monthly distribution of frames.  detail the negative and positive sentiment of each frame category, we observed a few trends:  \u2022 Attribution of responsibility: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Switzerland, and the United Kingdom. \u2022 Conflict: Negative sentiment represents 20-35% of the cases. \u2022 Economic: Predominantly neutral, with only negative tone in Italy and UK (in the latter case, all headlines with this frame were considered negative.)  \u2022 Human interest: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Spain, and Switzerland.  \u2022 Morality: Predominantly neutral, with negative tone in Italy,  Switzerland, and the United Kingdom, \u2022 No frame: 20-30% of negative content.  Figure 5: Sentiment of headline by frame and by country  Figure 3: Non-normalized distribution of frames per country  The monthly distribution of frames aggregated for all countries is shown in Fig. 4. We can see two big peaks, the first one in January 2021 and the second one in August 2021. In all countries, the vac- cination process started at the end of December 2020, so it makes sense that the no-vax movement started to be more predominant in the news in January 2021. Human interest is the most predominant frame. Manual inspection shows that this is because the headlines are about personal cases of people who are pro- or anti- vaccine. Attribution of responsibility is also present. Manual inspection in- dicates that local politicians and health authorities had to make decisions about who could be vaccinated at the beginning of the process. The second peak at the end of summer 2021 coincided with the health pass (also called Covid passport in some countries), and we can observe a peak in the curve corresponding to the con- flict frame, reflecting the demonstrations against the measure of mandatory health passes taken by country governments.  In Figure 5, we compare the sentiment per frame and per country, to understand if there were any major differences. The sentiment analysis labels were obtained using BERT-sent from the Hugging Face package [47], used in our previous work (please refer to our original analysis in [3] for details.) We normalized the results be- tween 0 and 1 to compare frames between countries. We see that the sentiment is predominantly neutral (in blue). Examining in more  632\fFraming the News: From Human Perception to Large Language Model Inferences  Regarding the results of the annotation process, the fact that the distribution of the 6 frame types is relatively similar between coun- tries suggests that the anti-vaccine movement issue was treated in a similar way in these countries. The fact that human interest is the most dominant frame indicates that this issue was treated from a more human and emotional approach, with headlines about personal experiences, celebrities giving their opinion about vacci- nation, and politicians defending vaccine policies. Moreover, the reason for many headlines being classified as no-frame is partly due to how data was selected. We chose articles that contained words related to no-vax, either in the headline or in the article. This resulted in many headlines not containing anything specific related to no-vax, while the no-vax content was actually included in the main text of the corresponding articles.  It is worth mentioning that prior to obtaining the results, we had expected that attribution of responsibility would be among the most prominent frames, since governments took many measures such as mandatory health pass requirements to access certain sites; we had also expected that the conflict frame would be prominent, since there were many demonstrations in Europe. In reality, however, these frames categories were not reflected as frequently at the headline level.  Regarding the analysis at the temporal level, it is clear that certain events were captured by the press, such as the start of vaccination or the mandatory vaccination passport.  Finally, the sentiment analysis of the different frames shows that the predominant tone in all of them is neutral or negative, with very similar trends between countries. This association between senti- ment analysis and frames has been discussed in previous literature [11] [43].  6 RESULTS: GPT-3.5 FOR FRAME  CLASSIFICATION OF HEADLINES (RQ2) Here, we present and discuss the results related to our second RQ.  6.1 Fine-tuning GPT-3.5 Table 4 shows the results of the 6-class classification task using 5-cross validation. Three models were used: GPT-3.5 and two BERT- based models. We observe that, on average, GPT-3.5 performs better than the BERT-based models. This is somehow expected as GPT- 3.5 is a much larger model. Overall, in the case of fine-tuning, the best performance for the six-class frame classification task is 72% accuracy, which is promising, with an improvement over previous models based on BERT. Yet, it should be noted that the performance differences are modest (2% improvement between GPT-3.5 and RoBERTa).  Table 4: Classification results for six-class frame classifica- tion and 5-fold cross validation  ACCURACY  BERT RoBERTa GPT3  0  0.68 0.70 0.75  1  0.69 0.72 0.70  2  0.72 0.72 0.72  3  0.64 0.67 0.71  4  0.70 0.71 0.71  AVERAGE  0.67 0.70 0.72  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  On the other hand, BERT is open-source, while GPT-3 has an economic cost as the use of the model is not free, which monetarily limits the number of experiments that can be performed with it, as well as the different configurations one can explore to improve performance. This is important because much of the improvement in performance requires empirical explorations of model parameters More specifically, the cost of an experiment for each of the folds has a cost of 4 dollars (at the time of writing this paper.) This represents a limitation in practice.  Furthermore, GPT-3 has a significant carbon footprint. Similarly, for prompt engineering (discussed in the next subsection), choosing the right prompt (i.e., the words that best define the task so that the model is able to perform adequately) is also based on trial and error. This also has an impact on carbon footprint. In connection with this topic, Strubell et al.[55] argue that improvements in the accu- racy of models depend on the availability of large computational resources, which involve large economic and environmental costs. A criticism has been made as \u2019the rich get richer\u2019, in the sense that not all research groups have sufficient infrastructure resources and access to funding needed to use these models and improve their performance. Also in relation to this analysis, the work of Bender et al. [4] evaluates the costs and risks of the use of large language models, stating that researchers should be aware of the impact that these models have on the environment, and assess whether the benefits outweigh the risks. The work in [4] provides a very telling example, where people living in the Maldives or Sudan are affected by floods and pay the environmental price of training English LLMs, when similar models have not been produced for languages like Dhivehi or Sudanese Arab. In short, there is a need to establish ways to use this technological development responsibly, and it all starts with being aware of the risks it presents.  6.2 Prompt-engineering with GPT-3.5 For each headline, we got the frame that the model considered the most likely, and we compared these GPT-3.5 inferences with the frames labeled by the annotators. The agreement between model and annotator was of 49%. Analyzing the results, and specifically looking at the cases where the annotator and GPT-3.5 disagreed, we discovered that according to the frame definitions, the model in some cases proposed a frame that indeed made sense. This ob- servation, together with our previous experience in the annotation process, where headlines could have more than one valid frame, led us to design a second post-hoc experiment. We took all the headlines where each of the two annotators had disagreed with GPT-3.5, and we asked the annotators to state whether they would agree (or not) with each GPT-inferred label for a given headline. It is important to emphasize that the annotators did not know the origin of that label, i.e., they did not know if it was the label they had originally assigned, or if it was a random one. In this way, we could quantify how GPT-3.5 worked according to valid arguments provided by the annotators. In this post-hoc experiment, the model agreed in 76% of cases with the annotators.  Looking at the results of the classification models, the 49% accu- racy of the prompt-engineering approach can be considered low, yet we consider that it is a valid avenue for further investigation, as in the second post-hoc analysis, we found that the model agrees  633\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  with human annotators in 76% of the cases. Clearly, framing in- volves aspects of subjectivity [42]. Much of what we do as people has a subjective component, influenced by how we feel or how we express opinions.  News reading is never fully objective, and the annotators en- gaged in the frame classification task, influenced by their personal state of mind, experience, and culture, may perceive information differently. Monarch affirms that \"for simple tasks, like binary labels on objective tasks, the statistics are fairly straightforward to decide which is the \u2018correct\u2019 label when different annotators disagree. But for subjective tasks, or even objective tasks with continuous data, there are no simple heuristics for deciding what the correct label should be\" [42].  Subjectivity is involved in both the generation and perception of information: the assumption that there is only one frame is com- plicated by the point of view of the reader. In the case of news, the information sender (the journalist) has an intention, but the receiver (the reader) plays a role and is influenced by it. In psychology, this is known as the lens model of interpersonal communication, where the sender has certain objectives, but the receiver can interpret or re-interpret what the sender wants to say, with more or less accuracy [26].  Following this discussion on subjectivity, the question arose as to what would happen if, instead of headlines, we used the complete article as a source of analysis. We wondered if longer text could make the frame labeling task clearer than when using headlines. Yet another possible hypothesis is that having to read longer texts could lead to the same subject being presented from different angles. Please recall that in the existing literature discussed in Section 2, both headlines and full articles have been used from frame analysis (see Table 1.) This remains as an issue for future work.  7 CONCLUSIONS In this paper, we first presented an analysis of human-generated news frames on the covid-19 no-vax movement in Europe, and then studied different approaches using large language models for automatic inference of frames. We conclude by answering the two research questions we posed:  RQ1: What are the main frames in the news headlines about the covid-19 anti-vaccine movement in 5 European countries? After annotating the headlines, we found that of the 1786 headlines, the predominant frame is human interest (45.3% of cases), which presents a news item with an emotional angle, putting a face to a problem or situation. We also found that a substantial proportion of headlines were annotated as not presenting any frame (40.2% of cases). Finally, the other frame types are found more infrequently. RQ2: Can prompt engineering be used for classification of head- lines according to frames? We first used fine-tuning of a number of language models, and found that GPT-3.5 produced classification ac- curacy of 72% on a six-frame classification task. This represented a modest 2% improvement over BERT-based models, at a significantly larger environmental cost. We then presented a new way of classi- fying frames using prompts. At the headline level, inferences made with GPT-3.5 reached 49% of agreement with human-generated frame labels. In many cases, the GPT-3.5 model inferred frame types that were considered as valid choices by human annotators,  and in an post-doc experiment, the human-machine agreement reached 76%. These results have opened several new directions for future work.  ACKNOWLEDGMENTS",
  "references": [
    "1] Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online media: Quantitative framing analysis on Detik. com\u2019s coverage of Covid-19. Jurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153\u2013170.  ",
    "2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. 2021. RAFT: A real-world few-shot text classification benchmark. arXiv preprint arXiv:2109.14076 (2021).  ",
    "3] David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe\u2019s Press (2022), 35\u201343.  Cover Covid-19 Vaccination News? A Five-Country Analysis. https://doi.org/10.1145/3512732.3533588  ",
    "4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? (2021), 610\u2013623.  ",
    "5] Santosh Kumar Biswal and Nikhil Kumar Gouda. 2020. Artificial intelligence in journalism: A boon or bane? In Optimization in machine learning and applications. Springer, 155\u2013167.  ",
    "6] Erik Bleich, Hannah Stonebraker, Hasher Nisar, and Rana Abdelhamid. 2015. Media portrayals of minorities: Muslims in British newspaper headlines, 2001\u2013 2012. Journal of Ethnic and Migration Studies 41, 6 (2015), 942\u2013962.  ",
    "7] Michael Bommarito and Daniel Martin Katz. 2022. GPT Takes the Bar Exam.  https://doi.org/10.48550/ARXIV.2212.14402  ",
    "8] Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe, Michel Dupagne, and Ching-Hua Chuan. 2019. Artificial intelligence and jour- nalism. Journalism & Mass Communication Quarterly 96, 3 (2019), 673\u2013695. ",
    "9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901.  ",
    "10] Bj\u00f6rn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, and Claes H De Vreese. 2014. Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis. Communication Methods and Measures 8, 3 (2014), 190\u2013206.  ",
    "11] Bjorn Burscher, Rens Vliegenthart, and Claes H de Vreese. 2016. Frames beyond words: Applying cluster and sentiment analysis to news coverage of the nuclear power issue. Social Science Computer Review 34, 5 (2016), 530\u2013545.  ",
    "12] Dallas Card, Amber Boydstun, Justin Gross, Philip Resnik, and Noah Smith. 2015. The Media Frames Corpus: Annotations of Frames Across Issues. 2 (01 2015), 438\u2013444. https://doi.org/10.3115/v1/P15-2072  ",
    "13] Daniel Catalan-Matamoros and Carlos El\u00edas. 2020. Vaccine hesitancy in the age of coronavirus and fake news: analysis of journalistic sources in the Spanish quality press. International Journal of Environmental Research and Public Health 17, 21 (2020), 8136.  ",
    "14] Daniel Catal\u00e1n-Matamoros and Carmen Pe\u00f1afiel-Saiz. 2019. Media and mistrust of vaccines: a content analysis of press headlines. Revista latina de comunicaci\u00f3n social 74 (2019), 786\u2013802.  ",
    "15] Mark Coddington. 2015. Clarifying journalism\u2019s quantitative turn: A typology for evaluating data journalism, computational journalism, and computer-assisted reporting. Digital journalism 3, 3 (2015), 331\u2013348.  ",
    "16] Stephen D Cooper. 2010. The oppositional framing of bloggers. In Doing News  Framing Analysis. Routledge, 151\u2013172.  ",
    "17] Robert Dale. 2021. GPT-3: What\u2019s it good for? Natural Language Engineering 27,  1 (2021), 113\u2013118.  ",
    "18] Astrid Dirikx and Dave Gelders. 2010.  To frame is to explain: A de- ductive frame-analysis of Dutch and French climate change coverage dur- ing the annual UN Conferences of the Parties. Public Understanding of Science 19, 6 (2010), 732\u2013742. https://doi.org/10.1177/0963662509352044 arXiv:https://doi.org/10.1177/0963662509352044 PMID: 21560546.  ",
    "19] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A deductive frame- analysis of Dutch and French climate change coverage during the annual UN Conferences of the Parties. Public understanding of science 19, 6 (2010), 732\u2013742.  634\fFraming the News: From Human Perception to Large Language Model Inferences  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  ",
    "20] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2020. Gsum: A general framework for guided neural abstractive summarization. arXiv preprint arXiv:2010.08014 (2020).  ",
    "21] Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA Gesondheid (Online) 27 (2022), 1\u20138.  ",
    "22] Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021). ",
    "23] Robert M Entman. 1993. Framing: Towards clarification of a fractured paradigm.  McQuail\u2019s reader in mass communication theory 390 (1993), 397.  ",
    "24] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723 (2020). ",
    "25] Piyush Ghasiya and Koji Okamura. 2021. Investigating COVID-19 news across four nations: a topic modeling and sentiment analysis approach. Ieee Access 9 (2021), 36645\u201336656.  ",
    "26] Robert Gifford. 1994. A Lens-Mapping Framework for Understanding the En- coding and Decoding of Interpersonal Dispositions in Nonverbal Behavior. Journal of Personality and Social Psychology 66 (02 1994), 398\u2013412. https: //doi.org/10.1037//0022-3514.66.2.398  ",
    "27] Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based transformer architectures for long document summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 1792\u20131810.  ",
    "28] Anushka Gupta, Diksha Chugh, Rahul Katarya, et al. 2022. Automated news summarization using transformers. In Sustainable Advanced Computing. Springer, 249\u2013259.  ",
    "29] Alfred Hermida and Mary Lynn Young. 2017. Finding the data unicorn: A hierar- chy of hybridity in data and computational journalism. Digital Journalism 5, 2 (2017), 159\u2013176.  ",
    "30] Karoliina Isoaho, Daria Gritsenko, and Eetu M\u00e4kel\u00e4. 2021. Topic modeling and text analysis for qualitative policy research. Policy Studies Journal 49, 1 (2021), 300\u2013324.  ",
    "31] Carina Jacobi, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative analysis of large amounts of journalistic texts using topic modelling. Digital journalism 4, 1 (2016), 89\u2013106.  ",
    "32] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computa- tional Linguistics 8 (2020), 423\u2013438.  ",
    "33] Shima Khanehzar, Andrew Turpin, and Gosia Miko\u0142ajczak. 2019. Modeling  ",
    "46] Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with  mixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021).  ",
    "47] Rabindra Lamsal. 2021. Sentiment Analysis of English Tweets with BERTsent. https://huggingface.co/rabindralamsal/finetuned-bertweet-sentiment-analysis. ",
    "48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.  ",
    "49] Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, and Ahad Ali. 2022. Fake News Classification using transformer based enhanced LSTM and BERT. International Journal of Cognitive Computing in Engineering 3 (2022), 98\u2013105. https://doi.org/10.1016/j.ijcce.2022.03.003  ",
    "50] Frida V Rodelo. 2021. Framing of the Covid-19 pandemic and its organizational  predictors. Cuadernos. info 50 (2021), 91\u2013112.  ",
    "51] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).  ",
    "52] Holli Semetko and Patti Valkenburg. 2000. Framing European Politics: A Content Analysis of Press and Television News. Journal of Communication 50 (06 2000), 93 \u2013 109. https://doi.org/10.1111/j.1460-2466.2000.tb02843.x  ",
    "53] Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Em- manouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. arXiv preprint arXiv:2104.08768 (2021).  ",
    "54] Efstathios Sidiropoulos and Andreas Veglis. 2017. Computer Supported Collab- orative Work trends on Media Organizations: Mixing Qualitative and Quan- titative Approaches. Studies in Media and Communication 5 (04 2017), 63. https://doi.org/10.11114/smc.v5i1.2279  ",
    "55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019). ",
    "56] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503 (2021).  ",
    "57] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning.  arXiv preprint arXiv:1806.02847 (2018).  ",
    "58] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems 34 (2021), 200\u2013212.  ",
    "59] Sandra A Vannoy and Prashant Palvia. 2010. The social influence model of  technology adoption. Commun. ACM 53, 6 (2010), 149\u2013153.  ",
    "60] Tuukka Yl\u00e4-Anttila, Veikko Eranti, and Anna Kukkonen. 2022. Topic modeling for frame analysis: A study of media debates on climate change in India and USA. Global Media and Communication 18, 1 (2022), 91\u2013112.  Political Framing Across Policy Issues and Contexts. In ALTA.  ",
    "34] Jeesun Kim and Wayne Wanta. 2018. News framing of the US immigration debate during election years: Focus on generic frames. The Communication Review 21, 2 (2018), 89\u2013115.  ",
    "35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi- hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).  ",
    "36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. (2021). https://doi.org/10. 48550/ARXIV.2107.13586  ",
    "37] Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and Derry Tanti Wijaya. 2019. Detecting frames in news headlines and its application to analyzing news framing trends surrounding US gun violence. In Proceedings of the 23rd conference on computational natural language learning (CoNLL).  ",
    "38] J\u00f6rg Matthes and Matthias Kohring. 2008. The Content Analysis of Media Frames: Toward Improving Reliability and Validity. Journal of Communication 58 (06 2008). https://doi.org/10.1111/j.1460-2466.2008.00384.x  ",
    "39] Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, and David E Losada. 2022. Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI. In Proceedings of the 4th Conference on Conversational User Interfaces. 1\u20136.  ",
    "40] Stuart E Middleton, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Social computing for verifying social media content in breaking news. IEEE Internet Computing 22, 2 (2018), 83\u201389.  ",
    "41] Marko Milosavljevi\u0107 and Igor Vobi\u010d. 2021.  \u2018Our task is to demystify fears\u2019: Analysing newsroom management of automation in journalism. Journalism 22, 9 (2021), 2203\u20132221.  ",
    "42] R. Monarch. 2021. Human-in-the-Loop Machine Learning: Active Learning and Annotation for Human-centered AI. Manning. https://books.google.ch/books? id=LCh0zQEACAAJ  ",
    "43] Tom Nicholls and Pepper D Culpepper. 2021. Computational identification of media frames: Strengths, weaknesses, and opportunities. Political Communication 38, 1-2 (2021), 159\u2013181.  ",
    "44] Zhongdang Pan and Gerald M Kosicki. 1993. Framing analysis: An approach to  news discourse. Political communication 10, 1 (1993), 55\u201375.  ",
    "45] Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative  language models. arXiv preprint arXiv:1912.10165 (2019).  635\f"
  ],
  "url": "https://drive.google.com/uc?id=1vmiWI3NUHORkl-g7D9WA8Xh7YbTOHfmB",
  "date": "2024-01-09 21:25:05",
  "is_published": false
}
{
  "title": "Framing the News: From Human Perception to Large Language Model Inferences",
  "authors": [
    "David Alonso del Barrio",
    "Idiap Research Institute",
    "Daniel Gatica-Perez",
    "Idiap Research Institute and EPFL"
  ],
  "institutions": [
    "Switzerland",
    "Switzerland"
  ],
  "abstract": "Identifying the frames of news is important to understand the arti- cles\u2019 vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of jour- nalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to under- stand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No- Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Nat- ural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT- 3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.",
  "keywords": [
    "Covid-19 no-vax",
    " news framing",
    " GPT-3",
    " prompt-engineering",
    " trans-",
    "formers",
    " large language models"
  ],
  "article": "In recent years, there has been a proliferation in the use of concepts such as data journalism, computational journalism, and computer- assisted reporting [15] [29], which all share the vision of bridging journalism and technology. The progress made in NLP has been gradually integrated into the journalistic field [5][8][54]. More specifically, machine learning models based on transformers have been integrated in the media sector in different tasks [41] such as the creation of headlines with generative languages models [17], summarization of news articles [28][27], false news detection [49], and topic modeling and sentiment analysis [25]. The development of large language models such as GPT-3 [9], BLOOM [51] or ChatGPT show a clear trend towards human-machine interaction becoming easier and more intuitive, opening up a wide range of research possibilities. At the same time, the use of these models is also associated with a lack of transparency regarding how these models work, but efforts are being made to bring some transparency to where they cannot [35]. Based on the premises that these models open up a wide range of research directions [7], and that at the same time (and needless to say) they are not the solution to all problems, can be potentially useful, while acknowledging and systematically documenting their limitations [56]. More specifically, the aim of this work is to analyze the performance of GPT-3.5 for a specific use case, namely the analysis of frames in news, from an empirical point of view, with the objective of shedding light on a potential use of generative models in journalistic tasks.  Frame analysis is a concept from journalism, which consists of studying the way in which news stories are presented on an issue, and what aspects are emphasized: Is a merely informative vision given in an article? Or is it intended to leave a moral lesson? Is a news article being presented from an economic point of view? Or from a more human, emotional angle? The examples above correspond to different frames with which an article can be written. The concept of news framing has been studied in computing as a step beyond topic modeling and sentiment analysis, and for this purpose, in recent years, pre-trained language models have been used for fine-tuning the classification process of these frames [60] [10], but the emergence of generative models opens the possibility of doing prompt-engineering of these classification tasks, instead of the fine-tuning approach investigated so far.  Our work aims to address this research gap by posing the fol-  lowing research questions:  RQ1: What are the main frames in the news headlines about the anti-vaccine movement, as reported in newspapers across 5 European countries?  627\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  RQ2: Can prompt engineering be used for classification of head-  lines according to frames?  By addressing the above research questions, our work makes the  following contributions:  Contribution 1. We implemented a process to do human an- notation of the main frame of 1786 headlines of articles about the Covid-19 no-vax movement, as reported in 19 newspapers from 5 European countries (France, Italy, Spain, Switzerland and United Kingdom.) At the headline level, we found that the predominant frame was human interest, where this frame corresponds to a per- sonification of an event, either through a statement by a person, or the explanation of a specific event that happened to a person. Furthermore, we found a large number of headlines annotated as containing no frame, as they simply present information without entering into evaluations. We also found that for all the countries involved, the distribution of frame types was very similar, i.e., hu- man interest and no frame are the two predominant frames. Finally, the generated annotations allowed to subsequently study the per- formance of a large language model.  Contribution 2. We studied the performance of GPT-3.5 on the task of frame classification of headlines. In addition to using the fine-tuning approach from previous literature, we propose an alternative approach for frame classification that requires no labeled data for training, namely prompt-engineering using GPT-3.5. The results show that fine-tuning with GPT-3.5 produces 72% accuracy (slightly higher than other smaller models), and that the prompt- engineering approach results in lower performance (49% accuracy.) Our analysis also shows that the subjectivity of the human labeling task has an effect on the obtained accufracy.  The paper is organized as follows. In Section 2, we discuss related work. In Section 3, we describe the news dataset. In Section 4, we describe the methodology for both human labeling and machine classification of news frames. We present and discuss results for RQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide conclusions in Section 7.  2 RELATED WORK Framing has been a concept widely studied in journalism, with a definition that is rooted in the study of this domain [23]: \u201cTo frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a par- ticular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.\u201d  For frame recognition, there are two main approaches: the induc- tive approach [16], where one can extract the frames after reading the article, and the deductive approach [38], where a predefined list of frames exists and the goal is to interpret if any of them ap- pears in the article. In the deductive case, there are generic frames and subject-specific frames, and the way to detect them typically involves reading and identifying one frame at a time, or through answers to yes/no questions that represent the frames. Semetko et al. [52] used 5 types of generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) based on previous literature, and they defined a list of 20 yes/no questions to detect frames in articles. For instance, the questions about morality are the following: \"Does the story contain any moral  message? Does the story make reference to morality, God, and other religious tenets? Does the story offer specific social prescriptions about how to behave?\", and so on for each of the frame types. This categorization of frames has been used in various topics such as climate change [18] [19], vaccine hesitance [13], or immigration [34].  We now compare the two approaches on a common topic, such as Covid-19. Ebrahim et al. [21] followed an inductive approach in which the frames were not predefined but emerged from the text (e.g., deadly spread, stay home, what if, the cost of Covid-19) using headlines as the unit of analysis. In contrast, the deductive approach has studied very different labels. El-Behary et al. [22] followed the method of yes/no questions, but in addition to the 5 generic frames presented before, they also used blame frame and fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5 generic frames with yes/no questions, while Catal\u00e1n-Matamoros et al. [14] used the 5 frames and read the headline and subheadline to decide the main frame. Table 1 summarizes some of the the existing approaches. This previous work showed how frame labels can be different, and also that frame analysis has been done at both headline and article levels. These two approaches (inductive and deductive) that originated in journalism have since been replicated in the computing literature.  We decided to follow the deductive approach because a prede- fined list of frames allows to compare among topics, countries, previous literature, and also because they represent a fixed list of labels for machine classification models. Furthermore, the induc- tive approach tends to be more specific to a topic, and from the computing viewpoint, past work has tried to justify topic modeling as a technique to extract frames from articles.  Yl\u00e4-Antitila et al. [60] proposed topic modeling as a frame ex- traction technique. They argued that topics can be interpreted as frames if three requirements are met: frames are operationalized as connections between concepts; subject-specific data is selected; and topics are adequately validated as frames, for which they suggested a practical procedure. This approach was based on the choice of a specific topic (e.g., climate change) and the use of Latent Dirichlet Allocation (LDA) as a technique to extract a number of subtopics. In a second phase, a qualitative study of the top 10 words of each subtopic was performed, and the different subtopics were elimi- nated or grouped, reducing the number and establishing a tentative description. In a third phase, the top 10 articles belonging to that frame/topic were taken, and if the description of the topic fitted at least 8 of the 10 articles, that topic/frame remained. The frames found in this article were: green growth, emission cuts, negotiations and treaties, environmental risk, cost of carbon emissions, Chinese emissions, economics of energy production, climate change, en- vironmental activism, North-South burden sharing, state leaders negotiating, and citizen participation.  From Entman\u2019s definition of frame [23], it seems that the deduc- tive approach is more refined than the inductive approach (which seems to resemble the detection of sub-themes.) For example, with regard to climate change, there are stories on how people have been affected by climate change from an emotional point of view, thus personalizing the problem. In this case, we could categorize the corresponding frame as human interest, as the writer of the article is selecting \"some aspects of a perceived reality and make them  628\fFraming the News: From Human Perception to Large Language Model Inferences  more salient\". The language subtleties with which news articles are presented cannot be captured with basic topic modeling.  Isoaho et al.[30] held the position that while the benefits of scale and scope in topic modeling were clear, there were also a number of problems, namely that topic outputs do not correspond to the methodological definition of frames, and thus topic modeling remained an incomplete method for frame analysis. Topic modeling, in the practice of journalistic research, is a useful technique to deal with the large datasets that are available, yet is often not enough to do more thorough analyses [31]. In our work, we clearly notice that frame analysis is not topic modeling. For example, two documents could be about the same topic, say Covid-19 vaccination, but one article could emphasize the number of deaths after vaccination, while the other emphasized the role of the vaccine as a solution to the epidemic.  We also consider that the larger the number of possible frame types, the more likely it is to end up doing topic modeling instead of frame analysis. Using a deductive approach, Dallas et al. [12] created a dataset with articles about polemic topics such as immigration, same sex marriage, or smoking, and they defined 15 types of frames: \"economic, capacity and resources, morality, fairness and equality, legality, constitutionality and jurisprudence, policy prescription and evaluation, crime and punishment, security and defense, health and safety, quality of life, cultural identity, political, external regulation and reputation, other\". In this case, they authors did not use a list of questions. Instead, for each article, annotators were asked to identify any of the 15 framing dimensions present in the article and to label text blurbs that cued them (based on the definitions of each of the frame dimensions) and decide the main frame of each article. In our case, we followed the idea of detecting the main frame by reading the text instead of answering questions, but instead of using the 15 frames proposed in [12] , we used the 5 generic frames proposed in [52].  A final decision in our work was the type of text to analyze, whether headlines or whole article. For this decision, the chosen classification method was also going to be important. For example, Khanehzar et al. [33] used traditional approaches such as SVMs as baseline, and demonstrated the improvement in frame classifica- tion with the use of pre-trained languages models such as BERT, RoBERTa and XLNet, following a fine-tuning approach, setting as input text a maximum of 256 tokens (although the maximum number of input tokens in these models is 512 tokens.) Liu et al. [37] classified news headlines about the gun problem in the United States, arguing for the choice of headlines as a unit of analysis based on previous journalism literature [6], [44], that advocated for the importance and influence of headlines on readers and the subsequent perception of articles. From a computational viewpoint, using headlines is also an advantage, since you avoid the 512 token limitation in BERT-based models. Therefore, we decided to work with headlines about a controversial issue, namely the Covid-19 no-vax movement.  Continuing with the question of the methods used for classi- fication, much work has been developed in prompt engineering, especially since the release of GPT-3. Liu et al.[36] presented a good overview of the work done on this new NLP paradigm, not only explaining the concept of prompt engineering, but also the differ- ent strategies that can be followed both in the design of prompts,  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  Table 1: Summary of deductive approaches for frame analysis  Ref Frames  Goal  Technique  To label frames of full articles  [12] 15 generic frames: \"Economic\", \"Capac- ity and resources\", \"Morality\", \"Fair- ness and equality\", \"Legality, constitu- tionality and jurisprudence\", \"Policy prescription and evaluation\", \"Crime and punishment\", \"Security and de- fense\", \"Health and safety\", \"Quality of life\", \"Cultural identity\", \"Public opin- ion\", \"Political\", \"External regulation and reputation\", \"Other\".  [33] 15 generic frames  Classification  [52] 5 generic frames: \"human interest\", \"conflict\", \"morality\", \"attribution of responsibility\", and \"economic conse- quences\".  \u201cSociety/Culture\u201d, ,  [37] 9 specific frames:\u201cPolitics\u201d, \u201cPublic opinion\u201d, and \u201c2nd \u201cEconomic consequences\u201d Amendment\u201d (Gun Rights), \u201cGun control/regulation\u201d, \u201cMental health\u201d, \u201cSchool/Public space safety\u201d, and \u201cRace/Ethnicity\u201d.  [22] 5 generic frames + blame frame and  fear frame 5 generic frames  [1]  To label frames of full articles  To label frames of full articles/ Classification  To label frames of full articles To label frames of full articles  [50] 5 generic frames + pandemic frames  [14] 5 generic frames, journalistic role and  pandemic frames  To label frames of full articles To label frames of full articles  the Reading article, full the annotator defines the main frame  BERT based models Yes/No ques- tions.  the Reading full article, the annotator the defines main frame. BERT based models Yes/No ques- tions. Reading the article, full the annotator defines the main frame. Yes/No ques- tions. Reading the headline and subheadline, the annotator defines the main frame.  Number samples  of  20000 articles  12000 articles  2600 articles and 1522 tv news stories  2990 headlines  1170 articles  6713 articles  2742 articles  131 headlines + subheadlines  the potential applications, and the challenges to face when using this approach. Prompt engineering applications include knowledge probing [46], information extraction [53], NLP reasoning [57], ques- tion answering [32], text generation [20], multi-modal learning [58], and text classification [24], the latter being the prompt-engineering use case in our work. Puri et al.[45] presented a very interesting idea that we apply to our classification task. This consists of pro- viding the language model with natural language descriptions of classification tasks as input, and training it to generate the correct answer in natural language via a language modeling objective. It is a zero-shot learning approach, in which no examples are used to explain the task to the model. Radford et al. [48] demonstrated that language models can learn tasks without any explicit supervision. We have followed this approach to find an alternative way to do frame analysis.  As mentioned before, the emergence of giant models like GPT-3, BLOOM, and ChatGPT are a very active research topic. To the best of our knowledge, on one hand our work extends the computational analysis of news related to the covid-19 no-vax movement, which illustrates the influence of the press on the ways societies think about relevant issues [40], [59], and on the other hand it adds to the literature of human-machine interaction, regarding the design of GPT-3 prompts for classification tasks [39], [2].  3 DATA: EUROPEAN COVID-19 NEWS  DATASET  We used part of the European Covid-19 News dataset collected in our recent work [3]. This dataset contains 51320 articles on Covid- 19 vaccination from 19 newspapers from 5 different countries: Italy,  629\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  France, Spain, Switzerland and UK. The articles cover a time period of 22 months, from January 2020 to October 2021. All content was translated into English to be able to work in a common language. The dataset was used for various analyses, such as name entity recognition, sentiment analysis, and subtopic modeling, to under- stand how Covid-19 vaccination was reported in Europe through the print media (in digital format.) The subtopic modeling analysis revealed a subsample of articles on the no-vax movement, which is the one we have used in this paper. We took the headlines of the articles associated with the no-vax movement, selecting all articles containing any of the keywords in Table 2 in the headline or in the main text. This corresponds to a total of 1786 headlines.  Table 2: Keywords used to identify no-vax articles  NO VAX TOPIC  \"anti-vaxxers\", \"anti-vaccine\", \"anti-vaxx\", \"anti-corona\", \"no-vax\", \"no vax\",\"anti-vaccin\"  Keywords  In Table 3, we show the number of headlines per country and newspaper. France is the country with the most no-vax articles in the corpus, with 523 articles, followed by Italy with 508. However, note that there are 6 newspapers from France, while only 2 from Italy. Corriere della Sera is the newspaper that dealt most frequently with the subject (429 articles), while The Telegraph is the second one (206 articles). The total number of articles normalized by the number of newspapers per country is also shown in the last column of the Table. Using these normalized values, the ranking is Italy, UK, France, Switzerland, and Spain.  Table 3: Number of headlines by newspaper and country  COUNTRY  NEWSPAPER  HEADLINES  TOTAL (NORM. TOTAL)  FRANCE  ITALY  SPAIN  SWITZERLAND  UNITED KINGDOM  La Croix Le Monde Les Echos Liberation Lyon Capitale Ouest France Corriere della Sera Il Sole 24 Ore 20 minutos ABC El Diario El Mundo El Espa\u00f1ol La Vanguardia 24 heures La Libert\u00e9 Le Temps The Irish News The Telegraph  94 125 49 97 8 150 429 79 27 50 32 77 22 95 97 22 111 16 206  523 (87.1)  508 (254.0)  303 (50.5)  230 (76.6)  222 (111.0)  1786  4 METHODOLOGY 4.1 Human labeling of news frames To carry out the labeling of the frames in our corpus of headlines, we first designed a codebook, which contained the definitions of each of the frame types and a couple of examples of each type, as well as a definition of the corpus subject matter and definitions of the concept of frame analysis, so that the annotators could understand the task to be performed. The codebook follows the proposed by  [52] with 5 generic frames (attribution of responsibility, human interest, conflict, morality, and economic consequences) plus one additional \u2019no-frame\u2019 category. Two researchers were engaged to annotate a sample of the collected newspaper articles following a three-phase training procedure.  In the first phase, annotators had to read the codebook and get familiar with the task. In the second phase, they were asked to identify the main frame in the same subset of 50 headlines. At the end of the second phase, the intercoder reliability (ICR) was 0.58 between the 2 annotators. We analyzed those cases where there were discrepancies, and observed that in some cases, there was not a unique main frame, because both annotators had valid arguments to select one of the frames. In other cases, the discrepancies were due to slight misunderstanding of the definitions. In the third phase, the annotators coded again 50 headlines, and the ICR increased to was 0.66. We realized that the possibility of having two frames remained. They discussed the cases in which they had disagreed, and if the other person\u2019s arguments were considered valid, it could be said that there were two frames. After this three-phase training procedure, annotators were ready to annotate the dataset independently. We divided the dataset into two equal parts, and each person annotated 893 headlines.  4.2 Fine-tuning GPT-3.5 and BERT-based  models  With the annotated dataset, we investigated two NLP approaches: the first one involves fine-tuning a pre-trained model; the second one is prompt engineering. Pre-trained language models have been  Figure 1: Pre-train, fine-tune, prompt  trained with large text strings based on two unsupervised tasks, next sentence prediction and masked language model. Figure 1 summarizes these techniques.  In the first approach, a model with a fixed architecture is pre- trained as a language model (LM), predicting the likelihood of the observed textual data. This can be done due to the availability of large, raw text data needed to train LMs. This learning process can produce general purpose features of the modeled language. The learning process produces robust, general-purpose features of the language being modeled. The above pre-trained LM is then adapted to different downstream tasks, by introducing additional parameters and adjusting them using task-specific objective functions. In this approach, the focus was primarily on goal engineering, designing the training targets used in both the pre-training and the fine-tuning stages [36].  630\fFraming the News: From Human Perception to Large Language Model Inferences  We present an example to illustrate the idea. Imagine that the task is sentiment analysis, and we have a dataset with sentences and their associated sentiment, and a pre-trained model, which is a saved neural network trained with a much larger dataset. For that pre-trained model to address the target task, we unfreeze a few of the top layers of the saved model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows to \"fine-tune\" the higher-order feature representations in the base model to make them more relevant for the sentiment analysis task. In this way, instead of having to obtain a very large dataset with target labels to train a model, we can reuse the pre- trained model and use a much smaller train dataset. We use a part of our dataset as examples for the model to learn the task, while the other part of the dataset is used to evaluate model performance. Previous works related to frame classification in the computing literature have used fine-tuning, BERT-based models. In our work, we have done the same as a baseline, but we aimed to go one step further and also produce results using fine-tuning of GPT-3.5.  4.3 Prompt-engineering with GPT-3.5 Model fine-tuning has been widely used, but with the emergence of generative models such as GPT-3, another way to approach classification tasks has appeared. The idea is to use the pre-trained model directly and convert the task to be performed into a format as close as possible to the tasks for which it has been pre-trained. That is, if the model has been pre-trained from next word prediction as in the case of GPT-3, classification can be done by defining a prompt, where the input to the model is an incomplete sentence, and the model must complete it with a word or several words, just as it has been trained. This avoids having to use part of the already labeled dataset to teach the task to be performed to the model, and a previous labeling is not needed [36].  In this approach, instead of adapting pre-trained LMs to down- stream tasks via objective engineering, downstream tasks are re- formulated to look more like those solved during the original LM training with the help of a textual prompt. For example, when recog- nizing the emotion of a social media post, \u201cI missed the bus today.\u201d, we may continue with a prompt \u201cI felt so _\u201d, and ask the LM to fill the blank with an emotion-bearing word. Or if we choose the prompt \u201cEnglish: I missed the bus today. French: _\u201d), an LM may be able to fill in the blank with a French translation. In this way, by selecting the appropriate prompts, we can influence the model behavior so that the pre-trained LM itself can be used to predict the desired output, even without any additional task-specific training [36].  We use this emerging NLP approach to classify frames at headline level. We are not aware of previous uses of this strategy to classify frames as we propose here. The idea is the following. Prompt engi- neering consists of giving a prompt to the model, and understands that prompt as an incomplete sentence. To do prompt engineer- ing with our dataset, we needed to define an appropriate prompt that would produce the headline frames as output. We defined sev- eral experiments with the Playground of GPT-3, in order to find the best prompt for our task. In our initial experiments, we fol- lowed existing approaches in prompt engineering to do sentiment analysis, where the individual answer was an adjective, and this  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  adjective was matched with a sentiment. In a similar fashion, we decided to build a thesaurus of adjectives that define each of the frames. For instance, the human interest frame could be \u2019interest- ing\u2019, \u2019emotional\u2019, \u2019personal\u2019, \u2019human\u2019. The conflict frame could be: \u2019conflictive\u2019, \u2019bellicose\u2019, \u2019troublesome\u2019, \u2019rowdy\u2019, \u2019quarrelsome\u2019, \u2019trou- blemaker\u2019, \u2019agitator\u2019, etc. After the list of adjectives was defined, we needed to define the prompt in order to get, as an answer, one of the adjectives in our thesaurus to match them with the frame. We used the GPT-3 playground using the headline as input and asking for the frame as output, but the strategy did not work. In our final experiment, instead of giving the headline as input, we gave the definitions of each type of frame plus the headline, and we asked the model to choose between the different types of frames as output. In this way, the output of the model was directly one of the frames, and we avoided the step of matching adjectives with frames. An example is shown in Figure 2.  Figure 2: GPT-3.5 for frame inference: input and output  For the GPT-3 configuration 1, there are 3 main concepts:  \u2022 TEMPERATURE [0-1]. This parameter controls randomness,  lowering it results in less random completions.  \u2022 TOP_P [0-1]. This parameter controls diversity via nucleus  sampling.  \u2022 MAX_TOKENS[1-4000]. This parameter indicates the maxi-  mum number of tokens to generate,  \u2022 MODEL. GPT-3 offer four main models with different levels of power, suitable for different tasks. Davinci is the most capable model, and Ada is the fastest.  After testing with the GPT-3 playground and varying different hyper-parameters to assess performance, we set the temperature to 0, since the higher the temperature the more random the response. Furthermore, the Top-p parameter was set to 1, as it would likely get a set of the most likely words for the model to choose from. The maximum number of tokens was set to 2; in this way, the model is asked to choose between one of the responses. As a model, we used the one with the best performance at the time of experimental design, which was TEXT-DAVINCI-003, recognized as GPT 3.5.  5 RESULTS: HUMAN LABELING OF FRAMES  IN NO-VAX NEWS HEADLINES (RQ1)  In this section, we present and discuss the results of the analysis related to our first RQ.  Figure 3 shows the distribution of frames per country at headline level, with human interest and no-frame being the predominant   631\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  ones. Attribution of responsibility is the third one except in Switzer- land, where the corresponding frame is conflict. Finally, morality and economic are the least represented in the dataset for every country.  Figure 4: Non-normalized monthly distribution of frames.  detail the negative and positive sentiment of each frame category, we observed a few trends:  \u2022 Attribution of responsibility: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Switzerland, and the United Kingdom. \u2022 Conflict: Negative sentiment represents 20-35% of the cases. \u2022 Economic: Predominantly neutral, with only negative tone in Italy and UK (in the latter case, all headlines with this frame were considered negative.)  \u2022 Human interest: Negative sentiment represents 30-40% of the cases, while positive tone is only found in residual form in Italy, Spain, and Switzerland.  \u2022 Morality: Predominantly neutral, with negative tone in Italy,  Switzerland, and the United Kingdom, \u2022 No frame: 20-30% of negative content.  Figure 5: Sentiment of headline by frame and by country  Figure 3: Non-normalized distribution of frames per country  The monthly distribution of frames aggregated for all countries is shown in Fig. 4. We can see two big peaks, the first one in January 2021 and the second one in August 2021. In all countries, the vac- cination process started at the end of December 2020, so it makes sense that the no-vax movement started to be more predominant in the news in January 2021. Human interest is the most predominant frame. Manual inspection shows that this is because the headlines are about personal cases of people who are pro- or anti- vaccine. Attribution of responsibility is also present. Manual inspection in- dicates that local politicians and health authorities had to make decisions about who could be vaccinated at the beginning of the process. The second peak at the end of summer 2021 coincided with the health pass (also called Covid passport in some countries), and we can observe a peak in the curve corresponding to the con- flict frame, reflecting the demonstrations against the measure of mandatory health passes taken by country governments.  In Figure 5, we compare the sentiment per frame and per country, to understand if there were any major differences. The sentiment analysis labels were obtained using BERT-sent from the Hugging Face package [47], used in our previous work (please refer to our original analysis in [3] for details.) We normalized the results be- tween 0 and 1 to compare frames between countries. We see that the sentiment is predominantly neutral (in blue). Examining in more  632\fFraming the News: From Human Perception to Large Language Model Inferences  Regarding the results of the annotation process, the fact that the distribution of the 6 frame types is relatively similar between coun- tries suggests that the anti-vaccine movement issue was treated in a similar way in these countries. The fact that human interest is the most dominant frame indicates that this issue was treated from a more human and emotional approach, with headlines about personal experiences, celebrities giving their opinion about vacci- nation, and politicians defending vaccine policies. Moreover, the reason for many headlines being classified as no-frame is partly due to how data was selected. We chose articles that contained words related to no-vax, either in the headline or in the article. This resulted in many headlines not containing anything specific related to no-vax, while the no-vax content was actually included in the main text of the corresponding articles.  It is worth mentioning that prior to obtaining the results, we had expected that attribution of responsibility would be among the most prominent frames, since governments took many measures such as mandatory health pass requirements to access certain sites; we had also expected that the conflict frame would be prominent, since there were many demonstrations in Europe. In reality, however, these frames categories were not reflected as frequently at the headline level.  Regarding the analysis at the temporal level, it is clear that certain events were captured by the press, such as the start of vaccination or the mandatory vaccination passport.  Finally, the sentiment analysis of the different frames shows that the predominant tone in all of them is neutral or negative, with very similar trends between countries. This association between senti- ment analysis and frames has been discussed in previous literature [11] [43].  6 RESULTS: GPT-3.5 FOR FRAME  CLASSIFICATION OF HEADLINES (RQ2) Here, we present and discuss the results related to our second RQ.  6.1 Fine-tuning GPT-3.5 Table 4 shows the results of the 6-class classification task using 5-cross validation. Three models were used: GPT-3.5 and two BERT- based models. We observe that, on average, GPT-3.5 performs better than the BERT-based models. This is somehow expected as GPT- 3.5 is a much larger model. Overall, in the case of fine-tuning, the best performance for the six-class frame classification task is 72% accuracy, which is promising, with an improvement over previous models based on BERT. Yet, it should be noted that the performance differences are modest (2% improvement between GPT-3.5 and RoBERTa).  Table 4: Classification results for six-class frame classifica- tion and 5-fold cross validation  ACCURACY  BERT RoBERTa GPT3  0  0.68 0.70 0.75  1  0.69 0.72 0.70  2  0.72 0.72 0.72  3  0.64 0.67 0.71  4  0.70 0.71 0.71  AVERAGE  0.67 0.70 0.72  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  On the other hand, BERT is open-source, while GPT-3 has an economic cost as the use of the model is not free, which monetarily limits the number of experiments that can be performed with it, as well as the different configurations one can explore to improve performance. This is important because much of the improvement in performance requires empirical explorations of model parameters More specifically, the cost of an experiment for each of the folds has a cost of 4 dollars (at the time of writing this paper.) This represents a limitation in practice.  Furthermore, GPT-3 has a significant carbon footprint. Similarly, for prompt engineering (discussed in the next subsection), choosing the right prompt (i.e., the words that best define the task so that the model is able to perform adequately) is also based on trial and error. This also has an impact on carbon footprint. In connection with this topic, Strubell et al.[55] argue that improvements in the accu- racy of models depend on the availability of large computational resources, which involve large economic and environmental costs. A criticism has been made as \u2019the rich get richer\u2019, in the sense that not all research groups have sufficient infrastructure resources and access to funding needed to use these models and improve their performance. Also in relation to this analysis, the work of Bender et al. [4] evaluates the costs and risks of the use of large language models, stating that researchers should be aware of the impact that these models have on the environment, and assess whether the benefits outweigh the risks. The work in [4] provides a very telling example, where people living in the Maldives or Sudan are affected by floods and pay the environmental price of training English LLMs, when similar models have not been produced for languages like Dhivehi or Sudanese Arab. In short, there is a need to establish ways to use this technological development responsibly, and it all starts with being aware of the risks it presents.  6.2 Prompt-engineering with GPT-3.5 For each headline, we got the frame that the model considered the most likely, and we compared these GPT-3.5 inferences with the frames labeled by the annotators. The agreement between model and annotator was of 49%. Analyzing the results, and specifically looking at the cases where the annotator and GPT-3.5 disagreed, we discovered that according to the frame definitions, the model in some cases proposed a frame that indeed made sense. This ob- servation, together with our previous experience in the annotation process, where headlines could have more than one valid frame, led us to design a second post-hoc experiment. We took all the headlines where each of the two annotators had disagreed with GPT-3.5, and we asked the annotators to state whether they would agree (or not) with each GPT-inferred label for a given headline. It is important to emphasize that the annotators did not know the origin of that label, i.e., they did not know if it was the label they had originally assigned, or if it was a random one. In this way, we could quantify how GPT-3.5 worked according to valid arguments provided by the annotators. In this post-hoc experiment, the model agreed in 76% of cases with the annotators.  Looking at the results of the classification models, the 49% accu- racy of the prompt-engineering approach can be considered low, yet we consider that it is a valid avenue for further investigation, as in the second post-hoc analysis, we found that the model agrees  633\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  David Alonso del Barrio and Daniel Gatica-Perez  with human annotators in 76% of the cases. Clearly, framing in- volves aspects of subjectivity [42]. Much of what we do as people has a subjective component, influenced by how we feel or how we express opinions.  News reading is never fully objective, and the annotators en- gaged in the frame classification task, influenced by their personal state of mind, experience, and culture, may perceive information differently. Monarch affirms that \"for simple tasks, like binary labels on objective tasks, the statistics are fairly straightforward to decide which is the \u2018correct\u2019 label when different annotators disagree. But for subjective tasks, or even objective tasks with continuous data, there are no simple heuristics for deciding what the correct label should be\" [42].  Subjectivity is involved in both the generation and perception of information: the assumption that there is only one frame is com- plicated by the point of view of the reader. In the case of news, the information sender (the journalist) has an intention, but the receiver (the reader) plays a role and is influenced by it. In psychology, this is known as the lens model of interpersonal communication, where the sender has certain objectives, but the receiver can interpret or re-interpret what the sender wants to say, with more or less accuracy [26].  Following this discussion on subjectivity, the question arose as to what would happen if, instead of headlines, we used the complete article as a source of analysis. We wondered if longer text could make the frame labeling task clearer than when using headlines. Yet another possible hypothesis is that having to read longer texts could lead to the same subject being presented from different angles. Please recall that in the existing literature discussed in Section 2, both headlines and full articles have been used from frame analysis (see Table 1.) This remains as an issue for future work.  7 CONCLUSIONS In this paper, we first presented an analysis of human-generated news frames on the covid-19 no-vax movement in Europe, and then studied different approaches using large language models for automatic inference of frames. We conclude by answering the two research questions we posed:  RQ1: What are the main frames in the news headlines about the covid-19 anti-vaccine movement in 5 European countries? After annotating the headlines, we found that of the 1786 headlines, the predominant frame is human interest (45.3% of cases), which presents a news item with an emotional angle, putting a face to a problem or situation. We also found that a substantial proportion of headlines were annotated as not presenting any frame (40.2% of cases). Finally, the other frame types are found more infrequently. RQ2: Can prompt engineering be used for classification of head- lines according to frames? We first used fine-tuning of a number of language models, and found that GPT-3.5 produced classification ac- curacy of 72% on a six-frame classification task. This represented a modest 2% improvement over BERT-based models, at a significantly larger environmental cost. We then presented a new way of classi- fying frames using prompts. At the headline level, inferences made with GPT-3.5 reached 49% of agreement with human-generated frame labels. In many cases, the GPT-3.5 model inferred frame types that were considered as valid choices by human annotators,  and in an post-doc experiment, the human-machine agreement reached 76%. These results have opened several new directions for future work.  ACKNOWLEDGMENTS",
  "references": [
    "1] Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online media: Quantitative framing analysis on Detik. com\u2019s coverage of Covid-19. Jurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153\u2013170.  ",
    "2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. 2021. RAFT: A real-world few-shot text classification benchmark. arXiv preprint arXiv:2109.14076 (2021).  ",
    "3] David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe\u2019s Press (2022), 35\u201343.  Cover Covid-19 Vaccination News? A Five-Country Analysis. https://doi.org/10.1145/3512732.3533588  ",
    "4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? (2021), 610\u2013623.  ",
    "5] Santosh Kumar Biswal and Nikhil Kumar Gouda. 2020. Artificial intelligence in journalism: A boon or bane? In Optimization in machine learning and applications. Springer, 155\u2013167.  ",
    "6] Erik Bleich, Hannah Stonebraker, Hasher Nisar, and Rana Abdelhamid. 2015. Media portrayals of minorities: Muslims in British newspaper headlines, 2001\u2013 2012. Journal of Ethnic and Migration Studies 41, 6 (2015), 942\u2013962.  ",
    "7] Michael Bommarito and Daniel Martin Katz. 2022. GPT Takes the Bar Exam.  https://doi.org/10.48550/ARXIV.2212.14402  ",
    "8] Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe, Michel Dupagne, and Ching-Hua Chuan. 2019. Artificial intelligence and jour- nalism. Journalism & Mass Communication Quarterly 96, 3 (2019), 673\u2013695. ",
    "9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901.  ",
    "10] Bj\u00f6rn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, and Claes H De Vreese. 2014. Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis. Communication Methods and Measures 8, 3 (2014), 190\u2013206.  ",
    "11] Bjorn Burscher, Rens Vliegenthart, and Claes H de Vreese. 2016. Frames beyond words: Applying cluster and sentiment analysis to news coverage of the nuclear power issue. Social Science Computer Review 34, 5 (2016), 530\u2013545.  ",
    "12] Dallas Card, Amber Boydstun, Justin Gross, Philip Resnik, and Noah Smith. 2015. The Media Frames Corpus: Annotations of Frames Across Issues. 2 (01 2015), 438\u2013444. https://doi.org/10.3115/v1/P15-2072  ",
    "13] Daniel Catalan-Matamoros and Carlos El\u00edas. 2020. Vaccine hesitancy in the age of coronavirus and fake news: analysis of journalistic sources in the Spanish quality press. International Journal of Environmental Research and Public Health 17, 21 (2020), 8136.  ",
    "14] Daniel Catal\u00e1n-Matamoros and Carmen Pe\u00f1afiel-Saiz. 2019. Media and mistrust of vaccines: a content analysis of press headlines. Revista latina de comunicaci\u00f3n social 74 (2019), 786\u2013802.  ",
    "15] Mark Coddington. 2015. Clarifying journalism\u2019s quantitative turn: A typology for evaluating data journalism, computational journalism, and computer-assisted reporting. Digital journalism 3, 3 (2015), 331\u2013348.  ",
    "16] Stephen D Cooper. 2010. The oppositional framing of bloggers. In Doing News  Framing Analysis. Routledge, 151\u2013172.  ",
    "17] Robert Dale. 2021. GPT-3: What\u2019s it good for? Natural Language Engineering 27,  1 (2021), 113\u2013118.  ",
    "18] Astrid Dirikx and Dave Gelders. 2010.  To frame is to explain: A de- ductive frame-analysis of Dutch and French climate change coverage dur- ing the annual UN Conferences of the Parties. Public Understanding of Science 19, 6 (2010), 732\u2013742. https://doi.org/10.1177/0963662509352044 arXiv:https://doi.org/10.1177/0963662509352044 PMID: 21560546.  ",
    "19] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A deductive frame- analysis of Dutch and French climate change coverage during the annual UN Conferences of the Parties. Public understanding of science 19, 6 (2010), 732\u2013742.  634\fFraming the News: From Human Perception to Large Language Model Inferences  ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece  ",
    "20] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2020. Gsum: A general framework for guided neural abstractive summarization. arXiv preprint arXiv:2010.08014 (2020).  ",
    "21] Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA Gesondheid (Online) 27 (2022), 1\u20138.  ",
    "22] Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021). ",
    "23] Robert M Entman. 1993. Framing: Towards clarification of a fractured paradigm.  McQuail\u2019s reader in mass communication theory 390 (1993), 397.  ",
    "24] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723 (2020). ",
    "25] Piyush Ghasiya and Koji Okamura. 2021. Investigating COVID-19 news across four nations: a topic modeling and sentiment analysis approach. Ieee Access 9 (2021), 36645\u201336656.  ",
    "26] Robert Gifford. 1994. A Lens-Mapping Framework for Understanding the En- coding and Decoding of Interpersonal Dispositions in Nonverbal Behavior. Journal of Personality and Social Psychology 66 (02 1994), 398\u2013412. https: //doi.org/10.1037//0022-3514.66.2.398  ",
    "27] Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based transformer architectures for long document summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 1792\u20131810.  ",
    "28] Anushka Gupta, Diksha Chugh, Rahul Katarya, et al. 2022. Automated news summarization using transformers. In Sustainable Advanced Computing. Springer, 249\u2013259.  ",
    "29] Alfred Hermida and Mary Lynn Young. 2017. Finding the data unicorn: A hierar- chy of hybridity in data and computational journalism. Digital Journalism 5, 2 (2017), 159\u2013176.  ",
    "30] Karoliina Isoaho, Daria Gritsenko, and Eetu M\u00e4kel\u00e4. 2021. Topic modeling and text analysis for qualitative policy research. Policy Studies Journal 49, 1 (2021), 300\u2013324.  ",
    "31] Carina Jacobi, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative analysis of large amounts of journalistic texts using topic modelling. Digital journalism 4, 1 (2016), 89\u2013106.  ",
    "32] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computa- tional Linguistics 8 (2020), 423\u2013438.  ",
    "33] Shima Khanehzar, Andrew Turpin, and Gosia Miko\u0142ajczak. 2019. Modeling  ",
    "46] Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with  mixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021).  ",
    "47] Rabindra Lamsal. 2021. Sentiment Analysis of English Tweets with BERTsent. https://huggingface.co/rabindralamsal/finetuned-bertweet-sentiment-analysis. ",
    "48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.  ",
    "49] Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, and Ahad Ali. 2022. Fake News Classification using transformer based enhanced LSTM and BERT. International Journal of Cognitive Computing in Engineering 3 (2022), 98\u2013105. https://doi.org/10.1016/j.ijcce.2022.03.003  ",
    "50] Frida V Rodelo. 2021. Framing of the Covid-19 pandemic and its organizational  predictors. Cuadernos. info 50 (2021), 91\u2013112.  ",
    "51] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).  ",
    "52] Holli Semetko and Patti Valkenburg. 2000. Framing European Politics: A Content Analysis of Press and Television News. Journal of Communication 50 (06 2000), 93 \u2013 109. https://doi.org/10.1111/j.1460-2466.2000.tb02843.x  ",
    "53] Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Em- manouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. arXiv preprint arXiv:2104.08768 (2021).  ",
    "54] Efstathios Sidiropoulos and Andreas Veglis. 2017. Computer Supported Collab- orative Work trends on Media Organizations: Mixing Qualitative and Quan- titative Approaches. Studies in Media and Communication 5 (04 2017), 63. https://doi.org/10.11114/smc.v5i1.2279  ",
    "55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019). ",
    "56] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503 (2021).  ",
    "57] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning.  arXiv preprint arXiv:1806.02847 (2018).  ",
    "58] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems 34 (2021), 200\u2013212.  ",
    "59] Sandra A Vannoy and Prashant Palvia. 2010. The social influence model of  technology adoption. Commun. ACM 53, 6 (2010), 149\u2013153.  ",
    "60] Tuukka Yl\u00e4-Anttila, Veikko Eranti, and Anna Kukkonen. 2022. Topic modeling for frame analysis: A study of media debates on climate change in India and USA. Global Media and Communication 18, 1 (2022), 91\u2013112.  Political Framing Across Policy Issues and Contexts. In ALTA.  ",
    "34] Jeesun Kim and Wayne Wanta. 2018. News framing of the US immigration debate during election years: Focus on generic frames. The Communication Review 21, 2 (2018), 89\u2013115.  ",
    "35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi- hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).  ",
    "36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. (2021). https://doi.org/10. 48550/ARXIV.2107.13586  ",
    "37] Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and Derry Tanti Wijaya. 2019. Detecting frames in news headlines and its application to analyzing news framing trends surrounding US gun violence. In Proceedings of the 23rd conference on computational natural language learning (CoNLL).  ",
    "38] J\u00f6rg Matthes and Matthias Kohring. 2008. The Content Analysis of Media Frames: Toward Improving Reliability and Validity. Journal of Communication 58 (06 2008). https://doi.org/10.1111/j.1460-2466.2008.00384.x  ",
    "39] Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, and David E Losada. 2022. Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI. In Proceedings of the 4th Conference on Conversational User Interfaces. 1\u20136.  ",
    "40] Stuart E Middleton, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Social computing for verifying social media content in breaking news. IEEE Internet Computing 22, 2 (2018), 83\u201389.  ",
    "41] Marko Milosavljevi\u0107 and Igor Vobi\u010d. 2021.  \u2018Our task is to demystify fears\u2019: Analysing newsroom management of automation in journalism. Journalism 22, 9 (2021), 2203\u20132221.  ",
    "42] R. Monarch. 2021. Human-in-the-Loop Machine Learning: Active Learning and Annotation for Human-centered AI. Manning. https://books.google.ch/books? id=LCh0zQEACAAJ  ",
    "43] Tom Nicholls and Pepper D Culpepper. 2021. Computational identification of media frames: Strengths, weaknesses, and opportunities. Political Communication 38, 1-2 (2021), 159\u2013181.  ",
    "44] Zhongdang Pan and Gerald M Kosicki. 1993. Framing analysis: An approach to  news discourse. Political communication 10, 1 (1993), 55\u201375.  ",
    "45] Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative  language models. arXiv preprint arXiv:1912.10165 (2019).  635\f"
  ],
  "url": "https://drive.google.com/uc?id=1vmiWI3NUHORkl-g7D9WA8Xh7YbTOHfmB",
  "date": "2024-01-09 21:28:56",
  "is_published": false
}
